0
('Epoch Training Loss:', 221.62683057785034)
('Epoch Training Acc:', 0.56162500358186662)
('test loss', 0.85293531)
('test Acc:', 0.7298491)
model_080 F1_score: 77.64% >>>
1
('Epoch Training Loss:', 99.046404778957367)
('Epoch Training Acc:', 0.79762499965727329)
('test loss', 0.78332055)
('test Acc:', 0.74089068)
model_080 F1_score: 78.47% >>>
2
('Epoch Training Loss:', 48.545811176300049)
('Epoch Training Acc:', 0.90524999611079693)
('test loss', 0.69722515)
('test Acc:', 0.76665437)
model_080 F1_score: 80.90% >>>
3
('Epoch Training Loss:', 17.98623263835907)
('Epoch Training Acc:', 0.97675000168383119)
('test loss', 0.72506452)
('test Acc:', 0.77327937)
model_080 F1_score: 81.61% >>>
4
('Epoch Training Loss:', 5.7311379630118608)
('Epoch Training Acc:', 0.99812500104308133)
('test loss', 0.73281336)
('test Acc:', 0.77438349)
model_080 F1_score: 81.44% >>>
5
('Epoch Training Loss:', 2.5126265631988645)
('Epoch Training Acc:', 0.99987500011920927)
('test loss', 0.76106924)
('test Acc:', 0.78321677)
model_080 F1_score: 82.41% >>>
6
('Epoch Training Loss:', 1.4272820516489446)
('Epoch Training Acc:', 1.0)
('test loss', 0.7833699)
('test Acc:', 0.78726536)
model_080 F1_score: 82.83% >>>
7
('Epoch Training Loss:', 1.0305978741962463)
('Epoch Training Acc:', 1.0)
('test loss', 0.7911436)
('test Acc:', 0.7868973)
model_080 F1_score: 82.74% >>>
8
('Epoch Training Loss:', 0.81067013391293585)
('Epoch Training Acc:', 1.0)
('test loss', 0.80708611)
('test Acc:', 0.79020977)
model_080 F1_score: 82.93% >>>
9
('Epoch Training Loss:', 0.63111760304309428)
('Epoch Training Acc:', 1.0)
('test loss', 0.82549226)
('test Acc:', 0.79241812)
model_080 F1_score: 83.27% >>>
10
('Epoch Training Loss:', 0.55378098704386503)
('Epoch Training Acc:', 1.0)
('test loss', 0.83811033)
('test Acc:', 0.79020977)
model_080 F1_score: 82.91% >>>
11
('Epoch Training Loss:', 0.47179834241978824)
('Epoch Training Acc:', 1.0)
('test loss', 0.84173787)
('test Acc:', 0.78910565)
model_080 F1_score: 82.93% >>>
12
('Epoch Training Loss:', 0.40825437044259161)
('Epoch Training Acc:', 1.0)
('test loss', 0.8513695)
('test Acc:', 0.79205006)
model_080 F1_score: 83.22% >>>
13
('Epoch Training Loss:', 0.36982831847853959)
('Epoch Training Acc:', 1.0)
('test loss', 0.85973263)
('test Acc:', 0.78542513)
model_080 F1_score: 82.46% >>>
14
('Epoch Training Loss:', 0.3317353879683651)
('Epoch Training Acc:', 1.0)
('test loss', 0.86153513)
('test Acc:', 0.78947371)
model_080 F1_score: 82.89% >>>
15
('Epoch Training Loss:', 0.3017671681009233)
('Epoch Training Acc:', 1.0)
('test loss', 0.86758447)
('test Acc:', 0.79425836)
model_080 F1_score: 83.05% >>>
16
('Epoch Training Loss:', 0.27548780554207042)
('Epoch Training Acc:', 1.0)
('test loss', 0.87025851)
('test Acc:', 0.79131395)
model_080 F1_score: 82.90% >>>
17
('Epoch Training Loss:', 0.26539754628902301)
('Epoch Training Acc:', 1.0)
('test loss', 0.87462008)
('test Acc:', 0.79205006)
model_080 F1_score: 83.09% >>>
18
('Epoch Training Loss:', 0.24159857601625845)
('Epoch Training Acc:', 1.0)
('test loss', 0.89678127)
('test Acc:', 0.78984171)
model_080 F1_score: 82.79% >>>
19
('Epoch Training Loss:', 0.22434305644128472)
('Epoch Training Acc:', 1.0)
('test loss', 0.88650697)
('test Acc:', 0.79609865)
model_080 F1_score: 83.46% >>>
20
('Epoch Training Loss:', 0.20393676363164559)
('Epoch Training Acc:', 1.0)
('test loss', 0.90416515)
('test Acc:', 0.78800148)
model_080 F1_score: 82.81% >>>
21
('Epoch Training Loss:', 0.19513430952792987)
('Epoch Training Acc:', 1.0)
('test loss', 0.90601099)
('test Acc:', 0.78910565)
model_080 F1_score: 82.74% >>>
22
('Epoch Training Loss:', 0.18286553688813001)
('Epoch Training Acc:', 1.0)
('test loss', 0.90407783)
('test Acc:', 0.78910565)
model_080 F1_score: 82.72% >>>
23
('Epoch Training Loss:', 0.17521343880798668)
('Epoch Training Acc:', 1.0)
('test loss', 0.90577841)
('test Acc:', 0.79094589)
model_080 F1_score: 82.90% >>>
24
('Epoch Training Loss:', 0.16674430642160587)
('Epoch Training Acc:', 1.0)
('test loss', 0.91258883)
('test Acc:', 0.79315424)
model_080 F1_score: 83.15% >>>
25
('Epoch Training Loss:', 0.16040800657356158)
('Epoch Training Acc:', 1.0)
('test loss', 0.9226566)
('test Acc:', 0.79352224)
model_080 F1_score: 83.44% >>>
26
('Epoch Training Loss:', 0.15201246878132224)
('Epoch Training Acc:', 1.0)
('test loss', 0.91446525)
('test Acc:', 0.78800148)
model_080 F1_score: 83.09% >>>
27
('Epoch Training Loss:', 0.14685056428425014)
('Epoch Training Acc:', 1.0)
('test loss', 0.91749734)
('test Acc:', 0.79057783)
model_080 F1_score: 82.82% >>>
28
('Epoch Training Loss:', 0.14549555009580217)
('Epoch Training Acc:', 1.0)
('test loss', 0.92126936)
('test Acc:', 0.78836954)
model_080 F1_score: 82.68% >>>
29
('Epoch Training Loss:', 0.13510205791681074)
('Epoch Training Acc:', 1.0)
('test loss', 0.92467654)
('test Acc:', 0.79352224)
model_080 F1_score: 83.17% >>>
30
('Epoch Training Loss:', 0.12530352652538568)
('Epoch Training Acc:', 1.0)
('test loss', 0.94081903)
('test Acc:', 0.78652924)
model_080 F1_score: 82.58% >>>
31
('Epoch Training Loss:', 0.12571247384767048)
('Epoch Training Acc:', 1.0)
('test loss', 0.94484347)
('test Acc:', 0.78763342)
model_080 F1_score: 82.74% >>>
32
('Epoch Training Loss:', 0.12211674993159249)
('Epoch Training Acc:', 1.0)
('test loss', 0.93964148)
('test Acc:', 0.79241812)
model_080 F1_score: 83.01% >>>
33
('Epoch Training Loss:', 0.11465852198307402)
('Epoch Training Acc:', 1.0)
('test loss', 0.9388482)
('test Acc:', 0.78947371)
model_080 F1_score: 82.86% >>>
34
('Epoch Training Loss:', 0.10814734917948954)
('Epoch Training Acc:', 1.0)
('test loss', 0.93900925)
('test Acc:', 0.7868973)
model_080 F1_score: 82.59% >>>
35
('Epoch Training Loss:', 0.10560283888480626)
('Epoch Training Acc:', 1.0)
('test loss', 0.9515847)
('test Acc:', 0.78579313)
model_080 F1_score: 82.65% >>>
36
('Epoch Training Loss:', 0.10205706662964076)
('Epoch Training Acc:', 1.0)
('test loss', 0.95372462)
('test Acc:', 0.791682)
model_080 F1_score: 82.79% >>>
37
('Epoch Training Loss:', 0.10083794471574947)
('Epoch Training Acc:', 1.0)
('test loss', 0.96148384)
('test Acc:', 0.78726536)
model_080 F1_score: 82.59% >>>
38
('Epoch Training Loss:', 0.097006670257542282)
('Epoch Training Acc:', 1.0)
('test loss', 0.94175595)
('test Acc:', 0.78726536)
model_080 F1_score: 82.65% >>>
39
('Epoch Training Loss:', 0.095113139483146369)
('Epoch Training Acc:', 1.0)
('test loss', 0.96487582)
('test Acc:', 0.79425836)
model_080 F1_score: 83.16% >>>
40
('Epoch Training Loss:', 0.090883155731717125)
('Epoch Training Acc:', 1.0)
('test loss', 0.94922107)
('test Acc:', 0.79131395)
model_080 F1_score: 82.92% >>>
41
('Epoch Training Loss:', 0.087365038401912898)
('Epoch Training Acc:', 1.0)
('test loss', 0.96332014)
('test Acc:', 0.79278618)
model_080 F1_score: 83.11% >>>
42
('Epoch Training Loss:', 0.085599222773453221)
('Epoch Training Acc:', 1.0)
('test loss', 0.96316844)
('test Acc:', 0.79020977)
model_080 F1_score: 82.74% >>>
43
('Epoch Training Loss:', 0.086086840645293705)
('Epoch Training Acc:', 1.0)
('test loss', 0.96473914)
('test Acc:', 0.79315424)
model_080 F1_score: 83.10% >>>
44
('Epoch Training Loss:', 0.080963608212186955)
('Epoch Training Acc:', 1.0)
('test loss', 0.9580971)
('test Acc:', 0.79315424)
model_080 F1_score: 82.99% >>>
45
('Epoch Training Loss:', 0.080687656212830916)
('Epoch Training Acc:', 1.0)
('test loss', 0.966515)
('test Acc:', 0.79205006)
model_080 F1_score: 82.81% >>>
46
('Epoch Training Loss:', 0.076980525336693972)
('Epoch Training Acc:', 1.0)
('test loss', 0.97825187)
('test Acc:', 0.78579313)
model_080 F1_score: 82.57% >>>
47
('Epoch Training Loss:', 0.075834299423149787)
('Epoch Training Acc:', 1.0)
('test loss', 0.97976303)
('test Acc:', 0.78579313)
model_080 F1_score: 82.62% >>>
48
('Epoch Training Loss:', 0.074573242964106612)
('Epoch Training Acc:', 1.0)
('test loss', 0.97194761)
('test Acc:', 0.79131395)
model_080 F1_score: 82.97% >>>
49
('Epoch Training Loss:', 0.074712878951686434)
('Epoch Training Acc:', 1.0)
('test loss', 0.97699374)
('test Acc:', 0.791682)
model_080 F1_score: 82.91% >>>
50
('Epoch Training Loss:', 0.072325165107031353)
('Epoch Training Acc:', 1.0)
('test loss', 0.9760139)
('test Acc:', 0.78800148)
model_080 F1_score: 82.45% >>>
51
('Epoch Training Loss:', 0.071069547018851154)
('Epoch Training Acc:', 1.0)
('test loss', 0.97655261)
('test Acc:', 0.78910565)
model_080 F1_score: 82.80% >>>
52
('Epoch Training Loss:', 0.067063599592074752)
('Epoch Training Acc:', 1.0)
('test loss', 0.98275614)
('test Acc:', 0.78984171)
model_080 F1_score: 82.87% >>>
53
('Epoch Training Loss:', 0.065025038966268767)
('Epoch Training Acc:', 1.0)
('test loss', 0.98200178)
('test Acc:', 0.78579313)
model_080 F1_score: 82.57% >>>
54
('Epoch Training Loss:', 0.066253838362172246)
('Epoch Training Acc:', 1.0)
('test loss', 0.98449254)
('test Acc:', 0.79278618)
model_080 F1_score: 82.89% >>>
55
('Epoch Training Loss:', 0.062883215650799684)
('Epoch Training Acc:', 1.0)
('test loss', 0.98800755)
('test Acc:', 0.78910565)
model_080 F1_score: 82.76% >>>
56
('Epoch Training Loss:', 0.063837856039754115)
('Epoch Training Acc:', 1.0)
('test loss', 0.98366612)
('test Acc:', 0.79609865)
model_080 F1_score: 83.32% >>>
57
('Epoch Training Loss:', 0.063964613727875985)
('Epoch Training Acc:', 1.0)
('test loss', 1.000563)
('test Acc:', 0.7938903)
model_080 F1_score: 83.48% >>>
58
('Epoch Training Loss:', 0.061588040858623572)
('Epoch Training Acc:', 1.0)
('test loss', 0.98941541)
('test Acc:', 0.7868973)
model_080 F1_score: 82.81% >>>
59
('Epoch Training Loss:', 0.060900998985744081)
('Epoch Training Acc:', 1.0)
('test loss', 0.98490614)
('test Acc:', 0.79573059)
model_080 F1_score: 83.33% >>>
60
('Epoch Training Loss:', 0.059406955711892806)
('Epoch Training Acc:', 1.0)
('test loss', 0.99291909)
('test Acc:', 0.79462641)
model_080 F1_score: 83.32% >>>
61
('Epoch Training Loss:', 0.057814799249172211)
('Epoch Training Acc:', 1.0)
('test loss', 0.99473065)
('test Acc:', 0.79499447)
model_080 F1_score: 83.04% >>>
62
('Epoch Training Loss:', 0.056102272181306034)
('Epoch Training Acc:', 1.0)
('test loss', 1.0024313)
('test Acc:', 0.79131395)
model_080 F1_score: 82.92% >>>
63
('Epoch Training Loss:', 0.052816128882113844)
('Epoch Training Acc:', 1.0)
('test loss', 0.99840218)
('test Acc:', 0.7887376)
model_080 F1_score: 82.72% >>>
64
('Epoch Training Loss:', 0.054042745585320517)
('Epoch Training Acc:', 1.0)
('test loss', 1.0038983)
('test Acc:', 0.79205006)
model_080 F1_score: 83.25% >>>
65
('Epoch Training Loss:', 0.051320083934115246)
('Epoch Training Acc:', 1.0)
('test loss', 1.0029845)
('test Acc:', 0.78947371)
model_080 F1_score: 83.04% >>>
66
('Epoch Training Loss:', 0.051334446528926492)
('Epoch Training Acc:', 1.0)
('test loss', 1.019892)
('test Acc:', 0.78984171)
model_080 F1_score: 83.01% >>>
67
('Epoch Training Loss:', 0.053505523523199372)
('Epoch Training Acc:', 1.0)
('test loss', 1.0083003)
('test Acc:', 0.79131395)
model_080 F1_score: 82.95% >>>
68
('Epoch Training Loss:', 0.051783534829155542)
('Epoch Training Acc:', 1.0)
('test loss', 0.99926138)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
69
('Epoch Training Loss:', 0.048880232599913143)
('Epoch Training Acc:', 1.0)
('test loss', 1.0034305)
('test Acc:', 0.791682)
model_080 F1_score: 82.93% >>>
70
('Epoch Training Loss:', 0.046656790145789273)
('Epoch Training Acc:', 1.0)
('test loss', 1.0102354)
('test Acc:', 0.78910565)
model_080 F1_score: 82.76% >>>
71
('Epoch Training Loss:', 0.048071497745695524)
('Epoch Training Acc:', 1.0)
('test loss', 1.011088)
('test Acc:', 0.7868973)
model_080 F1_score: 82.57% >>>
72
('Epoch Training Loss:', 0.048493718240933958)
('Epoch Training Acc:', 1.0)
('test loss', 1.0014883)
('test Acc:', 0.78616118)
model_080 F1_score: 82.45% >>>
73
('Epoch Training Loss:', 0.045521426640334539)
('Epoch Training Acc:', 1.0)
('test loss', 1.0012082)
('test Acc:', 0.79205006)
model_080 F1_score: 82.92% >>>
74
('Epoch Training Loss:', 0.046937234161305241)
('Epoch Training Acc:', 1.0)
('test loss', 1.0102495)
('test Acc:', 0.78726536)
model_080 F1_score: 82.73% >>>
75
('Epoch Training Loss:', 0.045562984174466692)
('Epoch Training Acc:', 1.0)
('test loss', 1.0162679)
('test Acc:', 0.78947371)
model_080 F1_score: 82.70% >>>
76
('Epoch Training Loss:', 0.043398171532317065)
('Epoch Training Acc:', 1.0)
('test loss', 1.0256569)
('test Acc:', 0.78984171)
model_080 F1_score: 83.03% >>>
77
('Epoch Training Loss:', 0.04341544601629721)
('Epoch Training Acc:', 1.0)
('test loss', 1.0050229)
('test Acc:', 0.78836954)
model_080 F1_score: 82.77% >>>
78
('Epoch Training Loss:', 0.042550755890260916)
('Epoch Training Acc:', 1.0)
('test loss', 1.0143062)
('test Acc:', 0.78800148)
model_080 F1_score: 82.68% >>>
79
('Epoch Training Loss:', 0.041510343828122132)
('Epoch Training Acc:', 1.0)
('test loss', 1.0206497)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
80
('Epoch Training Loss:', 0.040950189097202383)
('Epoch Training Acc:', 1.0)
('test loss', 1.0149703)
('test Acc:', 0.78947371)
model_080 F1_score: 82.78% >>>
81
('Epoch Training Loss:', 0.040107996792357881)
('Epoch Training Acc:', 1.0)
('test loss', 1.0210806)
('test Acc:', 0.78947371)
model_080 F1_score: 82.82% >>>
82
('Epoch Training Loss:', 0.041118119508610107)
('Epoch Training Acc:', 1.0)
('test loss', 1.0149947)
('test Acc:', 0.7887376)
model_080 F1_score: 82.70% >>>
83
('Epoch Training Loss:', 0.040379592843237333)
('Epoch Training Acc:', 1.0)
('test loss', 1.0103775)
('test Acc:', 0.79315424)
model_080 F1_score: 83.08% >>>
84
('Epoch Training Loss:', 0.039860343145846855)
('Epoch Training Acc:', 1.0)
('test loss', 1.0210053)
('test Acc:', 0.791682)
model_080 F1_score: 83.20% >>>
85
('Epoch Training Loss:', 0.039032398912240751)
('Epoch Training Acc:', 1.0)
('test loss', 1.0296675)
('test Acc:', 0.78800148)
model_080 F1_score: 82.88% >>>
86
('Epoch Training Loss:', 0.039231016460689716)
('Epoch Training Acc:', 1.0)
('test loss', 1.0437444)
('test Acc:', 0.78726536)
model_080 F1_score: 82.73% >>>
87
('Epoch Training Loss:', 0.038716991926776245)
('Epoch Training Acc:', 1.0)
('test loss', 1.035273)
('test Acc:', 0.79094589)
model_080 F1_score: 82.97% >>>
88
('Epoch Training Loss:', 0.036494473890343215)
('Epoch Training Acc:', 1.0)
('test loss', 1.0300908)
('test Acc:', 0.79241812)
model_080 F1_score: 83.25% >>>
89
('Epoch Training Loss:', 0.037280231896147598)
('Epoch Training Acc:', 1.0)
('test loss', 1.0283927)
('test Acc:', 0.78984171)
model_080 F1_score: 82.82% >>>
90
('Epoch Training Loss:', 0.036341858358355239)
('Epoch Training Acc:', 1.0)
('test loss', 1.037825)
('test Acc:', 0.79057783)
model_080 F1_score: 83.10% >>>
91
('Epoch Training Loss:', 0.035812774280202575)
('Epoch Training Acc:', 1.0)
('test loss', 1.0214461)
('test Acc:', 0.79094589)
model_080 F1_score: 82.88% >>>
92
('Epoch Training Loss:', 0.035723928966035601)
('Epoch Training Acc:', 1.0)
('test loss', 1.0406545)
('test Acc:', 0.78984171)
model_080 F1_score: 82.99% >>>
93
('Epoch Training Loss:', 0.035441218853520695)
('Epoch Training Acc:', 1.0)
('test loss', 1.0334102)
('test Acc:', 0.78763342)
model_080 F1_score: 82.88% >>>
94
('Epoch Training Loss:', 0.034872553114837501)
('Epoch Training Acc:', 1.0)
('test loss', 1.0301982)
('test Acc:', 0.78947371)
model_080 F1_score: 82.81% >>>
95
('Epoch Training Loss:', 0.034951901812746655)
('Epoch Training Acc:', 1.0)
('test loss', 1.0302761)
('test Acc:', 0.79057783)
model_080 F1_score: 83.02% >>>
96
('Epoch Training Loss:', 0.033560567695531063)
('Epoch Training Acc:', 1.0)
('test loss', 1.0178254)
('test Acc:', 0.78947371)
model_080 F1_score: 82.78% >>>
97
('Epoch Training Loss:', 0.033092783276515547)
('Epoch Training Acc:', 1.0)
('test loss', 1.0401201)
('test Acc:', 0.78800148)
model_080 F1_score: 82.60% >>>
98
('Epoch Training Loss:', 0.034197738532384392)
('Epoch Training Acc:', 1.0)
('test loss', 1.0367689)
('test Acc:', 0.78726536)
model_080 F1_score: 82.66% >>>
99
('Epoch Training Loss:', 0.03184128412976861)
('Epoch Training Acc:', 1.0)
('test loss', 1.0400962)
('test Acc:', 0.78432095)
model_080 F1_score: 82.36% >>>
100
('Epoch Training Loss:', 0.032901077283895575)
('Epoch Training Acc:', 1.0)
('test loss', 1.036803)
('test Acc:', 0.79499447)
model_080 F1_score: 83.34% >>>
101
('Epoch Training Loss:', 0.033266747857851442)
('Epoch Training Acc:', 1.0)
('test loss', 1.0340536)
('test Acc:', 0.78984171)
model_080 F1_score: 82.72% >>>
102
('Epoch Training Loss:', 0.032054773073468823)
('Epoch Training Acc:', 1.0)
('test loss', 1.0440155)
('test Acc:', 0.79131395)
model_080 F1_score: 83.11% >>>
103
('Epoch Training Loss:', 0.031069685232068878)
('Epoch Training Acc:', 1.0)
('test loss', 1.0433211)
('test Acc:', 0.7868973)
model_080 F1_score: 82.45% >>>
104
('Epoch Training Loss:', 0.030807975861534942)
('Epoch Training Acc:', 1.0)
('test loss', 1.0592144)
('test Acc:', 0.79315424)
model_080 F1_score: 83.25% >>>
105
('Epoch Training Loss:', 0.0310810683076852)
('Epoch Training Acc:', 1.0)
('test loss', 1.0383775)
('test Acc:', 0.79057783)
model_080 F1_score: 82.80% >>>
106
('Epoch Training Loss:', 0.029859151996788569)
('Epoch Training Acc:', 1.0)
('test loss', 1.0606723)
('test Acc:', 0.78726536)
model_080 F1_score: 82.52% >>>
107
('Epoch Training Loss:', 0.029956819700601045)
('Epoch Training Acc:', 1.0)
('test loss', 1.0367308)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
108
('Epoch Training Loss:', 0.028764961462002248)
('Epoch Training Acc:', 1.0)
('test loss', 1.0385123)
('test Acc:', 0.7868973)
model_080 F1_score: 82.66% >>>
109
('Epoch Training Loss:', 0.030867105066135991)
('Epoch Training Acc:', 1.0)
('test loss', 1.0397681)
('test Acc:', 0.7887376)
model_080 F1_score: 82.92% >>>
110
('Epoch Training Loss:', 0.029099173196300399)
('Epoch Training Acc:', 1.0)
('test loss', 1.0568863)
('test Acc:', 0.79094589)
model_080 F1_score: 83.09% >>>
111
('Epoch Training Loss:', 0.030666470156575087)
('Epoch Training Acc:', 1.0)
('test loss', 1.0452303)
('test Acc:', 0.78984171)
model_080 F1_score: 82.85% >>>
112
('Epoch Training Loss:', 0.028332633017271291)
('Epoch Training Acc:', 1.0)
('test loss', 1.0440773)
('test Acc:', 0.79057783)
model_080 F1_score: 82.94% >>>
113
('Epoch Training Loss:', 0.02892755159700755)
('Epoch Training Acc:', 1.0)
('test loss', 1.058651)
('test Acc:', 0.78579313)
model_080 F1_score: 82.59% >>>
114
('Epoch Training Loss:', 0.029119713995896745)
('Epoch Training Acc:', 1.0)
('test loss', 1.0669863)
('test Acc:', 0.79020977)
model_080 F1_score: 83.09% >>>
115
('Epoch Training Loss:', 0.028369943094730843)
('Epoch Training Acc:', 1.0)
('test loss', 1.042767)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
116
('Epoch Training Loss:', 0.028923108384333318)
('Epoch Training Acc:', 1.0)
('test loss', 1.0511186)
('test Acc:', 0.79020977)
model_080 F1_score: 82.76% >>>
117
('Epoch Training Loss:', 0.027783158329839353)
('Epoch Training Acc:', 1.0)
('test loss', 1.0489086)
('test Acc:', 0.79020977)
model_080 F1_score: 82.91% >>>
118
('Epoch Training Loss:', 0.026334239366406109)
('Epoch Training Acc:', 1.0)
('test loss', 1.0608076)
('test Acc:', 0.79094589)
model_080 F1_score: 82.87% >>>
119
('Epoch Training Loss:', 0.027031159675971139)
('Epoch Training Acc:', 1.0)
('test loss', 1.0658439)
('test Acc:', 0.79020977)
model_080 F1_score: 82.90% >>>
120
('Epoch Training Loss:', 0.026906921855697874)
('Epoch Training Acc:', 1.0)
('test loss', 1.0503628)
('test Acc:', 0.78652924)
model_080 F1_score: 82.67% >>>
121
('Epoch Training Loss:', 0.027914104895899072)
('Epoch Training Acc:', 1.0)
('test loss', 1.0525573)
('test Acc:', 0.78616118)
model_080 F1_score: 82.56% >>>
122
('Epoch Training Loss:', 0.027341615466866642)
('Epoch Training Acc:', 1.0)
('test loss', 1.0618368)
('test Acc:', 0.78763342)
model_080 F1_score: 82.82% >>>
123
('Epoch Training Loss:', 0.025750382199476007)
('Epoch Training Acc:', 1.0)
('test loss', 1.0655215)
('test Acc:', 0.79131395)
model_080 F1_score: 83.03% >>>
124
('Epoch Training Loss:', 0.026900548094999976)
('Epoch Training Acc:', 1.0)
('test loss', 1.0573126)
('test Acc:', 0.7887376)
model_080 F1_score: 82.75% >>>
125
('Epoch Training Loss:', 0.025474930393102113)
('Epoch Training Acc:', 1.0)
('test loss', 1.0654796)
('test Acc:', 0.78800148)
model_080 F1_score: 82.54% >>>
126
('Epoch Training Loss:', 0.02555884605681058)
('Epoch Training Acc:', 1.0)
('test loss', 1.0593314)
('test Acc:', 0.78910565)
model_080 F1_score: 82.72% >>>
127
('Epoch Training Loss:', 0.025257733374019153)
('Epoch Training Acc:', 1.0)
('test loss', 1.0719653)
('test Acc:', 0.78800148)
model_080 F1_score: 82.89% >>>
128
('Epoch Training Loss:', 0.025009727534779813)
('Epoch Training Acc:', 1.0)
('test loss', 1.0589213)
('test Acc:', 0.79020977)
model_080 F1_score: 82.79% >>>
129
('Epoch Training Loss:', 0.024175988044589758)
('Epoch Training Acc:', 1.0)
('test loss', 1.064697)
('test Acc:', 0.78800148)
model_080 F1_score: 82.75% >>>
130
('Epoch Training Loss:', 0.025119177036685869)
('Epoch Training Acc:', 1.0)
('test loss', 1.0522548)
('test Acc:', 0.79131395)
model_080 F1_score: 82.93% >>>
131
('Epoch Training Loss:', 0.024548202323785517)
('Epoch Training Acc:', 1.0)
('test loss', 1.0744969)
('test Acc:', 0.78652924)
model_080 F1_score: 82.66% >>>
132
('Epoch Training Loss:', 0.024425193598290207)
('Epoch Training Acc:', 1.0)
('test loss', 1.0730578)
('test Acc:', 0.78947371)
model_080 F1_score: 82.89% >>>
133
('Epoch Training Loss:', 0.023997026539291255)
('Epoch Training Acc:', 1.0)
('test loss', 1.0492626)
('test Acc:', 0.79057783)
model_080 F1_score: 83.09% >>>
134
('Epoch Training Loss:', 0.023620263134944253)
('Epoch Training Acc:', 1.0)
('test loss', 1.0714477)
('test Acc:', 0.79278618)
model_080 F1_score: 83.13% >>>
135
('Epoch Training Loss:', 0.023972018228960223)
('Epoch Training Acc:', 1.0)
('test loss', 1.0637455)
('test Acc:', 0.7868973)
model_080 F1_score: 82.68% >>>
136
('Epoch Training Loss:', 0.023224435120937414)
('Epoch Training Acc:', 1.0)
('test loss', 1.0846652)
('test Acc:', 0.79131395)
model_080 F1_score: 83.00% >>>
137
('Epoch Training Loss:', 0.022917887203220744)
('Epoch Training Acc:', 1.0)
('test loss', 1.0576342)
('test Acc:', 0.79094589)
model_080 F1_score: 82.99% >>>
138
('Epoch Training Loss:', 0.022249199297220912)
('Epoch Training Acc:', 1.0)
('test loss', 1.0791047)
('test Acc:', 0.79499447)
model_080 F1_score: 83.21% >>>
139
('Epoch Training Loss:', 0.023975103082193527)
('Epoch Training Acc:', 1.0)
('test loss', 1.0800085)
('test Acc:', 0.79094589)
model_080 F1_score: 83.05% >>>
140
('Epoch Training Loss:', 0.022613000230194302)
('Epoch Training Acc:', 1.0)
('test loss', 1.0869895)
('test Acc:', 0.78910565)
model_080 F1_score: 82.85% >>>
141
('Epoch Training Loss:', 0.022665379095997196)
('Epoch Training Acc:', 1.0)
('test loss', 1.0652517)
('test Acc:', 0.78836954)
model_080 F1_score: 82.65% >>>
142
('Epoch Training Loss:', 0.021980784644256346)
('Epoch Training Acc:', 1.0)
('test loss', 1.0894191)
('test Acc:', 0.78984171)
model_080 F1_score: 82.93% >>>
143
('Epoch Training Loss:', 0.021713301852287259)
('Epoch Training Acc:', 1.0)
('test loss', 1.0706805)
('test Acc:', 0.78652924)
model_080 F1_score: 82.67% >>>
144
('Epoch Training Loss:', 0.022839197616121965)
('Epoch Training Acc:', 1.0)
('test loss', 1.0754861)
('test Acc:', 0.78947371)
model_080 F1_score: 82.84% >>>
145
('Epoch Training Loss:', 0.021205422344792169)
('Epoch Training Acc:', 1.0)
('test loss', 1.0655864)
('test Acc:', 0.791682)
model_080 F1_score: 82.84% >>>
146
('Epoch Training Loss:', 0.022073160813306458)
('Epoch Training Acc:', 1.0)
('test loss', 1.0733972)
('test Acc:', 0.79131395)
model_080 F1_score: 82.89% >>>
147
('Epoch Training Loss:', 0.021413894082797924)
('Epoch Training Acc:', 1.0)
('test loss', 1.0690522)
('test Acc:', 0.79131395)
model_080 F1_score: 82.85% >>>
148
('Epoch Training Loss:', 0.020686467854829971)
('Epoch Training Acc:', 1.0)
('test loss', 1.0917915)
('test Acc:', 0.79205006)
model_080 F1_score: 83.04% >>>
149
('Epoch Training Loss:', 0.021191628788074013)
('Epoch Training Acc:', 1.0)
('test loss', 1.0754151)
('test Acc:', 0.78763342)
model_080 F1_score: 82.74% >>>
150
('Epoch Training Loss:', 0.021237478384136921)
('Epoch Training Acc:', 1.0)
('test loss', 1.0679922)
('test Acc:', 0.79352224)
model_080 F1_score: 83.16% >>>
151
('Epoch Training Loss:', 0.022033073018974392)
('Epoch Training Acc:', 1.0)
('test loss', 1.0721278)
('test Acc:', 0.7938903)
model_080 F1_score: 83.13% >>>
152
('Epoch Training Loss:', 0.020985892246244475)
('Epoch Training Acc:', 1.0)
('test loss', 1.0706697)
('test Acc:', 0.79425836)
model_080 F1_score: 83.29% >>>
153
('Epoch Training Loss:', 0.020404123213666026)
('Epoch Training Acc:', 1.0)
('test loss', 1.0876296)
('test Acc:', 0.79425836)
model_080 F1_score: 83.32% >>>
154
('Epoch Training Loss:', 0.020394774128362769)
('Epoch Training Acc:', 1.0)
('test loss', 1.0821482)
('test Acc:', 0.79278618)
model_080 F1_score: 83.17% >>>
155
('Epoch Training Loss:', 0.020310405241616536)
('Epoch Training Acc:', 1.0)
('test loss', 1.0706704)
('test Acc:', 0.7868973)
model_080 F1_score: 82.80% >>>
156
('Epoch Training Loss:', 0.019718215702596353)
('Epoch Training Acc:', 1.0)
('test loss', 1.0724334)
('test Acc:', 0.79609865)
model_080 F1_score: 83.37% >>>
157
('Epoch Training Loss:', 0.019607770205766428)
('Epoch Training Acc:', 1.0)
('test loss', 1.0939165)
('test Acc:', 0.78579313)
model_080 F1_score: 82.48% >>>
158
('Epoch Training Loss:', 0.020073221650818596)
('Epoch Training Acc:', 1.0)
('test loss', 1.0750675)
('test Acc:', 0.79425836)
model_080 F1_score: 83.31% >>>
159
('Epoch Training Loss:', 0.019603434866439784)
('Epoch Training Acc:', 1.0)
('test loss', 1.077247)
('test Acc:', 0.79425836)
model_080 F1_score: 83.10% >>>
160
('Epoch Training Loss:', 0.019903171811165521)
('Epoch Training Acc:', 1.0)
('test loss', 1.0834966)
('test Acc:', 0.79094589)
model_080 F1_score: 82.89% >>>
161
('Epoch Training Loss:', 0.020559253742248984)
('Epoch Training Acc:', 1.0)
('test loss', 1.0541975)
('test Acc:', 0.79278618)
model_080 F1_score: 83.05% >>>
162
('Epoch Training Loss:', 0.018890908184403088)
('Epoch Training Acc:', 1.0)
('test loss', 1.0815648)
('test Acc:', 0.79057783)
model_080 F1_score: 82.85% >>>
163
('Epoch Training Loss:', 0.019188678295904538)
('Epoch Training Acc:', 1.0)
('test loss', 1.0932975)
('test Acc:', 0.79573059)
model_080 F1_score: 83.34% >>>
164
('Epoch Training Loss:', 0.019268774736701744)
('Epoch Training Acc:', 1.0)
('test loss', 1.0873649)
('test Acc:', 0.79205006)
model_080 F1_score: 83.02% >>>
165
('Epoch Training Loss:', 0.018908087004092522)
('Epoch Training Acc:', 1.0)
('test loss', 1.0811375)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
166
('Epoch Training Loss:', 0.018451443454978289)
('Epoch Training Acc:', 1.0)
('test loss', 1.0825752)
('test Acc:', 0.79241812)
model_080 F1_score: 83.10% >>>
167
('Epoch Training Loss:', 0.018636156717548147)
('Epoch Training Acc:', 1.0)
('test loss', 1.1050441)
('test Acc:', 0.79241812)
model_080 F1_score: 83.23% >>>
168
('Epoch Training Loss:', 0.01864224107703194)
('Epoch Training Acc:', 1.0)
('test loss', 1.0817502)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
169
('Epoch Training Loss:', 0.018568446601420874)
('Epoch Training Acc:', 1.0)
('test loss', 1.0866343)
('test Acc:', 0.78910565)
model_080 F1_score: 82.77% >>>
170
('Epoch Training Loss:', 0.017972476765862666)
('Epoch Training Acc:', 1.0)
('test loss', 1.0848461)
('test Acc:', 0.7938903)
model_080 F1_score: 83.25% >>>
171
('Epoch Training Loss:', 0.018413514564599609)
('Epoch Training Acc:', 1.0)
('test loss', 1.0915531)
('test Acc:', 0.78984171)
model_080 F1_score: 82.94% >>>
172
('Epoch Training Loss:', 0.018229480479931226)
('Epoch Training Acc:', 1.0)
('test loss', 1.0958058)
('test Acc:', 0.7887376)
model_080 F1_score: 82.69% >>>
173
('Epoch Training Loss:', 0.017816346873587463)
('Epoch Training Acc:', 1.0)
('test loss', 1.101439)
('test Acc:', 0.78910565)
model_080 F1_score: 83.02% >>>
174
('Epoch Training Loss:', 0.017229213604878169)
('Epoch Training Acc:', 1.0)
('test loss', 1.1048602)
('test Acc:', 0.78652924)
model_080 F1_score: 82.58% >>>
175
('Epoch Training Loss:', 0.017775403583073057)
('Epoch Training Acc:', 1.0)
('test loss', 1.0785289)
('test Acc:', 0.79646671)
model_080 F1_score: 83.32% >>>
176
('Epoch Training Loss:', 0.017994107245613122)
('Epoch Training Acc:', 1.0)
('test loss', 1.0904109)
('test Acc:', 0.79057783)
model_080 F1_score: 83.04% >>>
177
('Epoch Training Loss:', 0.017282323409745004)
('Epoch Training Acc:', 1.0)
('test loss', 1.0882506)
('test Acc:', 0.7887376)
model_080 F1_score: 82.65% >>>
178
('Epoch Training Loss:', 0.017296247304329881)
('Epoch Training Acc:', 1.0)
('test loss', 1.0870885)
('test Acc:', 0.7868973)
model_080 F1_score: 82.54% >>>
179
('Epoch Training Loss:', 0.017229955086804694)
('Epoch Training Acc:', 1.0)
('test loss', 1.0882159)
('test Acc:', 0.78542513)
model_080 F1_score: 82.46% >>>
180
('Epoch Training Loss:', 0.017332362989691319)
('Epoch Training Acc:', 1.0)
('test loss', 1.0817058)
('test Acc:', 0.79205006)
model_080 F1_score: 83.09% >>>
181
('Epoch Training Loss:', 0.016762649025622522)
('Epoch Training Acc:', 1.0)
('test loss', 1.0891947)
('test Acc:', 0.79315424)
model_080 F1_score: 82.99% >>>
182
('Epoch Training Loss:', 0.016014005661418196)
('Epoch Training Acc:', 1.0)
('test loss', 1.0962566)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
183
('Epoch Training Loss:', 0.017002777432935545)
('Epoch Training Acc:', 1.0)
('test loss', 1.0954655)
('test Acc:', 0.78800148)
model_080 F1_score: 82.85% >>>
184
('Epoch Training Loss:', 0.015941695361107122)
('Epoch Training Acc:', 1.0)
('test loss', 1.1113151)
('test Acc:', 0.78984171)
model_080 F1_score: 82.92% >>>
185
('Epoch Training Loss:', 0.017416810860595433)
('Epoch Training Acc:', 1.0)
('test loss', 1.0916088)
('test Acc:', 0.79462641)
model_080 F1_score: 83.23% >>>
186
('Epoch Training Loss:', 0.016207369502808433)
('Epoch Training Acc:', 1.0)
('test loss', 1.0994475)
('test Acc:', 0.79094589)
model_080 F1_score: 83.06% >>>
187
('Epoch Training Loss:', 0.016506899879459525)
('Epoch Training Acc:', 1.0)
('test loss', 1.1084214)
('test Acc:', 0.78579313)
model_080 F1_score: 82.50% >>>
188
('Epoch Training Loss:', 0.017300374936894514)
('Epoch Training Acc:', 1.0)
('test loss', 1.1178147)
('test Acc:', 0.78579313)
model_080 F1_score: 82.62% >>>
189
('Epoch Training Loss:', 0.016472550581966061)
('Epoch Training Acc:', 1.0)
('test loss', 1.0942465)
('test Acc:', 0.79536253)
model_080 F1_score: 83.24% >>>
190
('Epoch Training Loss:', 0.016762525654485216)
('Epoch Training Acc:', 1.0)
('test loss', 1.0996485)
('test Acc:', 0.79094589)
model_080 F1_score: 82.91% >>>
191
('Epoch Training Loss:', 0.01609347729026922)
('Epoch Training Acc:', 1.0)
('test loss', 1.1019553)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
192
('Epoch Training Loss:', 0.015490492161916336)
('Epoch Training Acc:', 1.0)
('test loss', 1.1121548)
('test Acc:', 0.7868973)
model_080 F1_score: 82.77% >>>
193
('Epoch Training Loss:', 0.015467488694412168)
('Epoch Training Acc:', 1.0)
('test loss', 1.0853111)
('test Acc:', 0.79462641)
model_080 F1_score: 83.24% >>>
194
('Epoch Training Loss:', 0.015357855667389231)
('Epoch Training Acc:', 1.0)
('test loss', 1.1055864)
('test Acc:', 0.78284872)
model_080 F1_score: 82.37% >>>
195
('Epoch Training Loss:', 0.016154007164004724)
('Epoch Training Acc:', 1.0)
('test loss', 1.0961556)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
196
('Epoch Training Loss:', 0.015095897218998289)
('Epoch Training Acc:', 1.0)
('test loss', 1.1090217)
('test Acc:', 0.78984171)
model_080 F1_score: 82.91% >>>
197
('Epoch Training Loss:', 0.015964073896611808)
('Epoch Training Acc:', 1.0)
('test loss', 1.0975167)
('test Acc:', 0.78984171)
model_080 F1_score: 82.92% >>>
198
('Epoch Training Loss:', 0.015262379783962388)
('Epoch Training Acc:', 1.0)
('test loss', 1.0980403)
('test Acc:', 0.79020977)
model_080 F1_score: 82.98% >>>
199
('Epoch Training Loss:', 0.015211653982987627)
('Epoch Training Acc:', 1.0)
('test loss', 1.1050539)
('test Acc:', 0.79057783)
model_080 F1_score: 82.77% >>>
200
('Epoch Training Loss:', 0.015003226537373848)
('Epoch Training Acc:', 1.0)
('test loss', 1.1273224)
('test Acc:', 0.791682)
model_080 F1_score: 83.07% >>>
201
('Epoch Training Loss:', 0.015440382194356062)
('Epoch Training Acc:', 1.0)
('test loss', 1.0844309)
('test Acc:', 0.7887376)
model_080 F1_score: 82.58% >>>
202
('Epoch Training Loss:', 0.015056394502607873)
('Epoch Training Acc:', 1.0)
('test loss', 1.1185673)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
203
('Epoch Training Loss:', 0.014864095770462882)
('Epoch Training Acc:', 1.0)
('test loss', 1.1051625)
('test Acc:', 0.791682)
model_080 F1_score: 83.16% >>>
204
('Epoch Training Loss:', 0.014562618824129459)
('Epoch Training Acc:', 1.0)
('test loss', 1.1170446)
('test Acc:', 0.78910565)
model_080 F1_score: 82.99% >>>
205
('Epoch Training Loss:', 0.014720491901243804)
('Epoch Training Acc:', 1.0)
('test loss', 1.0933633)
('test Acc:', 0.79315424)
model_080 F1_score: 83.23% >>>
206
('Epoch Training Loss:', 0.015064007613545982)
('Epoch Training Acc:', 1.0)
('test loss', 1.0939702)
('test Acc:', 0.791682)
model_080 F1_score: 83.23% >>>
207
('Epoch Training Loss:', 0.014465296917478554)
('Epoch Training Acc:', 1.0)
('test loss', 1.124819)
('test Acc:', 0.78763342)
model_080 F1_score: 82.63% >>>
208
('Epoch Training Loss:', 0.014405356811039383)
('Epoch Training Acc:', 1.0)
('test loss', 1.1172326)
('test Acc:', 0.78984171)
model_080 F1_score: 82.89% >>>
209
('Epoch Training Loss:', 0.015443680484167999)
('Epoch Training Acc:', 1.0)
('test loss', 1.0963646)
('test Acc:', 0.79683477)
model_080 F1_score: 83.48% >>>
210
('Epoch Training Loss:', 0.014230823930120096)
('Epoch Training Acc:', 1.0)
('test loss', 1.1253066)
('test Acc:', 0.78947371)
model_080 F1_score: 83.01% >>>
211
('Epoch Training Loss:', 0.014687989350932185)
('Epoch Training Acc:', 1.0)
('test loss', 1.1119882)
('test Acc:', 0.79352224)
model_080 F1_score: 83.05% >>>
212
('Epoch Training Loss:', 0.014665049071481917)
('Epoch Training Acc:', 1.0)
('test loss', 1.1047641)
('test Acc:', 0.78984171)
model_080 F1_score: 82.93% >>>
213
('Epoch Training Loss:', 0.014478254986897809)
('Epoch Training Acc:', 1.0)
('test loss', 1.1251061)
('test Acc:', 0.78616118)
model_080 F1_score: 82.51% >>>
214
('Epoch Training Loss:', 0.014073928552534198)
('Epoch Training Acc:', 1.0)
('test loss', 1.1008525)
('test Acc:', 0.78800148)
model_080 F1_score: 82.82% >>>
215
('Epoch Training Loss:', 0.014014678527018987)
('Epoch Training Acc:', 1.0)
('test loss', 1.1199521)
('test Acc:', 0.79057783)
model_080 F1_score: 83.01% >>>
216
('Epoch Training Loss:', 0.014139902541501215)
('Epoch Training Acc:', 1.0)
('test loss', 1.1157464)
('test Acc:', 0.79094589)
model_080 F1_score: 82.97% >>>
217
('Epoch Training Loss:', 0.013703033720958047)
('Epoch Training Acc:', 1.0)
('test loss', 1.1188048)
('test Acc:', 0.79057783)
model_080 F1_score: 82.93% >>>
218
('Epoch Training Loss:', 0.0141004206816433)
('Epoch Training Acc:', 1.0)
('test loss', 1.1100019)
('test Acc:', 0.79057783)
model_080 F1_score: 82.82% >>>
219
('Epoch Training Loss:', 0.014077719195483951)
('Epoch Training Acc:', 1.0)
('test loss', 1.1087458)
('test Acc:', 0.79241812)
model_080 F1_score: 83.05% >>>
220
('Epoch Training Loss:', 0.013450843056489248)
('Epoch Training Acc:', 1.0)
('test loss', 1.1124271)
('test Acc:', 0.7868973)
model_080 F1_score: 82.43% >>>
221
('Epoch Training Loss:', 0.013685722533409717)
('Epoch Training Acc:', 1.0)
('test loss', 1.1075516)
('test Acc:', 0.78910565)
model_080 F1_score: 82.78% >>>
222
('Epoch Training Loss:', 0.013352061858313391)
('Epoch Training Acc:', 1.0)
('test loss', 1.1286606)
('test Acc:', 0.78910565)
model_080 F1_score: 82.63% >>>
223
('Epoch Training Loss:', 0.014145803084829822)
('Epoch Training Acc:', 1.0)
('test loss', 1.1088287)
('test Acc:', 0.78763342)
model_080 F1_score: 82.71% >>>
224
('Epoch Training Loss:', 0.013309291753103025)
('Epoch Training Acc:', 1.0)
('test loss', 1.121238)
('test Acc:', 0.79131395)
model_080 F1_score: 83.06% >>>
225
('Epoch Training Loss:', 0.013395030084211612)
('Epoch Training Acc:', 1.0)
('test loss', 1.1140987)
('test Acc:', 0.79425836)
model_080 F1_score: 83.03% >>>
226
('Epoch Training Loss:', 0.013690884406969417)
('Epoch Training Acc:', 1.0)
('test loss', 1.1150466)
('test Acc:', 0.79094589)
model_080 F1_score: 82.91% >>>
227
('Epoch Training Loss:', 0.013188329732656712)
('Epoch Training Acc:', 1.0)
('test loss', 1.1065798)
('test Acc:', 0.79352224)
model_080 F1_score: 83.10% >>>
228
('Epoch Training Loss:', 0.012988278384000296)
('Epoch Training Acc:', 1.0)
('test loss', 1.1213923)
('test Acc:', 0.79131395)
model_080 F1_score: 83.01% >>>
229
('Epoch Training Loss:', 0.013066724433883792)
('Epoch Training Acc:', 1.0)
('test loss', 1.1205442)
('test Acc:', 0.78800148)
model_080 F1_score: 82.73% >>>
230
('Epoch Training Loss:', 0.01259304151244578)
('Epoch Training Acc:', 1.0)
('test loss', 1.1141362)
('test Acc:', 0.79020977)
model_080 F1_score: 82.76% >>>
231
('Epoch Training Loss:', 0.013125464958648081)
('Epoch Training Acc:', 1.0)
('test loss', 1.1269859)
('test Acc:', 0.79315424)
model_080 F1_score: 83.12% >>>
232
('Epoch Training Loss:', 0.013768197128229076)
('Epoch Training Acc:', 1.0)
('test loss', 1.110889)
('test Acc:', 0.79094589)
model_080 F1_score: 83.16% >>>
233
('Epoch Training Loss:', 0.013145054446795257)
('Epoch Training Acc:', 1.0)
('test loss', 1.1269156)
('test Acc:', 0.78800148)
model_080 F1_score: 82.62% >>>
234
('Epoch Training Loss:', 0.012694567005382851)
('Epoch Training Acc:', 1.0)
('test loss', 1.1263462)
('test Acc:', 0.79094589)
model_080 F1_score: 83.17% >>>
235
('Epoch Training Loss:', 0.01238169217685936)
('Epoch Training Acc:', 1.0)
('test loss', 1.1225585)
('test Acc:', 0.78616118)
model_080 F1_score: 82.67% >>>
236
('Epoch Training Loss:', 0.012975978654139908)
('Epoch Training Acc:', 1.0)
('test loss', 1.117892)
('test Acc:', 0.78800148)
model_080 F1_score: 82.65% >>>
237
('Epoch Training Loss:', 0.01225397170674114)
('Epoch Training Acc:', 1.0)
('test loss', 1.1147145)
('test Acc:', 0.79757082)
model_080 F1_score: 83.63% >>>
238
('Epoch Training Loss:', 0.012277442703634733)
('Epoch Training Acc:', 1.0)
('test loss', 1.1446834)
('test Acc:', 0.78726536)
model_080 F1_score: 82.72% >>>
239
('Epoch Training Loss:', 0.01254300819709897)
('Epoch Training Acc:', 1.0)
('test loss', 1.122257)
('test Acc:', 0.79205006)
model_080 F1_score: 83.00% >>>
240
('Epoch Training Loss:', 0.012457742930564564)
('Epoch Training Acc:', 1.0)
('test loss', 1.1134051)
('test Acc:', 0.78947371)
model_080 F1_score: 82.67% >>>
241
('Epoch Training Loss:', 0.012398743927406031)
('Epoch Training Acc:', 1.0)
('test loss', 1.1303722)
('test Acc:', 0.79094589)
model_080 F1_score: 82.87% >>>
242
('Epoch Training Loss:', 0.012020950049191015)
('Epoch Training Acc:', 1.0)
('test loss', 1.1162971)
('test Acc:', 0.78947371)
model_080 F1_score: 82.72% >>>
243
('Epoch Training Loss:', 0.013034180632530479)
('Epoch Training Acc:', 1.0)
('test loss', 1.1140946)
('test Acc:', 0.7887376)
model_080 F1_score: 82.97% >>>
244
('Epoch Training Loss:', 0.01195517440646654)
('Epoch Training Acc:', 1.0)
('test loss', 1.1065809)
('test Acc:', 0.78984171)
model_080 F1_score: 83.04% >>>
245
('Epoch Training Loss:', 0.012221679344293079)
('Epoch Training Acc:', 1.0)
('test loss', 1.1261408)
('test Acc:', 0.79020977)
model_080 F1_score: 82.84% >>>
246
('Epoch Training Loss:', 0.012627108528249664)
('Epoch Training Acc:', 1.0)
('test loss', 1.1178643)
('test Acc:', 0.7887376)
model_080 F1_score: 82.74% >>>
247
('Epoch Training Loss:', 0.011831676642032107)
('Epoch Training Acc:', 1.0)
('test loss', 1.1321013)
('test Acc:', 0.78542513)
model_080 F1_score: 82.59% >>>
248
('Epoch Training Loss:', 0.011933408321056049)
('Epoch Training Acc:', 1.0)
('test loss', 1.1205084)
('test Acc:', 0.79057783)
model_080 F1_score: 82.81% >>>
249
('Epoch Training Loss:', 0.012097094733690028)
('Epoch Training Acc:', 1.0)
('test loss', 1.1037328)
('test Acc:', 0.79462641)
model_080 F1_score: 83.35% >>>
250
('Epoch Training Loss:', 0.012607779044628842)
('Epoch Training Acc:', 1.0)
('test loss', 1.1176959)
('test Acc:', 0.78910565)
model_080 F1_score: 82.85% >>>
251
('Epoch Training Loss:', 0.011927533701964421)
('Epoch Training Acc:', 1.0)
('test loss', 1.1376669)
('test Acc:', 0.78763342)
model_080 F1_score: 82.62% >>>
252
('Epoch Training Loss:', 0.011600210107644671)
('Epoch Training Acc:', 1.0)
('test loss', 1.1265965)
('test Acc:', 0.79278618)
model_080 F1_score: 83.24% >>>
253
('Epoch Training Loss:', 0.011600973009990412)
('Epoch Training Acc:', 1.0)
('test loss', 1.1306782)
('test Acc:', 0.79020977)
model_080 F1_score: 82.91% >>>
254
('Epoch Training Loss:', 0.01191223118439666)
('Epoch Training Acc:', 1.0)
('test loss', 1.1137516)
('test Acc:', 0.79315424)
model_080 F1_score: 83.04% >>>
255
('Epoch Training Loss:', 0.011401142925024033)
('Epoch Training Acc:', 1.0)
('test loss', 1.1339059)
('test Acc:', 0.78652924)
model_080 F1_score: 82.63% >>>
256
('Epoch Training Loss:', 0.011840173074233462)
('Epoch Training Acc:', 1.0)
('test loss', 1.1225388)
('test Acc:', 0.79241812)
model_080 F1_score: 83.39% >>>
257
('Epoch Training Loss:', 0.011701807092322269)
('Epoch Training Acc:', 1.0)
('test loss', 1.1489692)
('test Acc:', 0.78432095)
model_080 F1_score: 82.46% >>>
258
('Epoch Training Loss:', 0.011756954190786928)
('Epoch Training Acc:', 1.0)
('test loss', 1.1204321)
('test Acc:', 0.78947371)
model_080 F1_score: 82.83% >>>
259
('Epoch Training Loss:', 0.011594427827731124)
('Epoch Training Acc:', 1.0)
('test loss', 1.1313571)
('test Acc:', 0.78579313)
model_080 F1_score: 82.42% >>>
260
('Epoch Training Loss:', 0.011616976426012116)
('Epoch Training Acc:', 1.0)
('test loss', 1.1378353)
('test Acc:', 0.79536253)
model_080 F1_score: 83.39% >>>
261
('Epoch Training Loss:', 0.011766152507334482)
('Epoch Training Acc:', 1.0)
('test loss', 1.1274672)
('test Acc:', 0.78763342)
model_080 F1_score: 82.79% >>>
262
('Epoch Training Loss:', 0.011424669308325974)
('Epoch Training Acc:', 1.0)
('test loss', 1.1257192)
('test Acc:', 0.78910565)
model_080 F1_score: 82.74% >>>
263
('Epoch Training Loss:', 0.011688185964885633)
('Epoch Training Acc:', 1.0)
('test loss', 1.1412826)
('test Acc:', 0.78616118)
model_080 F1_score: 82.79% >>>
264
('Epoch Training Loss:', 0.011397780606785091)
('Epoch Training Acc:', 1.0)
('test loss', 1.135177)
('test Acc:', 0.78947371)
model_080 F1_score: 82.87% >>>
265
('Epoch Training Loss:', 0.011649631307591335)
('Epoch Training Acc:', 1.0)
('test loss', 1.1417494)
('test Acc:', 0.78984171)
model_080 F1_score: 82.97% >>>
266
('Epoch Training Loss:', 0.011068328914916492)
('Epoch Training Acc:', 1.0)
('test loss', 1.1337141)
('test Acc:', 0.79094589)
model_080 F1_score: 82.87% >>>
267
('Epoch Training Loss:', 0.011145119791763136)
('Epoch Training Acc:', 1.0)
('test loss', 1.1290363)
('test Acc:', 0.79499447)
model_080 F1_score: 83.26% >>>
268
('Epoch Training Loss:', 0.0113152625872317)
('Epoch Training Acc:', 1.0)
('test loss', 1.1298171)
('test Acc:', 0.78947371)
model_080 F1_score: 82.85% >>>
269
('Epoch Training Loss:', 0.010620369437674526)
('Epoch Training Acc:', 1.0)
('test loss', 1.1454979)
('test Acc:', 0.7868973)
model_080 F1_score: 82.57% >>>
270
('Epoch Training Loss:', 0.010932733752269996)
('Epoch Training Acc:', 1.0)
('test loss', 1.1313552)
('test Acc:', 0.78763342)
model_080 F1_score: 82.81% >>>
271
('Epoch Training Loss:', 0.010956113359497976)
('Epoch Training Acc:', 1.0)
('test loss', 1.1469669)
('test Acc:', 0.7887376)
model_080 F1_score: 82.95% >>>
272
('Epoch Training Loss:', 0.010682176580303349)
('Epoch Training Acc:', 1.0)
('test loss', 1.1432797)
('test Acc:', 0.78726536)
model_080 F1_score: 82.70% >>>
273
('Epoch Training Loss:', 0.01100513164055883)
('Epoch Training Acc:', 1.0)
('test loss', 1.1315482)
('test Acc:', 0.79462641)
model_080 F1_score: 83.21% >>>
274
('Epoch Training Loss:', 0.010620375207508914)
('Epoch Training Acc:', 1.0)
('test loss', 1.1159269)
('test Acc:', 0.78947371)
model_080 F1_score: 82.91% >>>
275
('Epoch Training Loss:', 0.010611937057547038)
('Epoch Training Acc:', 1.0)
('test loss', 1.136173)
('test Acc:', 0.7887376)
model_080 F1_score: 82.76% >>>
276
('Epoch Training Loss:', 0.011121583524072776)
('Epoch Training Acc:', 1.0)
('test loss', 1.1165031)
('test Acc:', 0.78984171)
model_080 F1_score: 82.92% >>>
277
('Epoch Training Loss:', 0.010732045526310685)
('Epoch Training Acc:', 1.0)
('test loss', 1.157988)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
278
('Epoch Training Loss:', 0.010779587495562737)
('Epoch Training Acc:', 1.0)
('test loss', 1.1443778)
('test Acc:', 0.78800148)
model_080 F1_score: 82.85% >>>
279
('Epoch Training Loss:', 0.01061102315907192)
('Epoch Training Acc:', 1.0)
('test loss', 1.1424797)
('test Acc:', 0.79094589)
model_080 F1_score: 83.08% >>>
280
('Epoch Training Loss:', 0.011114079687104095)
('Epoch Training Acc:', 1.0)
('test loss', 1.1454668)
('test Acc:', 0.78910565)
model_080 F1_score: 82.87% >>>
281
('Epoch Training Loss:', 0.010310663232303341)
('Epoch Training Acc:', 1.0)
('test loss', 1.1333827)
('test Acc:', 0.79425836)
model_080 F1_score: 83.32% >>>
282
('Epoch Training Loss:', 0.010697877494749264)
('Epoch Training Acc:', 1.0)
('test loss', 1.1343968)
('test Acc:', 0.79315424)
model_080 F1_score: 82.97% >>>
283
('Epoch Training Loss:', 0.010431194687043899)
('Epoch Training Acc:', 1.0)
('test loss', 1.1391701)
('test Acc:', 0.78579313)
model_080 F1_score: 82.39% >>>
284
('Epoch Training Loss:', 0.010277193094225368)
('Epoch Training Acc:', 1.0)
('test loss', 1.1408825)
('test Acc:', 0.79315424)
model_080 F1_score: 83.04% >>>
285
('Epoch Training Loss:', 0.010465679986737086)
('Epoch Training Acc:', 1.0)
('test loss', 1.1263961)
('test Acc:', 0.78910565)
model_080 F1_score: 83.07% >>>
286
('Epoch Training Loss:', 0.010198815138210193)
('Epoch Training Acc:', 1.0)
('test loss', 1.1338118)
('test Acc:', 0.78836954)
model_080 F1_score: 82.91% >>>
287
('Epoch Training Loss:', 0.010283817729941802)
('Epoch Training Acc:', 1.0)
('test loss', 1.1292547)
('test Acc:', 0.79131395)
model_080 F1_score: 83.06% >>>
288
('Epoch Training Loss:', 0.010701768253056798)
('Epoch Training Acc:', 1.0)
('test loss', 1.1391424)
('test Acc:', 0.78800148)
model_080 F1_score: 82.67% >>>
289
('Epoch Training Loss:', 0.010462910940987058)
('Epoch Training Acc:', 1.0)
('test loss', 1.1310623)
('test Acc:', 0.78763342)
model_080 F1_score: 82.72% >>>
290
('Epoch Training Loss:', 0.010194113589022891)
('Epoch Training Acc:', 1.0)
('test loss', 1.1424379)
('test Acc:', 0.78505707)
model_080 F1_score: 82.31% >>>
291
('Epoch Training Loss:', 0.010021550822784775)
('Epoch Training Acc:', 1.0)
('test loss', 1.1582712)
('test Acc:', 0.78726536)
model_080 F1_score: 82.53% >>>
292
('Epoch Training Loss:', 0.010006425609390135)
('Epoch Training Acc:', 1.0)
('test loss', 1.1396333)
('test Acc:', 0.78616118)
model_080 F1_score: 82.49% >>>
293
('Epoch Training Loss:', 0.009477129728111322)
('Epoch Training Acc:', 1.0)
('test loss', 1.1468524)
('test Acc:', 0.79131395)
model_080 F1_score: 83.07% >>>
294
('Epoch Training Loss:', 0.0096427385924471309)
('Epoch Training Acc:', 1.0)
('test loss', 1.1390119)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
295
('Epoch Training Loss:', 0.009722193934067036)
('Epoch Training Acc:', 1.0)
('test loss', 1.1407337)
('test Acc:', 0.79462641)
model_080 F1_score: 83.12% >>>
296
('Epoch Training Loss:', 0.010036894022050546)
('Epoch Training Acc:', 1.0)
('test loss', 1.1421021)
('test Acc:', 0.79462641)
model_080 F1_score: 83.22% >>>
297
('Epoch Training Loss:', 0.0098846875971503323)
('Epoch Training Acc:', 1.0)
('test loss', 1.1422489)
('test Acc:', 0.79278618)
model_080 F1_score: 83.22% >>>
298
('Epoch Training Loss:', 0.0099048210413457127)
('Epoch Training Acc:', 1.0)
('test loss', 1.1370723)
('test Acc:', 0.79205006)
model_080 F1_score: 82.99% >>>
299
('Epoch Training Loss:', 0.0098922475117433351)
('Epoch Training Acc:', 1.0)
('test loss', 1.1373686)
('test Acc:', 0.78800148)
model_080 F1_score: 82.57% >>>
300
('Epoch Training Loss:', 0.0099255351833562599)
('Epoch Training Acc:', 1.0)
('test loss', 1.1455295)
('test Acc:', 0.791682)
model_080 F1_score: 83.05% >>>
301
('Epoch Training Loss:', 0.0099986994500795845)
('Epoch Training Acc:', 1.0)
('test loss', 1.1296483)
('test Acc:', 0.79573059)
model_080 F1_score: 83.31% >>>
302
('Epoch Training Loss:', 0.010062531146104448)
('Epoch Training Acc:', 1.0)
('test loss', 1.1317331)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
303
('Epoch Training Loss:', 0.0095997468179120915)
('Epoch Training Acc:', 1.0)
('test loss', 1.1393236)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
304
('Epoch Training Loss:', 0.0096079537379409885)
('Epoch Training Acc:', 1.0)
('test loss', 1.1590961)
('test Acc:', 0.78800148)
model_080 F1_score: 82.60% >>>
305
('Epoch Training Loss:', 0.009587345954059856)
('Epoch Training Acc:', 1.0)
('test loss', 1.1328657)
('test Acc:', 0.79131395)
model_080 F1_score: 83.04% >>>
306
('Epoch Training Loss:', 0.0092873056510143215)
('Epoch Training Acc:', 1.0)
('test loss', 1.1377443)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
307
('Epoch Training Loss:', 0.0096302641923102783)
('Epoch Training Acc:', 1.0)
('test loss', 1.1476452)
('test Acc:', 0.78910565)
model_080 F1_score: 82.82% >>>
308
('Epoch Training Loss:', 0.0093611817992496071)
('Epoch Training Acc:', 1.0)
('test loss', 1.1294476)
('test Acc:', 0.7887376)
model_080 F1_score: 82.61% >>>
309
('Epoch Training Loss:', 0.0094715645609539934)
('Epoch Training Acc:', 1.0)
('test loss', 1.1462615)
('test Acc:', 0.7868973)
model_080 F1_score: 82.62% >>>
310
('Epoch Training Loss:', 0.0094602376357215689)
('Epoch Training Acc:', 1.0)
('test loss', 1.1550038)
('test Acc:', 0.7938903)
model_080 F1_score: 83.29% >>>
311
('Epoch Training Loss:', 0.0094357187426794553)
('Epoch Training Acc:', 1.0)
('test loss', 1.1520733)
('test Acc:', 0.78836954)
model_080 F1_score: 82.86% >>>
312
('Epoch Training Loss:', 0.0092442343484435696)
('Epoch Training Acc:', 1.0)
('test loss', 1.138763)
('test Acc:', 0.79131395)
model_080 F1_score: 83.07% >>>
313
('Epoch Training Loss:', 0.0094669981917832047)
('Epoch Training Acc:', 1.0)
('test loss', 1.1565238)
('test Acc:', 0.79462641)
model_080 F1_score: 83.25% >>>
314
('Epoch Training Loss:', 0.0095011113589862362)
('Epoch Training Acc:', 1.0)
('test loss', 1.1390162)
('test Acc:', 0.79205006)
model_080 F1_score: 83.20% >>>
315
('Epoch Training Loss:', 0.0092782474548585014)
('Epoch Training Acc:', 1.0)
('test loss', 1.1445984)
('test Acc:', 0.79241812)
model_080 F1_score: 83.14% >>>
316
('Epoch Training Loss:', 0.0090439349132793723)
('Epoch Training Acc:', 1.0)
('test loss', 1.146775)
('test Acc:', 0.79352224)
model_080 F1_score: 83.24% >>>
317
('Epoch Training Loss:', 0.0091832471880479716)
('Epoch Training Acc:', 1.0)
('test loss', 1.1608877)
('test Acc:', 0.78652924)
model_080 F1_score: 82.84% >>>
318
('Epoch Training Loss:', 0.0091029084578622133)
('Epoch Training Acc:', 1.0)
('test loss', 1.1473961)
('test Acc:', 0.791682)
model_080 F1_score: 82.95% >>>
319
('Epoch Training Loss:', 0.009046326451425557)
('Epoch Training Acc:', 1.0)
('test loss', 1.1531118)
('test Acc:', 0.79315424)
model_080 F1_score: 83.19% >>>
320
('Epoch Training Loss:', 0.0091003561228717444)
('Epoch Training Acc:', 1.0)
('test loss', 1.1506736)
('test Acc:', 0.79131395)
model_080 F1_score: 82.97% >>>
321
('Epoch Training Loss:', 0.0088869162318587769)
('Epoch Training Acc:', 1.0)
('test loss', 1.1394486)
('test Acc:', 0.78726536)
model_080 F1_score: 82.48% >>>
322
('Epoch Training Loss:', 0.0089268283154524397)
('Epoch Training Acc:', 1.0)
('test loss', 1.1462342)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
323
('Epoch Training Loss:', 0.0088557391227368498)
('Epoch Training Acc:', 1.0)
('test loss', 1.135374)
('test Acc:', 0.79425836)
model_080 F1_score: 83.29% >>>
324
('Epoch Training Loss:', 0.0090541688095981954)
('Epoch Training Acc:', 1.0)
('test loss', 1.1525005)
('test Acc:', 0.79205006)
model_080 F1_score: 83.16% >>>
325
('Epoch Training Loss:', 0.0095062803175096633)
('Epoch Training Acc:', 1.0)
('test loss', 1.1496662)
('test Acc:', 0.78726536)
model_080 F1_score: 82.59% >>>
326
('Epoch Training Loss:', 0.0090360041813255521)
('Epoch Training Acc:', 1.0)
('test loss', 1.1635244)
('test Acc:', 0.79094589)
model_080 F1_score: 82.88% >>>
327
('Epoch Training Loss:', 0.0088592066913406597)
('Epoch Training Acc:', 1.0)
('test loss', 1.1598117)
('test Acc:', 0.79094589)
model_080 F1_score: 82.92% >>>
328
('Epoch Training Loss:', 0.0086837250346434303)
('Epoch Training Acc:', 1.0)
('test loss', 1.1462495)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
329
('Epoch Training Loss:', 0.008743967304326361)
('Epoch Training Acc:', 1.0)
('test loss', 1.1635447)
('test Acc:', 0.79315424)
model_080 F1_score: 83.16% >>>
330
('Epoch Training Loss:', 0.0085668802184954984)
('Epoch Training Acc:', 1.0)
('test loss', 1.1426769)
('test Acc:', 0.79315424)
model_080 F1_score: 83.18% >>>
331
('Epoch Training Loss:', 0.0088334885149379261)
('Epoch Training Acc:', 1.0)
('test loss', 1.1477797)
('test Acc:', 0.79131395)
model_080 F1_score: 82.90% >>>
332
('Epoch Training Loss:', 0.0088929357661982067)
('Epoch Training Acc:', 1.0)
('test loss', 1.1374139)
('test Acc:', 0.78763342)
model_080 F1_score: 82.66% >>>
333
('Epoch Training Loss:', 0.0092331824962457176)
('Epoch Training Acc:', 1.0)
('test loss', 1.1552216)
('test Acc:', 0.79094589)
model_080 F1_score: 82.80% >>>
334
('Epoch Training Loss:', 0.0085949207441444742)
('Epoch Training Acc:', 1.0)
('test loss', 1.1599)
('test Acc:', 0.78579313)
model_080 F1_score: 82.79% >>>
335
('Epoch Training Loss:', 0.0088693492052698275)
('Epoch Training Acc:', 1.0)
('test loss', 1.1579311)
('test Acc:', 0.79020977)
model_080 F1_score: 82.66% >>>
336
('Epoch Training Loss:', 0.0085390285421453882)
('Epoch Training Acc:', 1.0)
('test loss', 1.1646029)
('test Acc:', 0.78947371)
model_080 F1_score: 83.00% >>>
337
('Epoch Training Loss:', 0.0085023414922034135)
('Epoch Training Acc:', 1.0)
('test loss', 1.1562129)
('test Acc:', 0.78763342)
model_080 F1_score: 82.75% >>>
338
('Epoch Training Loss:', 0.0084556106212403392)
('Epoch Training Acc:', 1.0)
('test loss', 1.1418072)
('test Acc:', 0.7887376)
model_080 F1_score: 82.98% >>>
339
('Epoch Training Loss:', 0.0085972192500776146)
('Epoch Training Acc:', 1.0)
('test loss', 1.1469641)
('test Acc:', 0.78910565)
model_080 F1_score: 82.76% >>>
340
('Epoch Training Loss:', 0.0086162302559387172)
('Epoch Training Acc:', 1.0)
('test loss', 1.143077)
('test Acc:', 0.79683477)
model_080 F1_score: 83.42% >>>
341
('Epoch Training Loss:', 0.0087548312676517526)
('Epoch Training Acc:', 1.0)
('test loss', 1.1670456)
('test Acc:', 0.78836954)
model_080 F1_score: 82.64% >>>
342
('Epoch Training Loss:', 0.0082134292697446654)
('Epoch Training Acc:', 1.0)
('test loss', 1.1548839)
('test Acc:', 0.79536253)
model_080 F1_score: 83.26% >>>
343
('Epoch Training Loss:', 0.0082895348878082586)
('Epoch Training Acc:', 1.0)
('test loss', 1.1517913)
('test Acc:', 0.79462641)
model_080 F1_score: 83.35% >>>
344
('Epoch Training Loss:', 0.0085020737024024129)
('Epoch Training Acc:', 1.0)
('test loss', 1.1394746)
('test Acc:', 0.79425836)
model_080 F1_score: 83.13% >>>
345
('Epoch Training Loss:', 0.0083206059698568424)
('Epoch Training Acc:', 1.0)
('test loss', 1.1496886)
('test Acc:', 0.7887376)
model_080 F1_score: 82.75% >>>
346
('Epoch Training Loss:', 0.008378140772038023)
('Epoch Training Acc:', 1.0)
('test loss', 1.1437637)
('test Acc:', 0.78836954)
model_080 F1_score: 82.61% >>>
347
('Epoch Training Loss:', 0.0080439445355295902)
('Epoch Training Acc:', 1.0)
('test loss', 1.1565294)
('test Acc:', 0.78395289)
model_080 F1_score: 82.46% >>>
348
('Epoch Training Loss:', 0.0086746593879070133)
('Epoch Training Acc:', 1.0)
('test loss', 1.1566327)
('test Acc:', 0.78432095)
model_080 F1_score: 82.53% >>>
349
('Epoch Training Loss:', 0.0079190157775883563)
('Epoch Training Acc:', 1.0)
('test loss', 1.1736535)
('test Acc:', 0.7868973)
model_080 F1_score: 82.70% >>>
350
('Epoch Training Loss:', 0.008233514376115636)
('Epoch Training Acc:', 1.0)
('test loss', 1.1531091)
('test Acc:', 0.79462641)
model_080 F1_score: 83.31% >>>
351
('Epoch Training Loss:', 0.0081760175462477491)
('Epoch Training Acc:', 1.0)
('test loss', 1.1588441)
('test Acc:', 0.79278618)
model_080 F1_score: 83.28% >>>
352
('Epoch Training Loss:', 0.0084283149226394016)
('Epoch Training Acc:', 1.0)
('test loss', 1.1489023)
('test Acc:', 0.7887376)
model_080 F1_score: 82.74% >>>
353
('Epoch Training Loss:', 0.0082144242041977122)
('Epoch Training Acc:', 1.0)
('test loss', 1.1494825)
('test Acc:', 0.78836954)
model_080 F1_score: 82.69% >>>
354
('Epoch Training Loss:', 0.0086152250914892647)
('Epoch Training Acc:', 1.0)
('test loss', 1.1598299)
('test Acc:', 0.79205006)
model_080 F1_score: 83.25% >>>
355
('Epoch Training Loss:', 0.008043910282140132)
('Epoch Training Acc:', 1.0)
('test loss', 1.1591933)
('test Acc:', 0.7868973)
model_080 F1_score: 82.85% >>>
356
('Epoch Training Loss:', 0.0086013135332905222)
('Epoch Training Acc:', 1.0)
('test loss', 1.170614)
('test Acc:', 0.78726536)
model_080 F1_score: 82.77% >>>
357
('Epoch Training Loss:', 0.0081964956534648081)
('Epoch Training Acc:', 1.0)
('test loss', 1.151505)
('test Acc:', 0.7887376)
model_080 F1_score: 82.90% >>>
358
('Epoch Training Loss:', 0.0079918427618395071)
('Epoch Training Acc:', 1.0)
('test loss', 1.1468731)
('test Acc:', 0.78579313)
model_080 F1_score: 82.43% >>>
359
('Epoch Training Loss:', 0.0081868549968930893)
('Epoch Training Acc:', 1.0)
('test loss', 1.1541145)
('test Acc:', 0.78800148)
model_080 F1_score: 82.72% >>>
360
('Epoch Training Loss:', 0.0080665330933697987)
('Epoch Training Acc:', 1.0)
('test loss', 1.1680558)
('test Acc:', 0.78800148)
model_080 F1_score: 82.79% >>>
361
('Epoch Training Loss:', 0.0078209397270256886)
('Epoch Training Acc:', 1.0)
('test loss', 1.1450467)
('test Acc:', 0.79094589)
model_080 F1_score: 82.71% >>>
362
('Epoch Training Loss:', 0.0078517048314097337)
('Epoch Training Acc:', 1.0)
('test loss', 1.161445)
('test Acc:', 0.78947371)
model_080 F1_score: 82.88% >>>
363
('Epoch Training Loss:', 0.008015204270122922)
('Epoch Training Acc:', 1.0)
('test loss', 1.1715535)
('test Acc:', 0.79536253)
model_080 F1_score: 83.25% >>>
364
('Epoch Training Loss:', 0.00804982021691103)
('Epoch Training Acc:', 1.0)
('test loss', 1.1779368)
('test Acc:', 0.78395289)
model_080 F1_score: 82.50% >>>
365
('Epoch Training Loss:', 0.0078846049455023604)
('Epoch Training Acc:', 1.0)
('test loss', 1.1446435)
('test Acc:', 0.78763342)
model_080 F1_score: 82.61% >>>
366
('Epoch Training Loss:', 0.0076825408123113448)
('Epoch Training Acc:', 1.0)
('test loss', 1.1721058)
('test Acc:', 0.78652924)
model_080 F1_score: 82.81% >>>
367
('Epoch Training Loss:', 0.0074572566390997963)
('Epoch Training Acc:', 1.0)
('test loss', 1.162833)
('test Acc:', 0.78947371)
model_080 F1_score: 82.65% >>>
368
('Epoch Training Loss:', 0.0076959451944276225)
('Epoch Training Acc:', 1.0)
('test loss', 1.1640278)
('test Acc:', 0.79536253)
model_080 F1_score: 83.41% >>>
369
('Epoch Training Loss:', 0.0075974936535203597)
('Epoch Training Acc:', 1.0)
('test loss', 1.1631463)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
370
('Epoch Training Loss:', 0.0077872095407656161)
('Epoch Training Acc:', 1.0)
('test loss', 1.1704928)
('test Acc:', 0.79057783)
model_080 F1_score: 82.64% >>>
371
('Epoch Training Loss:', 0.007779621146255522)
('Epoch Training Acc:', 1.0)
('test loss', 1.167684)
('test Acc:', 0.79131395)
model_080 F1_score: 83.15% >>>
372
('Epoch Training Loss:', 0.0077427690011973027)
('Epoch Training Acc:', 1.0)
('test loss', 1.1528362)
('test Acc:', 0.78984171)
model_080 F1_score: 82.77% >>>
373
('Epoch Training Loss:', 0.0079795539350016043)
('Epoch Training Acc:', 1.0)
('test loss', 1.1668713)
('test Acc:', 0.79094589)
model_080 F1_score: 82.84% >>>
374
('Epoch Training Loss:', 0.0076327096339809941)
('Epoch Training Acc:', 1.0)
('test loss', 1.186264)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
375
('Epoch Training Loss:', 0.0077408602428477025)
('Epoch Training Acc:', 1.0)
('test loss', 1.1489991)
('test Acc:', 0.79352224)
model_080 F1_score: 83.16% >>>
376
('Epoch Training Loss:', 0.0072905823726614472)
('Epoch Training Acc:', 1.0)
('test loss', 1.1629148)
('test Acc:', 0.79462641)
model_080 F1_score: 83.16% >>>
377
('Epoch Training Loss:', 0.0077452286586776609)
('Epoch Training Acc:', 1.0)
('test loss', 1.1726837)
('test Acc:', 0.78984171)
model_080 F1_score: 82.88% >>>
378
('Epoch Training Loss:', 0.007419367468173732)
('Epoch Training Acc:', 1.0)
('test loss', 1.1572226)
('test Acc:', 0.79131395)
model_080 F1_score: 82.93% >>>
379
('Epoch Training Loss:', 0.0075237875535094645)
('Epoch Training Acc:', 1.0)
('test loss', 1.1368366)
('test Acc:', 0.79425836)
model_080 F1_score: 83.13% >>>
380
('Epoch Training Loss:', 0.0079770026986807352)
('Epoch Training Acc:', 1.0)
('test loss', 1.1704206)
('test Acc:', 0.79057783)
model_080 F1_score: 82.87% >>>
381
('Epoch Training Loss:', 0.0075971141395712039)
('Epoch Training Acc:', 1.0)
('test loss', 1.1692837)
('test Acc:', 0.79205006)
model_080 F1_score: 82.94% >>>
382
('Epoch Training Loss:', 0.0077713584796583746)
('Epoch Training Acc:', 1.0)
('test loss', 1.1777973)
('test Acc:', 0.78763342)
model_080 F1_score: 82.70% >>>
383
('Epoch Training Loss:', 0.0072895167268143268)
('Epoch Training Acc:', 1.0)
('test loss', 1.161841)
('test Acc:', 0.78800148)
model_080 F1_score: 82.95% >>>
384
('Epoch Training Loss:', 0.008054365194766433)
('Epoch Training Acc:', 1.0)
('test loss', 1.1547891)
('test Acc:', 0.79131395)
model_080 F1_score: 83.07% >>>
385
('Epoch Training Loss:', 0.0075898185041296529)
('Epoch Training Acc:', 1.0)
('test loss', 1.1775719)
('test Acc:', 0.78836954)
model_080 F1_score: 82.59% >>>
386
('Epoch Training Loss:', 0.007216658992547309)
('Epoch Training Acc:', 1.0)
('test loss', 1.1645902)
('test Acc:', 0.791682)
model_080 F1_score: 83.06% >>>
387
('Epoch Training Loss:', 0.0071534967210027389)
('Epoch Training Acc:', 1.0)
('test loss', 1.1549001)
('test Acc:', 0.78763342)
model_080 F1_score: 82.76% >>>
388
('Epoch Training Loss:', 0.007180308377428446)
('Epoch Training Acc:', 1.0)
('test loss', 1.1740528)
('test Acc:', 0.78542513)
model_080 F1_score: 82.60% >>>
389
('Epoch Training Loss:', 0.0072653548741072882)
('Epoch Training Acc:', 1.0)
('test loss', 1.1756023)
('test Acc:', 0.7868973)
model_080 F1_score: 82.52% >>>
390
('Epoch Training Loss:', 0.0074993129110225709)
('Epoch Training Acc:', 1.0)
('test loss', 1.1840593)
('test Acc:', 0.78763342)
model_080 F1_score: 82.52% >>>
391
('Epoch Training Loss:', 0.00712262790875684)
('Epoch Training Acc:', 1.0)
('test loss', 1.1483101)
('test Acc:', 0.79609865)
model_080 F1_score: 83.47% >>>
392
('Epoch Training Loss:', 0.0071746438534319168)
('Epoch Training Acc:', 1.0)
('test loss', 1.1695589)
('test Acc:', 0.7868973)
model_080 F1_score: 82.73% >>>
393
('Epoch Training Loss:', 0.0073030721832765266)
('Epoch Training Acc:', 1.0)
('test loss', 1.166419)
('test Acc:', 0.78836954)
model_080 F1_score: 82.81% >>>
394
('Epoch Training Loss:', 0.0073688355823833263)
('Epoch Training Acc:', 1.0)
('test loss', 1.1624851)
('test Acc:', 0.79315424)
model_080 F1_score: 83.21% >>>
395
('Epoch Training Loss:', 0.0073379688546992838)
('Epoch Training Acc:', 1.0)
('test loss', 1.1790513)
('test Acc:', 0.79020977)
model_080 F1_score: 82.68% >>>
396
('Epoch Training Loss:', 0.0074528328823362244)
('Epoch Training Acc:', 1.0)
('test loss', 1.1863739)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
397
('Epoch Training Loss:', 0.0071338702400680631)
('Epoch Training Acc:', 1.0)
('test loss', 1.1628611)
('test Acc:', 0.79057783)
model_080 F1_score: 82.97% >>>
398
('Epoch Training Loss:', 0.0069310002363636158)
('Epoch Training Acc:', 1.0)
('test loss', 1.1538517)
('test Acc:', 0.79462641)
model_080 F1_score: 83.24% >>>
399
('Epoch Training Loss:', 0.0071038340047380188)
('Epoch Training Acc:', 1.0)
('test loss', 1.1674656)
('test Acc:', 0.79020977)
model_080 F1_score: 83.00% >>>
400
('Epoch Training Loss:', 0.0071403741640096996)
('Epoch Training Acc:', 1.0)
('test loss', 1.1740655)
('test Acc:', 0.79057783)
model_080 F1_score: 82.85% >>>
401
('Epoch Training Loss:', 0.0071544952697877306)
('Epoch Training Acc:', 1.0)
('test loss', 1.1745939)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
402
('Epoch Training Loss:', 0.0070231396857707296)
('Epoch Training Acc:', 1.0)
('test loss', 1.1715649)
('test Acc:', 0.78763342)
model_080 F1_score: 82.69% >>>
403
('Epoch Training Loss:', 0.0070853399338375311)
('Epoch Training Acc:', 1.0)
('test loss', 1.1679047)
('test Acc:', 0.78947371)
model_080 F1_score: 82.64% >>>
404
('Epoch Training Loss:', 0.0074255716062907595)
('Epoch Training Acc:', 1.0)
('test loss', 1.1730096)
('test Acc:', 0.79241812)
model_080 F1_score: 82.94% >>>
405
('Epoch Training Loss:', 0.0071792164726502961)
('Epoch Training Acc:', 1.0)
('test loss', 1.1672453)
('test Acc:', 0.79462641)
model_080 F1_score: 83.24% >>>
406
('Epoch Training Loss:', 0.0072685971081227763)
('Epoch Training Acc:', 1.0)
('test loss', 1.1483918)
('test Acc:', 0.79020977)
model_080 F1_score: 83.01% >>>
407
('Epoch Training Loss:', 0.0068260816551628523)
('Epoch Training Acc:', 1.0)
('test loss', 1.1655574)
('test Acc:', 0.79278618)
model_080 F1_score: 82.93% >>>
408
('Epoch Training Loss:', 0.0067454386953613721)
('Epoch Training Acc:', 1.0)
('test loss', 1.1737432)
('test Acc:', 0.78984171)
model_080 F1_score: 82.91% >>>
409
('Epoch Training Loss:', 0.0067829484978574328)
('Epoch Training Acc:', 1.0)
('test loss', 1.1738271)
('test Acc:', 0.7868973)
model_080 F1_score: 82.74% >>>
410
('Epoch Training Loss:', 0.0066604238418221939)
('Epoch Training Acc:', 1.0)
('test loss', 1.1766884)
('test Acc:', 0.78616118)
model_080 F1_score: 82.75% >>>
411
('Epoch Training Loss:', 0.0066987996087846113)
('Epoch Training Acc:', 1.0)
('test loss', 1.1664895)
('test Acc:', 0.79057783)
model_080 F1_score: 82.95% >>>
412
('Epoch Training Loss:', 0.0069281129090086324)
('Epoch Training Acc:', 1.0)
('test loss', 1.1745026)
('test Acc:', 0.78984171)
model_080 F1_score: 82.93% >>>
413
('Epoch Training Loss:', 0.0071415437796531478)
('Epoch Training Acc:', 1.0)
('test loss', 1.1759453)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
414
('Epoch Training Loss:', 0.0068933599195588613)
('Epoch Training Acc:', 1.0)
('test loss', 1.1741623)
('test Acc:', 0.78800148)
model_080 F1_score: 82.77% >>>
415
('Epoch Training Loss:', 0.006484386203737813)
('Epoch Training Acc:', 1.0)
('test loss', 1.1829315)
('test Acc:', 0.79094589)
model_080 F1_score: 83.06% >>>
416
('Epoch Training Loss:', 0.0068686406293636537)
('Epoch Training Acc:', 1.0)
('test loss', 1.1531311)
('test Acc:', 0.78947371)
model_080 F1_score: 82.67% >>>
417
('Epoch Training Loss:', 0.0065468385146232322)
('Epoch Training Acc:', 1.0)
('test loss', 1.1636076)
('test Acc:', 0.78947371)
model_080 F1_score: 82.90% >>>
418
('Epoch Training Loss:', 0.0068763239760301076)
('Epoch Training Acc:', 1.0)
('test loss', 1.1659197)
('test Acc:', 0.79131395)
model_080 F1_score: 82.91% >>>
419
('Epoch Training Loss:', 0.0071912870607775403)
('Epoch Training Acc:', 1.0)
('test loss', 1.174947)
('test Acc:', 0.79499447)
model_080 F1_score: 83.32% >>>
420
('Epoch Training Loss:', 0.0067146640376449795)
('Epoch Training Acc:', 1.0)
('test loss', 1.1800476)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
421
('Epoch Training Loss:', 0.0066013961331918836)
('Epoch Training Acc:', 1.0)
('test loss', 1.1659268)
('test Acc:', 0.78579313)
model_080 F1_score: 82.51% >>>
422
('Epoch Training Loss:', 0.0065088044684671331)
('Epoch Training Acc:', 1.0)
('test loss', 1.1791861)
('test Acc:', 0.7938903)
model_080 F1_score: 83.27% >>>
423
('Epoch Training Loss:', 0.0066484394219514797)
('Epoch Training Acc:', 1.0)
('test loss', 1.1942831)
('test Acc:', 0.79352224)
model_080 F1_score: 83.14% >>>
424
('Epoch Training Loss:', 0.00648962862942426)
('Epoch Training Acc:', 1.0)
('test loss', 1.1633991)
('test Acc:', 0.79278618)
model_080 F1_score: 83.27% >>>
425
('Epoch Training Loss:', 0.0067945092760055559)
('Epoch Training Acc:', 1.0)
('test loss', 1.1891204)
('test Acc:', 0.79241812)
model_080 F1_score: 83.09% >>>
426
('Epoch Training Loss:', 0.0066043202568835113)
('Epoch Training Acc:', 1.0)
('test loss', 1.1585252)
('test Acc:', 0.79315424)
model_080 F1_score: 82.98% >>>
427
('Epoch Training Loss:', 0.0068275823014118942)
('Epoch Training Acc:', 1.0)
('test loss', 1.1809711)
('test Acc:', 0.78726536)
model_080 F1_score: 82.69% >>>
428
('Epoch Training Loss:', 0.0064974162633006927)
('Epoch Training Acc:', 1.0)
('test loss', 1.1545036)
('test Acc:', 0.79205006)
model_080 F1_score: 82.92% >>>
429
('Epoch Training Loss:', 0.006506041423563147)
('Epoch Training Acc:', 1.0)
('test loss', 1.1877134)
('test Acc:', 0.78505707)
model_080 F1_score: 82.69% >>>
430
('Epoch Training Loss:', 0.0067138780796085484)
('Epoch Training Acc:', 1.0)
('test loss', 1.1850673)
('test Acc:', 0.78505707)
model_080 F1_score: 82.70% >>>
431
('Epoch Training Loss:', 0.006569615474290913)
('Epoch Training Acc:', 1.0)
('test loss', 1.184348)
('test Acc:', 0.79020977)
model_080 F1_score: 82.91% >>>
432
('Epoch Training Loss:', 0.0066848007154476363)
('Epoch Training Acc:', 1.0)
('test loss', 1.1698221)
('test Acc:', 0.79241812)
model_080 F1_score: 83.01% >>>
433
('Epoch Training Loss:', 0.0065166004751517903)
('Epoch Training Acc:', 1.0)
('test loss', 1.1749815)
('test Acc:', 0.7938903)
model_080 F1_score: 83.33% >>>
434
('Epoch Training Loss:', 0.006537009541716543)
('Epoch Training Acc:', 1.0)
('test loss', 1.1707749)
('test Acc:', 0.78947371)
model_080 F1_score: 82.72% >>>
435
('Epoch Training Loss:', 0.0061564024581457488)
('Epoch Training Acc:', 1.0)
('test loss', 1.1714736)
('test Acc:', 0.78726536)
model_080 F1_score: 82.40% >>>
436
('Epoch Training Loss:', 0.006425712503187242)
('Epoch Training Acc:', 1.0)
('test loss', 1.1730624)
('test Acc:', 0.78984171)
model_080 F1_score: 82.88% >>>
437
('Epoch Training Loss:', 0.0065384685949538834)
('Epoch Training Acc:', 1.0)
('test loss', 1.1821618)
('test Acc:', 0.78726536)
model_080 F1_score: 82.67% >>>
438
('Epoch Training Loss:', 0.0066626379411900416)
('Epoch Training Acc:', 1.0)
('test loss', 1.184768)
('test Acc:', 0.79205006)
model_080 F1_score: 82.82% >>>
439
('Epoch Training Loss:', 0.0061278975608729525)
('Epoch Training Acc:', 1.0)
('test loss', 1.1616145)
('test Acc:', 0.79241812)
model_080 F1_score: 83.22% >>>
440
('Epoch Training Loss:', 0.0062456341929646442)
('Epoch Training Acc:', 1.0)
('test loss', 1.1846659)
('test Acc:', 0.78763342)
model_080 F1_score: 82.68% >>>
441
('Epoch Training Loss:', 0.006545676516907406)
('Epoch Training Acc:', 1.0)
('test loss', 1.1614939)
('test Acc:', 0.79131395)
model_080 F1_score: 82.93% >>>
442
('Epoch Training Loss:', 0.0066344280530756805)
('Epoch Training Acc:', 1.0)
('test loss', 1.1693184)
('test Acc:', 0.79131395)
model_080 F1_score: 82.68% >>>
443
('Epoch Training Loss:', 0.0065164306870428845)
('Epoch Training Acc:', 1.0)
('test loss', 1.1673083)
('test Acc:', 0.78616118)
model_080 F1_score: 82.62% >>>
444
('Epoch Training Loss:', 0.0064337675976275932)
('Epoch Training Acc:', 1.0)
('test loss', 1.1702969)
('test Acc:', 0.79536253)
model_080 F1_score: 83.45% >>>
445
('Epoch Training Loss:', 0.0065049245931732003)
('Epoch Training Acc:', 1.0)
('test loss', 1.1816568)
('test Acc:', 0.7887376)
model_080 F1_score: 82.78% >>>
446
('Epoch Training Loss:', 0.0065467171734781004)
('Epoch Training Acc:', 1.0)
('test loss', 1.1661988)
('test Acc:', 0.7887376)
model_080 F1_score: 82.92% >>>
447
('Epoch Training Loss:', 0.0063846127859505941)
('Epoch Training Acc:', 1.0)
('test loss', 1.1943132)
('test Acc:', 0.78984171)
model_080 F1_score: 83.02% >>>
448
('Epoch Training Loss:', 0.0065264630211459007)
('Epoch Training Acc:', 1.0)
('test loss', 1.1680865)
('test Acc:', 0.79131395)
model_080 F1_score: 83.11% >>>
449
('Epoch Training Loss:', 0.0066054549442924326)
('Epoch Training Acc:', 1.0)
('test loss', 1.1675969)
('test Acc:', 0.79315424)
model_080 F1_score: 82.93% >>>
450
('Epoch Training Loss:', 0.0059674875101336511)
('Epoch Training Acc:', 1.0)
('test loss', 1.1725866)
('test Acc:', 0.79057783)
model_080 F1_score: 82.91% >>>
451
('Epoch Training Loss:', 0.006191717282490572)
('Epoch Training Acc:', 1.0)
('test loss', 1.1669507)
('test Acc:', 0.79241812)
model_080 F1_score: 83.20% >>>
452
('Epoch Training Loss:', 0.0064344962283939822)
('Epoch Training Acc:', 1.0)
('test loss', 1.1833528)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
453
('Epoch Training Loss:', 0.0061933837105243583)
('Epoch Training Acc:', 1.0)
('test loss', 1.1746297)
('test Acc:', 0.78947371)
model_080 F1_score: 82.83% >>>
454
('Epoch Training Loss:', 0.0062018392818572465)
('Epoch Training Acc:', 1.0)
('test loss', 1.1784462)
('test Acc:', 0.7938903)
model_080 F1_score: 83.16% >>>
455
('Epoch Training Loss:', 0.0062980329666970647)
('Epoch Training Acc:', 1.0)
('test loss', 1.1704005)
('test Acc:', 0.7887376)
model_080 F1_score: 82.75% >>>
456
('Epoch Training Loss:', 0.0060768678667955101)
('Epoch Training Acc:', 1.0)
('test loss', 1.1677344)
('test Acc:', 0.791682)
model_080 F1_score: 83.00% >>>
457
('Epoch Training Loss:', 0.006318105137324892)
('Epoch Training Acc:', 1.0)
('test loss', 1.1783154)
('test Acc:', 0.78984171)
model_080 F1_score: 82.91% >>>
458
('Epoch Training Loss:', 0.0061965464974491624)
('Epoch Training Acc:', 1.0)
('test loss', 1.1783518)
('test Acc:', 0.79057783)
model_080 F1_score: 83.05% >>>
459
('Epoch Training Loss:', 0.006272538755183632)
('Epoch Training Acc:', 1.0)
('test loss', 1.1927961)
('test Acc:', 0.79057783)
model_080 F1_score: 82.93% >>>
460
('Epoch Training Loss:', 0.0059525561864575138)
('Epoch Training Acc:', 1.0)
('test loss', 1.1839633)
('test Acc:', 0.79278618)
model_080 F1_score: 83.09% >>>
461
('Epoch Training Loss:', 0.0060073258791817352)
('Epoch Training Acc:', 1.0)
('test loss', 1.2029259)
('test Acc:', 0.78800148)
model_080 F1_score: 82.70% >>>
462
('Epoch Training Loss:', 0.0065978105621979921)
('Epoch Training Acc:', 1.0)
('test loss', 1.1815283)
('test Acc:', 0.791682)
model_080 F1_score: 83.07% >>>
463
('Epoch Training Loss:', 0.0056663900986677618)
('Epoch Training Acc:', 1.0)
('test loss', 1.1790465)
('test Acc:', 0.78984171)
model_080 F1_score: 82.78% >>>
464
('Epoch Training Loss:', 0.0060477861288745771)
('Epoch Training Acc:', 1.0)
('test loss', 1.1942496)
('test Acc:', 0.7887376)
model_080 F1_score: 82.56% >>>
465
('Epoch Training Loss:', 0.0061324418747972231)
('Epoch Training Acc:', 1.0)
('test loss', 1.1888537)
('test Acc:', 0.78763342)
model_080 F1_score: 82.82% >>>
466
('Epoch Training Loss:', 0.0058357763573440025)
('Epoch Training Acc:', 1.0)
('test loss', 1.194841)
('test Acc:', 0.78836954)
model_080 F1_score: 82.72% >>>
467
('Epoch Training Loss:', 0.006084166472646757)
('Epoch Training Acc:', 1.0)
('test loss', 1.1783781)
('test Acc:', 0.79241812)
model_080 F1_score: 83.13% >>>
468
('Epoch Training Loss:', 0.0060951828772886074)
('Epoch Training Acc:', 1.0)
('test loss', 1.1848595)
('test Acc:', 0.79094589)
model_080 F1_score: 83.12% >>>
469
('Epoch Training Loss:', 0.0061644096249438007)
('Epoch Training Acc:', 1.0)
('test loss', 1.2017063)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
470
('Epoch Training Loss:', 0.0060355686382536078)
('Epoch Training Acc:', 1.0)
('test loss', 1.182528)
('test Acc:', 0.79352224)
model_080 F1_score: 83.01% >>>
471
('Epoch Training Loss:', 0.0060301960547803901)
('Epoch Training Acc:', 1.0)
('test loss', 1.1844534)
('test Acc:', 0.79205006)
model_080 F1_score: 82.93% >>>
472
('Epoch Training Loss:', 0.0060758966883440735)
('Epoch Training Acc:', 1.0)
('test loss', 1.1951381)
('test Acc:', 0.79278618)
model_080 F1_score: 82.83% >>>
473
('Epoch Training Loss:', 0.0057602282058724086)
('Epoch Training Acc:', 1.0)
('test loss', 1.1897964)
('test Acc:', 0.78836954)
model_080 F1_score: 82.76% >>>
474
('Epoch Training Loss:', 0.0059157297655474395)
('Epoch Training Acc:', 1.0)
('test loss', 1.1939949)
('test Acc:', 0.79425836)
model_080 F1_score: 83.32% >>>
475
('Epoch Training Loss:', 0.006299943539488595)
('Epoch Training Acc:', 1.0)
('test loss', 1.1991512)
('test Acc:', 0.78947371)
model_080 F1_score: 82.76% >>>
476
('Epoch Training Loss:', 0.0060473187222669367)
('Epoch Training Acc:', 1.0)
('test loss', 1.1746541)
('test Acc:', 0.79057783)
model_080 F1_score: 83.01% >>>
477
('Epoch Training Loss:', 0.0058537865534162847)
('Epoch Training Acc:', 1.0)
('test loss', 1.1840661)
('test Acc:', 0.79646671)
model_080 F1_score: 83.47% >>>
478
('Epoch Training Loss:', 0.0056999329462996684)
('Epoch Training Acc:', 1.0)
('test loss', 1.1817744)
('test Acc:', 0.79425836)
model_080 F1_score: 83.41% >>>
479
('Epoch Training Loss:', 0.0059009258375226636)
('Epoch Training Acc:', 1.0)
('test loss', 1.1742066)
('test Acc:', 0.791682)
model_080 F1_score: 82.72% >>>
480
('Epoch Training Loss:', 0.0059214513712504413)
('Epoch Training Acc:', 1.0)
('test loss', 1.1710266)
('test Acc:', 0.78726536)
model_080 F1_score: 82.40% >>>
481
('Epoch Training Loss:', 0.0059267942506266991)
('Epoch Training Acc:', 1.0)
('test loss', 1.1820066)
('test Acc:', 0.79315424)
model_080 F1_score: 82.91% >>>
482
('Epoch Training Loss:', 0.0057766343788898666)
('Epoch Training Acc:', 1.0)
('test loss', 1.1840421)
('test Acc:', 0.79057783)
model_080 F1_score: 82.77% >>>
483
('Epoch Training Loss:', 0.0057654188949527452)
('Epoch Training Acc:', 1.0)
('test loss', 1.1870145)
('test Acc:', 0.78616118)
model_080 F1_score: 82.67% >>>
484
('Epoch Training Loss:', 0.0060910037482244661)
('Epoch Training Acc:', 1.0)
('test loss', 1.1809726)
('test Acc:', 0.79425836)
model_080 F1_score: 83.40% >>>
485
('Epoch Training Loss:', 0.0058961047288903501)
('Epoch Training Acc:', 1.0)
('test loss', 1.1693012)
('test Acc:', 0.78836954)
model_080 F1_score: 82.78% >>>
486
('Epoch Training Loss:', 0.005802844062600343)
('Epoch Training Acc:', 1.0)
('test loss', 1.1884071)
('test Acc:', 0.78984171)
model_080 F1_score: 82.77% >>>
487
('Epoch Training Loss:', 0.0061343888764895382)
('Epoch Training Acc:', 1.0)
('test loss', 1.1914582)
('test Acc:', 0.79278618)
model_080 F1_score: 83.05% >>>
488
('Epoch Training Loss:', 0.0055934497258931515)
('Epoch Training Acc:', 1.0)
('test loss', 1.176529)
('test Acc:', 0.78836954)
model_080 F1_score: 82.70% >>>
489
('Epoch Training Loss:', 0.0058913063949148636)
('Epoch Training Acc:', 1.0)
('test loss', 1.1801614)
('test Acc:', 0.78984171)
model_080 F1_score: 82.96% >>>
490
('Epoch Training Loss:', 0.0057273836482636398)
('Epoch Training Acc:', 1.0)
('test loss', 1.1942315)
('test Acc:', 0.7887376)
model_080 F1_score: 82.99% >>>
491
('Epoch Training Loss:', 0.0057135394636134151)
('Epoch Training Acc:', 1.0)
('test loss', 1.1944017)
('test Acc:', 0.791682)
model_080 F1_score: 83.03% >>>
492
('Epoch Training Loss:', 0.0057074021797234309)
('Epoch Training Acc:', 1.0)
('test loss', 1.1825002)
('test Acc:', 0.79131395)
model_080 F1_score: 82.93% >>>
493
('Epoch Training Loss:', 0.0061653263710468309)
('Epoch Training Acc:', 1.0)
('test loss', 1.1969545)
('test Acc:', 0.79094589)
model_080 F1_score: 82.90% >>>
494
('Epoch Training Loss:', 0.0058422461333975662)
('Epoch Training Acc:', 1.0)
('test loss', 1.1846411)
('test Acc:', 0.79499447)
model_080 F1_score: 83.21% >>>
495
('Epoch Training Loss:', 0.0057438070725766011)
('Epoch Training Acc:', 1.0)
('test loss', 1.1801487)
('test Acc:', 0.78910565)
model_080 F1_score: 82.98% >>>
496
('Epoch Training Loss:', 0.0055094736580940662)
('Epoch Training Acc:', 1.0)
('test loss', 1.1864762)
('test Acc:', 0.78910565)
model_080 F1_score: 82.90% >>>
497
('Epoch Training Loss:', 0.0058146057417616248)
('Epoch Training Acc:', 1.0)
('test loss', 1.2027786)
('test Acc:', 0.7887376)
model_080 F1_score: 82.63% >>>
498
('Epoch Training Loss:', 0.0056624027065481641)
('Epoch Training Acc:', 1.0)
('test loss', 1.1943473)
('test Acc:', 0.79425836)
model_080 F1_score: 83.15% >>>
499
('Epoch Training Loss:', 0.0056667961080165696)
('Epoch Training Acc:', 1.0)
('test loss', 1.1776828)
('test Acc:', 0.78800148)
model_080 F1_score: 82.66% >>>
500
('Epoch Training Loss:', 0.0054300530873661046)
('Epoch Training Acc:', 1.0)
('test loss', 1.1746757)
('test Acc:', 0.791682)
model_080 F1_score: 83.27% >>>
501
('Epoch Training Loss:', 0.005899424394556263)
('Epoch Training Acc:', 1.0)
('test loss', 1.1855104)
('test Acc:', 0.78910565)
model_080 F1_score: 82.77% >>>
502
('Epoch Training Loss:', 0.0055937610350156319)
('Epoch Training Acc:', 1.0)
('test loss', 1.1771308)
('test Acc:', 0.78800148)
model_080 F1_score: 82.75% >>>
503
('Epoch Training Loss:', 0.0058606087577572907)
('Epoch Training Acc:', 1.0)
('test loss', 1.1934667)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
504
('Epoch Training Loss:', 0.0053183367226665723)
('Epoch Training Acc:', 1.0)
('test loss', 1.1887163)
('test Acc:', 0.78763342)
model_080 F1_score: 82.92% >>>
505
('Epoch Training Loss:', 0.0055784350970498053)
('Epoch Training Acc:', 1.0)
('test loss', 1.1839396)
('test Acc:', 0.79241812)
model_080 F1_score: 83.21% >>>
506
('Epoch Training Loss:', 0.0055116645626185345)
('Epoch Training Acc:', 1.0)
('test loss', 1.1961211)
('test Acc:', 0.78395289)
model_080 F1_score: 82.52% >>>
507
('Epoch Training Loss:', 0.0054221688887992059)
('Epoch Training Acc:', 1.0)
('test loss', 1.1869068)
('test Acc:', 0.78836954)
model_080 F1_score: 82.90% >>>
508
('Epoch Training Loss:', 0.0054758764163125306)
('Epoch Training Acc:', 1.0)
('test loss', 1.2000283)
('test Acc:', 0.79094589)
model_080 F1_score: 82.93% >>>
509
('Epoch Training Loss:', 0.0055702721774650854)
('Epoch Training Acc:', 1.0)
('test loss', 1.1743762)
('test Acc:', 0.79462641)
model_080 F1_score: 83.11% >>>
510
('Epoch Training Loss:', 0.0055926345721672988)
('Epoch Training Acc:', 1.0)
('test loss', 1.2082027)
('test Acc:', 0.79057783)
model_080 F1_score: 82.85% >>>
511
('Epoch Training Loss:', 0.0055290220134338597)
('Epoch Training Acc:', 1.0)
('test loss', 1.2063982)
('test Acc:', 0.791682)
model_080 F1_score: 82.98% >>>
512
('Epoch Training Loss:', 0.0056243323106173193)
('Epoch Training Acc:', 1.0)
('test loss', 1.2003058)
('test Acc:', 0.78652924)
model_080 F1_score: 82.72% >>>
513
('Epoch Training Loss:', 0.0053750582028442295)
('Epoch Training Acc:', 1.0)
('test loss', 1.1959198)
('test Acc:', 0.79057783)
model_080 F1_score: 82.86% >>>
514
('Epoch Training Loss:', 0.0054786290984338848)
('Epoch Training Acc:', 1.0)
('test loss', 1.1951973)
('test Acc:', 0.79205006)
model_080 F1_score: 82.92% >>>
515
('Epoch Training Loss:', 0.0055501921278846567)
('Epoch Training Acc:', 1.0)
('test loss', 1.190954)
('test Acc:', 0.78800148)
model_080 F1_score: 82.67% >>>
516
('Epoch Training Loss:', 0.0053315046734496718)
('Epoch Training Acc:', 1.0)
('test loss', 1.2001115)
('test Acc:', 0.79205006)
model_080 F1_score: 82.89% >>>
517
('Epoch Training Loss:', 0.0055743362991051981)
('Epoch Training Acc:', 1.0)
('test loss', 1.1960301)
('test Acc:', 0.79315424)
model_080 F1_score: 82.98% >>>
518
('Epoch Training Loss:', 0.0052698674180646776)
('Epoch Training Acc:', 1.0)
('test loss', 1.1883938)
('test Acc:', 0.7887376)
model_080 F1_score: 82.76% >>>
519
('Epoch Training Loss:', 0.0053518939994319226)
('Epoch Training Acc:', 1.0)
('test loss', 1.1858069)
('test Acc:', 0.7938903)
model_080 F1_score: 83.29% >>>
520
('Epoch Training Loss:', 0.0054227594609983498)
('Epoch Training Acc:', 1.0)
('test loss', 1.1988431)
('test Acc:', 0.7887376)
model_080 F1_score: 82.83% >>>
521
('Epoch Training Loss:', 0.0053041268438391853)
('Epoch Training Acc:', 1.0)
('test loss', 1.2170935)
('test Acc:', 0.78284872)
model_080 F1_score: 82.43% >>>
522
('Epoch Training Loss:', 0.0052478146890280186)
('Epoch Training Acc:', 1.0)
('test loss', 1.1624544)
('test Acc:', 0.79020977)
model_080 F1_score: 83.09% >>>
523
('Epoch Training Loss:', 0.0055800106201786548)
('Epoch Training Acc:', 1.0)
('test loss', 1.1980339)
('test Acc:', 0.78947371)
model_080 F1_score: 82.64% >>>
524
('Epoch Training Loss:', 0.0054405028176915948)
('Epoch Training Acc:', 1.0)
('test loss', 1.1978481)
('test Acc:', 0.79315424)
model_080 F1_score: 83.19% >>>
525
('Epoch Training Loss:', 0.0053455127927009016)
('Epoch Training Acc:', 1.0)
('test loss', 1.1779119)
('test Acc:', 0.79462641)
model_080 F1_score: 83.38% >>>
526
('Epoch Training Loss:', 0.005626156018479378)
('Epoch Training Acc:', 1.0)
('test loss', 1.1685296)
('test Acc:', 0.791682)
model_080 F1_score: 83.03% >>>
527
('Epoch Training Loss:', 0.0055567505442013498)
('Epoch Training Acc:', 1.0)
('test loss', 1.1972558)
('test Acc:', 0.79131395)
model_080 F1_score: 82.88% >>>
528
('Epoch Training Loss:', 0.0050224071119373548)
('Epoch Training Acc:', 1.0)
('test loss', 1.2057685)
('test Acc:', 0.79352224)
model_080 F1_score: 83.11% >>>
529
('Epoch Training Loss:', 0.0053880000950812246)
('Epoch Training Acc:', 1.0)
('test loss', 1.1771519)
('test Acc:', 0.79425836)
model_080 F1_score: 83.34% >>>
530
('Epoch Training Loss:', 0.0052427669370445074)
('Epoch Training Acc:', 1.0)
('test loss', 1.1924378)
('test Acc:', 0.79205006)
model_080 F1_score: 83.23% >>>
531
('Epoch Training Loss:', 0.0053947928181514726)
('Epoch Training Acc:', 1.0)
('test loss', 1.1834949)
('test Acc:', 0.79793888)
model_080 F1_score: 83.52% >>>
532
('Epoch Training Loss:', 0.0054564914526054054)
('Epoch Training Acc:', 1.0)
('test loss', 1.1945151)
('test Acc:', 0.7887376)
model_080 F1_score: 82.89% >>>
533
('Epoch Training Loss:', 0.0054566212038480444)
('Epoch Training Acc:', 1.0)
('test loss', 1.1952074)
('test Acc:', 0.78984171)
model_080 F1_score: 82.63% >>>
534
('Epoch Training Loss:', 0.0051009060753131052)
('Epoch Training Acc:', 1.0)
('test loss', 1.1946455)
('test Acc:', 0.78616118)
model_080 F1_score: 82.62% >>>
535
('Epoch Training Loss:', 0.0052006161404278828)
('Epoch Training Acc:', 1.0)
('test loss', 1.2145742)
('test Acc:', 0.78726536)
model_080 F1_score: 82.67% >>>
536
('Epoch Training Loss:', 0.0051989471385240904)
('Epoch Training Acc:', 1.0)
('test loss', 1.2006642)
('test Acc:', 0.79094589)
model_080 F1_score: 83.02% >>>
537
('Epoch Training Loss:', 0.0052325164388093981)
('Epoch Training Acc:', 1.0)
('test loss', 1.2011597)
('test Acc:', 0.78910565)
model_080 F1_score: 82.79% >>>
538
('Epoch Training Loss:', 0.0051403976176516153)
('Epoch Training Acc:', 1.0)
('test loss', 1.1977551)
('test Acc:', 0.79057783)
model_080 F1_score: 82.95% >>>
539
('Epoch Training Loss:', 0.0050500248089520028)
('Epoch Training Acc:', 1.0)
('test loss', 1.2023185)
('test Acc:', 0.79020977)
model_080 F1_score: 82.90% >>>
540
('Epoch Training Loss:', 0.0053871263507971889)
('Epoch Training Acc:', 1.0)
('test loss', 1.2052331)
('test Acc:', 0.78726536)
model_080 F1_score: 82.50% >>>
541
('Epoch Training Loss:', 0.0052043496771148057)
('Epoch Training Acc:', 1.0)
('test loss', 1.1842499)
('test Acc:', 0.79241812)
model_080 F1_score: 82.90% >>>
542
('Epoch Training Loss:', 0.0050342795420874609)
('Epoch Training Acc:', 1.0)
('test loss', 1.2003495)
('test Acc:', 0.7887376)
model_080 F1_score: 82.84% >>>
543
('Epoch Training Loss:', 0.004891443983069621)
('Epoch Training Acc:', 1.0)
('test loss', 1.1967911)
('test Acc:', 0.78984171)
model_080 F1_score: 82.77% >>>
544
('Epoch Training Loss:', 0.0053412261204357492)
('Epoch Training Acc:', 1.0)
('test loss', 1.2104406)
('test Acc:', 0.78800148)
model_080 F1_score: 82.74% >>>
545
('Epoch Training Loss:', 0.0050806979061235324)
('Epoch Training Acc:', 1.0)
('test loss', 1.1972108)
('test Acc:', 0.79131395)
model_080 F1_score: 82.96% >>>
546
('Epoch Training Loss:', 0.0054109201601022505)
('Epoch Training Acc:', 1.0)
('test loss', 1.1881391)
('test Acc:', 0.79094589)
model_080 F1_score: 83.08% >>>
547
('Epoch Training Loss:', 0.0049078182519224356)
('Epoch Training Acc:', 1.0)
('test loss', 1.2117872)
('test Acc:', 0.78800148)
model_080 F1_score: 82.63% >>>
548
('Epoch Training Loss:', 0.0049570950204724795)
('Epoch Training Acc:', 1.0)
('test loss', 1.2156233)
('test Acc:', 0.7887376)
model_080 F1_score: 82.69% >>>
549
('Epoch Training Loss:', 0.0049813170207926305)
('Epoch Training Acc:', 1.0)
('test loss', 1.1975545)
('test Acc:', 0.791682)
model_080 F1_score: 83.00% >>>
550
('Epoch Training Loss:', 0.0053103585451026447)
('Epoch Training Acc:', 1.0)
('test loss', 1.2020899)
('test Acc:', 0.78800148)
model_080 F1_score: 82.87% >>>
551
('Epoch Training Loss:', 0.0051415697498669033)
('Epoch Training Acc:', 1.0)
('test loss', 1.1877143)
('test Acc:', 0.79425836)
model_080 F1_score: 83.28% >>>
552
('Epoch Training Loss:', 0.0047763714374013944)
('Epoch Training Acc:', 1.0)
('test loss', 1.211529)
('test Acc:', 0.7887376)
model_080 F1_score: 82.95% >>>
553
('Epoch Training Loss:', 0.0048636692863510689)
('Epoch Training Acc:', 1.0)
('test loss', 1.204741)
('test Acc:', 0.78432095)
model_080 F1_score: 82.43% >>>
554
('Epoch Training Loss:', 0.0050637714539334411)
('Epoch Training Acc:', 1.0)
('test loss', 1.2112603)
('test Acc:', 0.79020977)
model_080 F1_score: 82.73% >>>
555
('Epoch Training Loss:', 0.0054473345098813297)
('Epoch Training Acc:', 1.0)
('test loss', 1.186728)
('test Acc:', 0.79057783)
model_080 F1_score: 82.85% >>>
556
('Epoch Training Loss:', 0.0048293028821717598)
('Epoch Training Acc:', 1.0)
('test loss', 1.2052521)
('test Acc:', 0.78763342)
model_080 F1_score: 82.76% >>>
557
('Epoch Training Loss:', 0.005114160251650901)
('Epoch Training Acc:', 1.0)
('test loss', 1.1960377)
('test Acc:', 0.79425836)
model_080 F1_score: 83.30% >>>
558
('Epoch Training Loss:', 0.0051752072322415188)
('Epoch Training Acc:', 1.0)
('test loss', 1.1926589)
('test Acc:', 0.78947371)
model_080 F1_score: 82.72% >>>
559
('Epoch Training Loss:', 0.0048860931456147227)
('Epoch Training Acc:', 1.0)
('test loss', 1.2020457)
('test Acc:', 0.79094589)
model_080 F1_score: 82.80% >>>
560
('Epoch Training Loss:', 0.004917192426546535)
('Epoch Training Acc:', 1.0)
('test loss', 1.1943769)
('test Acc:', 0.79315424)
model_080 F1_score: 83.35% >>>
561
('Epoch Training Loss:', 0.0050366995610602316)
('Epoch Training Acc:', 1.0)
('test loss', 1.1916432)
('test Acc:', 0.79241812)
model_080 F1_score: 83.29% >>>
562
('Epoch Training Loss:', 0.0048204665381490486)
('Epoch Training Acc:', 1.0)
('test loss', 1.177978)
('test Acc:', 0.79094589)
model_080 F1_score: 82.96% >>>
563
('Epoch Training Loss:', 0.0051115389196638716)
('Epoch Training Acc:', 1.0)
('test loss', 1.1864623)
('test Acc:', 0.79131395)
model_080 F1_score: 82.73% >>>
564
('Epoch Training Loss:', 0.0046906355610190076)
('Epoch Training Acc:', 1.0)
('test loss', 1.2239031)
('test Acc:', 0.7887376)
model_080 F1_score: 82.76% >>>
565
('Epoch Training Loss:', 0.0047197364865496638)
('Epoch Training Acc:', 1.0)
('test loss', 1.2048107)
('test Acc:', 0.7887376)
model_080 F1_score: 82.63% >>>
566
('Epoch Training Loss:', 0.0048810402413437259)
('Epoch Training Acc:', 1.0)
('test loss', 1.1926979)
('test Acc:', 0.79131395)
model_080 F1_score: 83.20% >>>
567
('Epoch Training Loss:', 0.0047731118265801342)
('Epoch Training Acc:', 1.0)
('test loss', 1.201288)
('test Acc:', 0.7887376)
model_080 F1_score: 82.59% >>>
568
('Epoch Training Loss:', 0.0048228121067950269)
('Epoch Training Acc:', 1.0)
('test loss', 1.2169207)
('test Acc:', 0.78726536)
model_080 F1_score: 82.68% >>>
569
('Epoch Training Loss:', 0.0049174484029208543)
('Epoch Training Acc:', 1.0)
('test loss', 1.1915071)
('test Acc:', 0.79205006)
model_080 F1_score: 83.00% >>>
570
('Epoch Training Loss:', 0.004840116886953183)
('Epoch Training Acc:', 1.0)
('test loss', 1.199816)
('test Acc:', 0.78836954)
model_080 F1_score: 82.96% >>>
571
('Epoch Training Loss:', 0.0048920715662461589)
('Epoch Training Acc:', 1.0)
('test loss', 1.1938037)
('test Acc:', 0.79131395)
model_080 F1_score: 83.14% >>>
572
('Epoch Training Loss:', 0.0049667039020278025)
('Epoch Training Acc:', 1.0)
('test loss', 1.1957768)
('test Acc:', 0.79425836)
model_080 F1_score: 83.21% >>>
573
('Epoch Training Loss:', 0.0048796866676639183)
('Epoch Training Acc:', 1.0)
('test loss', 1.1944551)
('test Acc:', 0.79131395)
model_080 F1_score: 82.86% >>>
574
('Epoch Training Loss:', 0.0046220663707572385)
('Epoch Training Acc:', 1.0)
('test loss', 1.2130828)
('test Acc:', 0.791682)
model_080 F1_score: 82.92% >>>
575
('Epoch Training Loss:', 0.0048529604509894853)
('Epoch Training Acc:', 1.0)
('test loss', 1.1930479)
('test Acc:', 0.78984171)
model_080 F1_score: 82.73% >>>
576
('Epoch Training Loss:', 0.0048060973922474659)
('Epoch Training Acc:', 1.0)
('test loss', 1.1954795)
('test Acc:', 0.79241812)
model_080 F1_score: 82.98% >>>
577
('Epoch Training Loss:', 0.0046564634549213224)
('Epoch Training Acc:', 1.0)
('test loss', 1.2071682)
('test Acc:', 0.78763342)
model_080 F1_score: 82.73% >>>
578
('Epoch Training Loss:', 0.004579324147925945)
('Epoch Training Acc:', 1.0)
('test loss', 1.1779004)
('test Acc:', 0.79315424)
model_080 F1_score: 83.15% >>>
579
('Epoch Training Loss:', 0.0047791429578865063)
('Epoch Training Acc:', 1.0)
('test loss', 1.1865598)
('test Acc:', 0.7887376)
model_080 F1_score: 82.57% >>>
580
('Epoch Training Loss:', 0.0045155988464102848)
('Epoch Training Acc:', 1.0)
('test loss', 1.2097011)
('test Acc:', 0.79057783)
model_080 F1_score: 82.90% >>>
581
('Epoch Training Loss:', 0.0048063708527479321)
('Epoch Training Acc:', 1.0)
('test loss', 1.2109885)
('test Acc:', 0.78579313)
model_080 F1_score: 82.59% >>>
582
('Epoch Training Loss:', 0.0048455016030857223)
('Epoch Training Acc:', 1.0)
('test loss', 1.1977993)
('test Acc:', 0.79131395)
model_080 F1_score: 82.76% >>>
583
('Epoch Training Loss:', 0.0047194324706651969)
('Epoch Training Acc:', 1.0)
('test loss', 1.2160226)
('test Acc:', 0.79020977)
model_080 F1_score: 82.75% >>>
584
('Epoch Training Loss:', 0.0046768309557592147)
('Epoch Training Acc:', 1.0)
('test loss', 1.1833212)
('test Acc:', 0.79278618)
model_080 F1_score: 83.13% >>>
585
('Epoch Training Loss:', 0.0048831527465154068)
('Epoch Training Acc:', 1.0)
('test loss', 1.207896)
('test Acc:', 0.78579313)
model_080 F1_score: 82.61% >>>
586
('Epoch Training Loss:', 0.0046267080415418604)
('Epoch Training Acc:', 1.0)
('test loss', 1.2042419)
('test Acc:', 0.79241812)
model_080 F1_score: 82.89% >>>
587
('Epoch Training Loss:', 0.0047714154397908715)
('Epoch Training Acc:', 1.0)
('test loss', 1.191305)
('test Acc:', 0.79315424)
model_080 F1_score: 83.21% >>>
588
('Epoch Training Loss:', 0.004669701624152367)
('Epoch Training Acc:', 1.0)
('test loss', 1.2080017)
('test Acc:', 0.78248066)
model_080 F1_score: 82.20% >>>
589
('Epoch Training Loss:', 0.0044578964389074827)
('Epoch Training Acc:', 1.0)
('test loss', 1.191557)
('test Acc:', 0.791682)
model_080 F1_score: 83.05% >>>
590
('Epoch Training Loss:', 0.0046268528512882767)
('Epoch Training Acc:', 1.0)
('test loss', 1.2079096)
('test Acc:', 0.791682)
model_080 F1_score: 83.13% >>>
591
('Epoch Training Loss:', 0.0045899346341684577)
('Epoch Training Acc:', 1.0)
('test loss', 1.227486)
('test Acc:', 0.79131395)
model_080 F1_score: 83.10% >>>
592
('Epoch Training Loss:', 0.0047745424290042138)
('Epoch Training Acc:', 1.0)
('test loss', 1.212556)
('test Acc:', 0.79094589)
model_080 F1_score: 83.12% >>>
593
('Epoch Training Loss:', 0.0045895936482338584)
('Epoch Training Acc:', 1.0)
('test loss', 1.2021482)
('test Acc:', 0.7887376)
model_080 F1_score: 82.59% >>>
594
('Epoch Training Loss:', 0.0046171991098162835)
('Epoch Training Acc:', 1.0)
('test loss', 1.2231838)
('test Acc:', 0.78542513)
model_080 F1_score: 82.65% >>>
595
('Epoch Training Loss:', 0.0046512231938322657)
('Epoch Training Acc:', 1.0)
('test loss', 1.2211738)
('test Acc:', 0.78616118)
model_080 F1_score: 82.60% >>>
596
('Epoch Training Loss:', 0.0044822420604759827)
('Epoch Training Acc:', 1.0)
('test loss', 1.2073543)
('test Acc:', 0.79352224)
model_080 F1_score: 83.05% >>>
597
('Epoch Training Loss:', 0.0046072733557593892)
('Epoch Training Acc:', 1.0)
('test loss', 1.2081492)
('test Acc:', 0.78800148)
model_080 F1_score: 82.77% >>>
598
('Epoch Training Loss:', 0.0045740207242488395)
('Epoch Training Acc:', 1.0)
('test loss', 1.1780657)
('test Acc:', 0.79646671)
model_080 F1_score: 83.57% >>>
599
('Epoch Training Loss:', 0.0045100227898728917)
('Epoch Training Acc:', 1.0)
('test loss', 1.2193443)
('test Acc:', 0.78726536)
model_080 F1_score: 82.73% >>>
600
('Epoch Training Loss:', 0.0045949539553475915)
('Epoch Training Acc:', 1.0)
('test loss', 1.2089217)
('test Acc:', 0.78395289)
model_080 F1_score: 82.37% >>>
601
('Epoch Training Loss:', 0.0046466075909847859)
('Epoch Training Acc:', 1.0)
('test loss', 1.2217729)
('test Acc:', 0.791682)
model_080 F1_score: 83.10% >>>
602
('Epoch Training Loss:', 0.004568495171952236)
('Epoch Training Acc:', 1.0)
('test loss', 1.2035278)
('test Acc:', 0.79020977)
model_080 F1_score: 83.05% >>>
603
('Epoch Training Loss:', 0.0048382305030827411)
('Epoch Training Acc:', 1.0)
('test loss', 1.2109207)
('test Acc:', 0.78836954)
model_080 F1_score: 82.47% >>>
604
('Epoch Training Loss:', 0.004557156815280905)
('Epoch Training Acc:', 1.0)
('test loss', 1.1856693)
('test Acc:', 0.78984171)
model_080 F1_score: 82.83% >>>
605
('Epoch Training Loss:', 0.0045402308569464367)
('Epoch Training Acc:', 1.0)
('test loss', 1.212621)
('test Acc:', 0.78468901)
model_080 F1_score: 82.29% >>>
606
('Epoch Training Loss:', 0.004600730947458942)
('Epoch Training Acc:', 1.0)
('test loss', 1.2113336)
('test Acc:', 0.79499447)
model_080 F1_score: 83.33% >>>
607
('Epoch Training Loss:', 0.0045242374790177564)
('Epoch Training Acc:', 1.0)
('test loss', 1.2101501)
('test Acc:', 0.79646671)
model_080 F1_score: 83.34% >>>
608
('Epoch Training Loss:', 0.0044680156142931082)
('Epoch Training Acc:', 1.0)
('test loss', 1.1989719)
('test Acc:', 0.79646671)
model_080 F1_score: 83.38% >>>
609
('Epoch Training Loss:', 0.0045649888288608054)
('Epoch Training Acc:', 1.0)
('test loss', 1.2059301)
('test Acc:', 0.78910565)
model_080 F1_score: 82.85% >>>
610
('Epoch Training Loss:', 0.0045165889787313063)
('Epoch Training Acc:', 1.0)
('test loss', 1.207886)
('test Acc:', 0.79057783)
model_080 F1_score: 82.77% >>>
611
('Epoch Training Loss:', 0.0046078854566076188)
('Epoch Training Acc:', 1.0)
('test loss', 1.2211143)
('test Acc:', 0.7887376)
model_080 F1_score: 82.93% >>>
612
('Epoch Training Loss:', 0.0044862637369078584)
('Epoch Training Acc:', 1.0)
('test loss', 1.2226765)
('test Acc:', 0.78910565)
model_080 F1_score: 82.71% >>>
613
('Epoch Training Loss:', 0.004251643837960728)
('Epoch Training Acc:', 1.0)
('test loss', 1.1975796)
('test Acc:', 0.78836954)
model_080 F1_score: 82.85% >>>
614
('Epoch Training Loss:', 0.0045062795725243632)
('Epoch Training Acc:', 1.0)
('test loss', 1.207481)
('test Acc:', 0.7887376)
model_080 F1_score: 83.15% >>>
615
('Epoch Training Loss:', 0.0044842299203082803)
('Epoch Training Acc:', 1.0)
('test loss', 1.2077289)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
616
('Epoch Training Loss:', 0.0044292456941548153)
('Epoch Training Acc:', 1.0)
('test loss', 1.2282553)
('test Acc:', 0.78910565)
model_080 F1_score: 82.71% >>>
617
('Epoch Training Loss:', 0.004617217622580938)
('Epoch Training Acc:', 1.0)
('test loss', 1.2124761)
('test Acc:', 0.79315424)
model_080 F1_score: 83.05% >>>
618
('Epoch Training Loss:', 0.0044928969837201294)
('Epoch Training Acc:', 1.0)
('test loss', 1.1978234)
('test Acc:', 0.79573059)
model_080 F1_score: 83.46% >>>
619
('Epoch Training Loss:', 0.0044568002331288881)
('Epoch Training Acc:', 1.0)
('test loss', 1.199839)
('test Acc:', 0.79278618)
model_080 F1_score: 83.20% >>>
620
('Epoch Training Loss:', 0.004422134637934505)
('Epoch Training Acc:', 1.0)
('test loss', 1.2059307)
('test Acc:', 0.791682)
model_080 F1_score: 83.03% >>>
621
('Epoch Training Loss:', 0.0044613227883019135)
('Epoch Training Acc:', 1.0)
('test loss', 1.2163059)
('test Acc:', 0.78726536)
model_080 F1_score: 82.53% >>>
622
('Epoch Training Loss:', 0.0043851690124938614)
('Epoch Training Acc:', 1.0)
('test loss', 1.2163504)
('test Acc:', 0.78910565)
model_080 F1_score: 82.88% >>>
623
('Epoch Training Loss:', 0.0046944047562647029)
('Epoch Training Acc:', 1.0)
('test loss', 1.2017152)
('test Acc:', 0.79057783)
model_080 F1_score: 82.97% >>>
624
('Epoch Training Loss:', 0.0044124957830717904)
('Epoch Training Acc:', 1.0)
('test loss', 1.1959958)
('test Acc:', 0.78910565)
model_080 F1_score: 82.95% >>>
625
('Epoch Training Loss:', 0.0045842369554520701)
('Epoch Training Acc:', 1.0)
('test loss', 1.2201793)
('test Acc:', 0.78910565)
model_080 F1_score: 82.94% >>>
626
('Epoch Training Loss:', 0.0043792930036943289)
('Epoch Training Acc:', 1.0)
('test loss', 1.1996309)
('test Acc:', 0.79241812)
model_080 F1_score: 83.16% >>>
627
('Epoch Training Loss:', 0.004637498282136221)
('Epoch Training Acc:', 1.0)
('test loss', 1.2174364)
('test Acc:', 0.79241812)
model_080 F1_score: 83.00% >>>
628
('Epoch Training Loss:', 0.0044260533322812989)
('Epoch Training Acc:', 1.0)
('test loss', 1.2161766)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
629
('Epoch Training Loss:', 0.0043052200508100213)
('Epoch Training Acc:', 1.0)
('test loss', 1.2144963)
('test Acc:', 0.79241812)
model_080 F1_score: 83.13% >>>
630
('Epoch Training Loss:', 0.0044687646959573613)
('Epoch Training Acc:', 1.0)
('test loss', 1.197687)
('test Acc:', 0.78836954)
model_080 F1_score: 82.64% >>>
631
('Epoch Training Loss:', 0.0043035097387473797)
('Epoch Training Acc:', 1.0)
('test loss', 1.2052855)
('test Acc:', 0.79536253)
model_080 F1_score: 83.36% >>>
632
('Epoch Training Loss:', 0.0042666764456953388)
('Epoch Training Acc:', 1.0)
('test loss', 1.2135155)
('test Acc:', 0.79315424)
model_080 F1_score: 83.08% >>>
633
('Epoch Training Loss:', 0.0044109321734140394)
('Epoch Training Acc:', 1.0)
('test loss', 1.2058921)
('test Acc:', 0.79499447)
model_080 F1_score: 83.28% >>>
634
('Epoch Training Loss:', 0.0042541585762592149)
('Epoch Training Acc:', 1.0)
('test loss', 1.2080364)
('test Acc:', 0.79241812)
model_080 F1_score: 83.02% >>>
635
('Epoch Training Loss:', 0.0043569759100137162)
('Epoch Training Acc:', 1.0)
('test loss', 1.2202634)
('test Acc:', 0.79425836)
model_080 F1_score: 83.21% >>>
636
('Epoch Training Loss:', 0.0041844855950330384)
('Epoch Training Acc:', 1.0)
('test loss', 1.2142417)
('test Acc:', 0.78432095)
model_080 F1_score: 82.51% >>>
637
('Epoch Training Loss:', 0.0046286283168228692)
('Epoch Training Acc:', 1.0)
('test loss', 1.1963768)
('test Acc:', 0.79352224)
model_080 F1_score: 83.28% >>>
638
('Epoch Training Loss:', 0.0041988427592514199)
('Epoch Training Acc:', 1.0)
('test loss', 1.1923779)
('test Acc:', 0.78910565)
model_080 F1_score: 82.79% >>>
639
('Epoch Training Loss:', 0.0041985713296526228)
('Epoch Training Acc:', 1.0)
('test loss', 1.1950024)
('test Acc:', 0.7938903)
model_080 F1_score: 83.10% >>>
640
('Epoch Training Loss:', 0.0045280162830749759)
('Epoch Training Acc:', 1.0)
('test loss', 1.2248824)
('test Acc:', 0.79352224)
model_080 F1_score: 83.31% >>>
641
('Epoch Training Loss:', 0.0042518543295955169)
('Epoch Training Acc:', 1.0)
('test loss', 1.2217726)
('test Acc:', 0.79241812)
model_080 F1_score: 83.16% >>>
642
('Epoch Training Loss:', 0.0042227942522004014)
('Epoch Training Acc:', 1.0)
('test loss', 1.2126389)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
643
('Epoch Training Loss:', 0.0042362462245364441)
('Epoch Training Acc:', 1.0)
('test loss', 1.2039807)
('test Acc:', 0.79278618)
model_080 F1_score: 83.07% >>>
644
('Epoch Training Loss:', 0.0042181978251392138)
('Epoch Training Acc:', 1.0)
('test loss', 1.2171733)
('test Acc:', 0.78910565)
model_080 F1_score: 82.67% >>>
645
('Epoch Training Loss:', 0.0041878759457176784)
('Epoch Training Acc:', 1.0)
('test loss', 1.210704)
('test Acc:', 0.78984171)
model_080 F1_score: 83.03% >>>
646
('Epoch Training Loss:', 0.0042426795580468024)
('Epoch Training Acc:', 1.0)
('test loss', 1.2143942)
('test Acc:', 0.78321677)
model_080 F1_score: 82.44% >>>
647
('Epoch Training Loss:', 0.0047614217055524932)
('Epoch Training Acc:', 1.0)
('test loss', 1.232101)
('test Acc:', 0.78763342)
model_080 F1_score: 82.77% >>>
648
('Epoch Training Loss:', 0.0043340087886463152)
('Epoch Training Acc:', 1.0)
('test loss', 1.2317345)
('test Acc:', 0.79094589)
model_080 F1_score: 83.07% >>>
649
('Epoch Training Loss:', 0.0039267383708647685)
('Epoch Training Acc:', 1.0)
('test loss', 1.2227839)
('test Acc:', 0.78579313)
model_080 F1_score: 82.59% >>>
650
('Epoch Training Loss:', 0.0043776850534413825)
('Epoch Training Acc:', 1.0)
('test loss', 1.2118996)
('test Acc:', 0.78984171)
model_080 F1_score: 83.00% >>>
651
('Epoch Training Loss:', 0.0040943741969385883)
('Epoch Training Acc:', 1.0)
('test loss', 1.2199014)
('test Acc:', 0.79020977)
model_080 F1_score: 82.97% >>>
652
('Epoch Training Loss:', 0.004183297727649915)
('Epoch Training Acc:', 1.0)
('test loss', 1.2063198)
('test Acc:', 0.79573059)
model_080 F1_score: 83.33% >>>
653
('Epoch Training Loss:', 0.0040314892557944404)
('Epoch Training Acc:', 1.0)
('test loss', 1.2019216)
('test Acc:', 0.79462641)
model_080 F1_score: 83.19% >>>
654
('Epoch Training Loss:', 0.003970830204707454)
('Epoch Training Acc:', 1.0)
('test loss', 1.2054092)
('test Acc:', 0.79720283)
model_080 F1_score: 83.34% >>>
655
('Epoch Training Loss:', 0.0043137777593074134)
('Epoch Training Acc:', 1.0)
('test loss', 1.1913615)
('test Acc:', 0.79720283)
model_080 F1_score: 83.42% >>>
656
('Epoch Training Loss:', 0.0042841149752348429)
('Epoch Training Acc:', 1.0)
('test loss', 1.2258722)
('test Acc:', 0.79536253)
model_080 F1_score: 83.19% >>>
657
('Epoch Training Loss:', 0.0041009994010892115)
('Epoch Training Acc:', 1.0)
('test loss', 1.2145325)
('test Acc:', 0.79057783)
model_080 F1_score: 82.75% >>>
658
('Epoch Training Loss:', 0.0040210505831055343)
('Epoch Training Acc:', 1.0)
('test loss', 1.2393391)
('test Acc:', 0.78800148)
model_080 F1_score: 82.83% >>>
659
('Epoch Training Loss:', 0.0041771952865019557)
('Epoch Training Acc:', 1.0)
('test loss', 1.2321967)
('test Acc:', 0.79057783)
model_080 F1_score: 82.86% >>>
660
('Epoch Training Loss:', 0.0040418771905024187)
('Epoch Training Acc:', 1.0)
('test loss', 1.2190889)
('test Acc:', 0.79573059)
model_080 F1_score: 83.32% >>>
661
('Epoch Training Loss:', 0.0039349420039798133)
('Epoch Training Acc:', 1.0)
('test loss', 1.2185456)
('test Acc:', 0.78726536)
model_080 F1_score: 82.67% >>>
662
('Epoch Training Loss:', 0.0039416478775819996)
('Epoch Training Acc:', 1.0)
('test loss', 1.231514)
('test Acc:', 0.78910565)
model_080 F1_score: 82.57% >>>
663
('Epoch Training Loss:', 0.0041376016697540763)
('Epoch Training Acc:', 1.0)
('test loss', 1.2197437)
('test Acc:', 0.78947371)
model_080 F1_score: 82.69% >>>
664
('Epoch Training Loss:', 0.0042482698891035398)
('Epoch Training Acc:', 1.0)
('test loss', 1.2004035)
('test Acc:', 0.798675)
model_080 F1_score: 83.60% >>>
665
('Epoch Training Loss:', 0.0042013070678876829)
('Epoch Training Acc:', 1.0)
('test loss', 1.2079971)
('test Acc:', 0.78726536)
model_080 F1_score: 82.56% >>>
666
('Epoch Training Loss:', 0.0041498207665426889)
('Epoch Training Acc:', 1.0)
('test loss', 1.2280658)
('test Acc:', 0.7887376)
model_080 F1_score: 82.79% >>>
667
('Epoch Training Loss:', 0.0039896630914881825)
('Epoch Training Acc:', 1.0)
('test loss', 1.2092736)
('test Acc:', 0.79020977)
model_080 F1_score: 82.99% >>>
668
('Epoch Training Loss:', 0.0041517097633914091)
('Epoch Training Acc:', 1.0)
('test loss', 1.2172393)
('test Acc:', 0.79352224)
model_080 F1_score: 83.07% >>>
669
('Epoch Training Loss:', 0.0039809387844798039)
('Epoch Training Acc:', 1.0)
('test loss', 1.2018527)
('test Acc:', 0.79315424)
model_080 F1_score: 83.33% >>>
670
('Epoch Training Loss:', 0.0042157241023232928)
('Epoch Training Acc:', 1.0)
('test loss', 1.2209259)
('test Acc:', 0.78800148)
model_080 F1_score: 82.77% >>>
671
('Epoch Training Loss:', 0.0040686100355742383)
('Epoch Training Acc:', 1.0)
('test loss', 1.2019618)
('test Acc:', 0.79205006)
model_080 F1_score: 83.02% >>>
672
('Epoch Training Loss:', 0.0041565492492736666)
('Epoch Training Acc:', 1.0)
('test loss', 1.2109976)
('test Acc:', 0.78579313)
model_080 F1_score: 82.64% >>>
673
('Epoch Training Loss:', 0.0040646103816470713)
('Epoch Training Acc:', 1.0)
('test loss', 1.2210927)
('test Acc:', 0.78836954)
model_080 F1_score: 82.94% >>>
674
('Epoch Training Loss:', 0.0039440182699763682)
('Epoch Training Acc:', 1.0)
('test loss', 1.2197617)
('test Acc:', 0.791682)
model_080 F1_score: 83.00% >>>
675
('Epoch Training Loss:', 0.0039639702781641972)
('Epoch Training Acc:', 1.0)
('test loss', 1.2369932)
('test Acc:', 0.78616118)
model_080 F1_score: 82.62% >>>
676
('Epoch Training Loss:', 0.003997875109234883)
('Epoch Training Acc:', 1.0)
('test loss', 1.2291867)
('test Acc:', 0.79131395)
model_080 F1_score: 82.83% >>>
677
('Epoch Training Loss:', 0.0041049708534046658)
('Epoch Training Acc:', 1.0)
('test loss', 1.2162179)
('test Acc:', 0.79205006)
model_080 F1_score: 83.10% >>>
678
('Epoch Training Loss:', 0.0041055346291614114)
('Epoch Training Acc:', 1.0)
('test loss', 1.2335408)
('test Acc:', 0.78652924)
model_080 F1_score: 82.55% >>>
679
('Epoch Training Loss:', 0.0040292210960615193)
('Epoch Training Acc:', 1.0)
('test loss', 1.2295827)
('test Acc:', 0.79057783)
model_080 F1_score: 82.87% >>>
680
('Epoch Training Loss:', 0.0040786644321997301)
('Epoch Training Acc:', 1.0)
('test loss', 1.2247546)
('test Acc:', 0.79131395)
model_080 F1_score: 82.79% >>>
681
('Epoch Training Loss:', 0.0038086678887339076)
('Epoch Training Acc:', 1.0)
('test loss', 1.2131227)
('test Acc:', 0.79020977)
model_080 F1_score: 82.64% >>>
682
('Epoch Training Loss:', 0.0040276826221088413)
('Epoch Training Acc:', 1.0)
('test loss', 1.2213879)
('test Acc:', 0.79057783)
model_080 F1_score: 82.98% >>>
683
('Epoch Training Loss:', 0.003991489282270777)
('Epoch Training Acc:', 1.0)
('test loss', 1.2257441)
('test Acc:', 0.78984171)
model_080 F1_score: 83.08% >>>
684
('Epoch Training Loss:', 0.0039963143763088738)
('Epoch Training Acc:', 1.0)
('test loss', 1.2029588)
('test Acc:', 0.79205006)
model_080 F1_score: 83.13% >>>
685
('Epoch Training Loss:', 0.0041529045656716335)
('Epoch Training Acc:', 1.0)
('test loss', 1.2429808)
('test Acc:', 0.78652924)
model_080 F1_score: 82.67% >>>
686
('Epoch Training Loss:', 0.0039843011018092511)
('Epoch Training Acc:', 1.0)
('test loss', 1.2240856)
('test Acc:', 0.78910565)
model_080 F1_score: 82.67% >>>
687
('Epoch Training Loss:', 0.0039538785367767559)
('Epoch Training Acc:', 1.0)
('test loss', 1.2078708)
('test Acc:', 0.79020977)
model_080 F1_score: 82.84% >>>
688
('Epoch Training Loss:', 0.004053358821238362)
('Epoch Training Acc:', 1.0)
('test loss', 1.2139771)
('test Acc:', 0.79352224)
model_080 F1_score: 83.19% >>>
689
('Epoch Training Loss:', 0.0038715241544196033)
('Epoch Training Acc:', 1.0)
('test loss', 1.2224988)
('test Acc:', 0.79205006)
model_080 F1_score: 82.90% >>>
690
('Epoch Training Loss:', 0.004132763436246023)
('Epoch Training Acc:', 1.0)
('test loss', 1.2188619)
('test Acc:', 0.79131395)
model_080 F1_score: 82.98% >>>
691
('Epoch Training Loss:', 0.0039540746438433416)
('Epoch Training Acc:', 1.0)
('test loss', 1.2055734)
('test Acc:', 0.79720283)
model_080 F1_score: 83.38% >>>
692
('Epoch Training Loss:', 0.0039527021090179915)
('Epoch Training Acc:', 1.0)
('test loss', 1.2100334)
('test Acc:', 0.79499447)
model_080 F1_score: 83.42% >>>
693
('Epoch Training Loss:', 0.003938197413845046)
('Epoch Training Acc:', 1.0)
('test loss', 1.2200183)
('test Acc:', 0.78836954)
model_080 F1_score: 82.98% >>>
694
('Epoch Training Loss:', 0.0037726127729911241)
('Epoch Training Acc:', 1.0)
('test loss', 1.2068381)
('test Acc:', 0.79205006)
model_080 F1_score: 83.13% >>>
695
('Epoch Training Loss:', 0.0040923147907960811)
('Epoch Training Acc:', 1.0)
('test loss', 1.2163622)
('test Acc:', 0.7938903)
model_080 F1_score: 83.39% >>>
696
('Epoch Training Loss:', 0.0039943983529155958)
('Epoch Training Acc:', 1.0)
('test loss', 1.20469)
('test Acc:', 0.79462641)
model_080 F1_score: 83.18% >>>
697
('Epoch Training Loss:', 0.0039402407137458795)
('Epoch Training Acc:', 1.0)
('test loss', 1.2164161)
('test Acc:', 0.78910565)
model_080 F1_score: 82.90% >>>
698
('Epoch Training Loss:', 0.0039339817358268192)
('Epoch Training Acc:', 1.0)
('test loss', 1.211163)
('test Acc:', 0.79094589)
model_080 F1_score: 83.14% >>>
699
('Epoch Training Loss:', 0.0040226238188552088)
('Epoch Training Acc:', 1.0)
('test loss', 1.2168875)
('test Acc:', 0.79020977)
model_080 F1_score: 82.93% >>>
700
('Epoch Training Loss:', 0.0039516561837444897)
('Epoch Training Acc:', 1.0)
('test loss', 1.2041373)
('test Acc:', 0.78468901)
model_080 F1_score: 82.23% >>>
701
('Epoch Training Loss:', 0.0039078097543097101)
('Epoch Training Acc:', 1.0)
('test loss', 1.2261959)
('test Acc:', 0.79057783)
model_080 F1_score: 82.99% >>>
702
('Epoch Training Loss:', 0.0037093495866429294)
('Epoch Training Acc:', 1.0)
('test loss', 1.2187796)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
703
('Epoch Training Loss:', 0.0037926892637187848)
('Epoch Training Acc:', 1.0)
('test loss', 1.2380055)
('test Acc:', 0.79205006)
model_080 F1_score: 83.15% >>>
704
('Epoch Training Loss:', 0.0038982283776931581)
('Epoch Training Acc:', 1.0)
('test loss', 1.2360536)
('test Acc:', 0.791682)
model_080 F1_score: 83.28% >>>
705
('Epoch Training Loss:', 0.0037774635729874717)
('Epoch Training Acc:', 1.0)
('test loss', 1.214366)
('test Acc:', 0.78984171)
model_080 F1_score: 82.64% >>>
706
('Epoch Training Loss:', 0.0038585370693908771)
('Epoch Training Acc:', 1.0)
('test loss', 1.2351195)
('test Acc:', 0.79094589)
model_080 F1_score: 83.03% >>>
707
('Epoch Training Loss:', 0.0037514178775381879)
('Epoch Training Acc:', 1.0)
('test loss', 1.2021646)
('test Acc:', 0.79057783)
model_080 F1_score: 82.91% >>>
708
('Epoch Training Loss:', 0.0040184216932175332)
('Epoch Training Acc:', 1.0)
('test loss', 1.2326126)
('test Acc:', 0.7938903)
model_080 F1_score: 83.26% >>>
709
('Epoch Training Loss:', 0.0040293145948453457)
('Epoch Training Acc:', 1.0)
('test loss', 1.2162852)
('test Acc:', 0.78984171)
model_080 F1_score: 82.78% >>>
710
('Epoch Training Loss:', 0.0039488923221142613)
('Epoch Training Acc:', 1.0)
('test loss', 1.226262)
('test Acc:', 0.79536253)
model_080 F1_score: 83.46% >>>
711
('Epoch Training Loss:', 0.0037363176888902672)
('Epoch Training Acc:', 1.0)
('test loss', 1.215796)
('test Acc:', 0.78726536)
model_080 F1_score: 82.67% >>>
712
('Epoch Training Loss:', 0.0038849152988404967)
('Epoch Training Acc:', 1.0)
('test loss', 1.2398349)
('test Acc:', 0.78579313)
model_080 F1_score: 82.62% >>>
713
('Epoch Training Loss:', 0.0036616557008528616)
('Epoch Training Acc:', 1.0)
('test loss', 1.2129002)
('test Acc:', 0.79315424)
model_080 F1_score: 83.08% >>>
714
('Epoch Training Loss:', 0.0038643457000944181)
('Epoch Training Acc:', 1.0)
('test loss', 1.2110522)
('test Acc:', 0.79278618)
model_080 F1_score: 83.06% >>>
715
('Epoch Training Loss:', 0.0037953834125801222)
('Epoch Training Acc:', 1.0)
('test loss', 1.2167126)
('test Acc:', 0.79205006)
model_080 F1_score: 83.00% >>>
716
('Epoch Training Loss:', 0.003674329388559272)
('Epoch Training Acc:', 1.0)
('test loss', 1.228338)
('test Acc:', 0.78505707)
model_080 F1_score: 82.63% >>>
717
('Epoch Training Loss:', 0.0039357400437438628)
('Epoch Training Acc:', 1.0)
('test loss', 1.2348838)
('test Acc:', 0.78616118)
model_080 F1_score: 82.75% >>>
718
('Epoch Training Loss:', 0.0038510115000462974)
('Epoch Training Acc:', 1.0)
('test loss', 1.2292591)
('test Acc:', 0.79241812)
model_080 F1_score: 83.05% >>>
719
('Epoch Training Loss:', 0.0037449329229275463)
('Epoch Training Acc:', 1.0)
('test loss', 1.2201366)
('test Acc:', 0.79573059)
model_080 F1_score: 83.34% >>>
720
('Epoch Training Loss:', 0.0037231776441331021)
('Epoch Training Acc:', 1.0)
('test loss', 1.2181991)
('test Acc:', 0.79020977)
model_080 F1_score: 82.77% >>>
721
('Epoch Training Loss:', 0.0039880673111838405)
('Epoch Training Acc:', 1.0)
('test loss', 1.2174435)
('test Acc:', 0.79205006)
model_080 F1_score: 83.04% >>>
722
('Epoch Training Loss:', 0.003625708812251105)
('Epoch Training Acc:', 1.0)
('test loss', 1.2246473)
('test Acc:', 0.79057783)
model_080 F1_score: 82.90% >>>
723
('Epoch Training Loss:', 0.0040072653910101508)
('Epoch Training Acc:', 1.0)
('test loss', 1.2255452)
('test Acc:', 0.7868973)
model_080 F1_score: 82.64% >>>
724
('Epoch Training Loss:', 0.0038470376148325158)
('Epoch Training Acc:', 1.0)
('test loss', 1.2124649)
('test Acc:', 0.79205006)
model_080 F1_score: 83.05% >>>
725
('Epoch Training Loss:', 0.0036944680250599049)
('Epoch Training Acc:', 1.0)
('test loss', 1.2175621)
('test Acc:', 0.7868973)
model_080 F1_score: 82.54% >>>
726
('Epoch Training Loss:', 0.003754411776753841)
('Epoch Training Acc:', 1.0)
('test loss', 1.2319313)
('test Acc:', 0.7938903)
model_080 F1_score: 83.25% >>>
727
('Epoch Training Loss:', 0.0037600068581014057)
('Epoch Training Acc:', 1.0)
('test loss', 1.2305224)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
728
('Epoch Training Loss:', 0.003685078397666075)
('Epoch Training Acc:', 1.0)
('test loss', 1.2320619)
('test Acc:', 0.78763342)
model_080 F1_score: 82.79% >>>
729
('Epoch Training Loss:', 0.0037750231122117839)
('Epoch Training Acc:', 1.0)
('test loss', 1.2185484)
('test Acc:', 0.79609865)
model_080 F1_score: 83.24% >>>
730
('Epoch Training Loss:', 0.0036327168945717858)
('Epoch Training Acc:', 1.0)
('test loss', 1.220318)
('test Acc:', 0.78947371)
model_080 F1_score: 82.84% >>>
731
('Epoch Training Loss:', 0.0035920216241720482)
('Epoch Training Acc:', 1.0)
('test loss', 1.2134842)
('test Acc:', 0.7938903)
model_080 F1_score: 83.18% >>>
732
('Epoch Training Loss:', 0.0039954433295861236)
('Epoch Training Acc:', 1.0)
('test loss', 1.2284015)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
733
('Epoch Training Loss:', 0.0036960046454623807)
('Epoch Training Acc:', 1.0)
('test loss', 1.2414961)
('test Acc:', 0.78910565)
model_080 F1_score: 82.68% >>>
734
('Epoch Training Loss:', 0.0039708224940113723)
('Epoch Training Acc:', 1.0)
('test loss', 1.2100871)
('test Acc:', 0.7887376)
model_080 F1_score: 82.86% >>>
735
('Epoch Training Loss:', 0.0036860702221019892)
('Epoch Training Acc:', 1.0)
('test loss', 1.2354656)
('test Acc:', 0.78468901)
model_080 F1_score: 82.40% >>>
736
('Epoch Training Loss:', 0.0037553427409875439)
('Epoch Training Acc:', 1.0)
('test loss', 1.2360535)
('test Acc:', 0.7868973)
model_080 F1_score: 82.64% >>>
737
('Epoch Training Loss:', 0.0036569530830092845)
('Epoch Training Acc:', 1.0)
('test loss', 1.2311035)
('test Acc:', 0.7887376)
model_080 F1_score: 82.83% >>>
738
('Epoch Training Loss:', 0.0036391158018886927)
('Epoch Training Acc:', 1.0)
('test loss', 1.2289096)
('test Acc:', 0.78984171)
model_080 F1_score: 82.82% >>>
739
('Epoch Training Loss:', 0.0036961892183171585)
('Epoch Training Acc:', 1.0)
('test loss', 1.229237)
('test Acc:', 0.79094589)
model_080 F1_score: 82.77% >>>
740
('Epoch Training Loss:', 0.0035498342695063911)
('Epoch Training Acc:', 1.0)
('test loss', 1.2349411)
('test Acc:', 0.78763342)
model_080 F1_score: 82.50% >>>
741
('Epoch Training Loss:', 0.003649406726253801)
('Epoch Training Acc:', 1.0)
('test loss', 1.2098414)
('test Acc:', 0.79278618)
model_080 F1_score: 83.10% >>>
742
('Epoch Training Loss:', 0.0035647143013193272)
('Epoch Training Acc:', 1.0)
('test loss', 1.227445)
('test Acc:', 0.79020977)
model_080 F1_score: 82.89% >>>
743
('Epoch Training Loss:', 0.0036358536572151934)
('Epoch Training Acc:', 1.0)
('test loss', 1.2190489)
('test Acc:', 0.78763342)
model_080 F1_score: 82.59% >>>
744
('Epoch Training Loss:', 0.0036693970669148257)
('Epoch Training Acc:', 1.0)
('test loss', 1.2187207)
('test Acc:', 0.7938903)
model_080 F1_score: 83.16% >>>
745
('Epoch Training Loss:', 0.0034906295431937906)
('Epoch Training Acc:', 1.0)
('test loss', 1.2239593)
('test Acc:', 0.79057783)
model_080 F1_score: 82.82% >>>
746
('Epoch Training Loss:', 0.0035850771901095868)
('Epoch Training Acc:', 1.0)
('test loss', 1.217863)
('test Acc:', 0.7887376)
model_080 F1_score: 82.73% >>>
747
('Epoch Training Loss:', 0.003595300052438688)
('Epoch Training Acc:', 1.0)
('test loss', 1.2230217)
('test Acc:', 0.78910565)
model_080 F1_score: 82.67% >>>
748
('Epoch Training Loss:', 0.0035476347929943586)
('Epoch Training Acc:', 1.0)
('test loss', 1.2303888)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
749
('Epoch Training Loss:', 0.0037213041359791532)
('Epoch Training Acc:', 1.0)
('test loss', 1.2213655)
('test Acc:', 0.79020977)
model_080 F1_score: 82.86% >>>
750
('Epoch Training Loss:', 0.0036984196285629878)
('Epoch Training Acc:', 1.0)
('test loss', 1.2275496)
('test Acc:', 0.791682)
model_080 F1_score: 83.05% >>>
751
('Epoch Training Loss:', 0.0035508394421412959)
('Epoch Training Acc:', 1.0)
('test loss', 1.2204239)
('test Acc:', 0.79241812)
model_080 F1_score: 83.11% >>>
752
('Epoch Training Loss:', 0.0035792240132650477)
('Epoch Training Acc:', 1.0)
('test loss', 1.2256033)
('test Acc:', 0.79278618)
model_080 F1_score: 83.17% >>>
753
('Epoch Training Loss:', 0.0034990642070624745)
('Epoch Training Acc:', 1.0)
('test loss', 1.2282202)
('test Acc:', 0.79241812)
model_080 F1_score: 83.29% >>>
754
('Epoch Training Loss:', 0.0036493631259872927)
('Epoch Training Acc:', 1.0)
('test loss', 1.2171837)
('test Acc:', 0.79094589)
model_080 F1_score: 82.85% >>>
755
('Epoch Training Loss:', 0.0037213048644844093)
('Epoch Training Acc:', 1.0)
('test loss', 1.2260122)
('test Acc:', 0.78984171)
model_080 F1_score: 83.04% >>>
756
('Epoch Training Loss:', 0.0036411537785170367)
('Epoch Training Acc:', 1.0)
('test loss', 1.2165806)
('test Acc:', 0.78726536)
model_080 F1_score: 82.70% >>>
757
('Epoch Training Loss:', 0.0036174821416352643)
('Epoch Training Acc:', 1.0)
('test loss', 1.2262615)
('test Acc:', 0.79094589)
model_080 F1_score: 82.92% >>>
758
('Epoch Training Loss:', 0.0036508752473309869)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241844)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
759
('Epoch Training Loss:', 0.0035428852133918554)
('Epoch Training Acc:', 1.0)
('test loss', 1.2360016)
('test Acc:', 0.79131395)
model_080 F1_score: 82.84% >>>
760
('Epoch Training Loss:', 0.0036950882686141995)
('Epoch Training Acc:', 1.0)
('test loss', 1.2108417)
('test Acc:', 0.79352224)
model_080 F1_score: 83.08% >>>
761
('Epoch Training Loss:', 0.0035745755794778233)
('Epoch Training Acc:', 1.0)
('test loss', 1.2320346)
('test Acc:', 0.79020977)
model_080 F1_score: 83.07% >>>
762
('Epoch Training Loss:', 0.0036011333231726894)
('Epoch Training Acc:', 1.0)
('test loss', 1.2390409)
('test Acc:', 0.78947371)
model_080 F1_score: 82.89% >>>
763
('Epoch Training Loss:', 0.0035085098252238822)
('Epoch Training Acc:', 1.0)
('test loss', 1.2177655)
('test Acc:', 0.79205006)
model_080 F1_score: 83.15% >>>
764
('Epoch Training Loss:', 0.0034968998024851317)
('Epoch Training Acc:', 1.0)
('test loss', 1.2107766)
('test Acc:', 0.79462641)
model_080 F1_score: 83.23% >>>
765
('Epoch Training Loss:', 0.0035338337524990493)
('Epoch Training Acc:', 1.0)
('test loss', 1.2372545)
('test Acc:', 0.78984171)
model_080 F1_score: 82.66% >>>
766
('Epoch Training Loss:', 0.0036160417475912254)
('Epoch Training Acc:', 1.0)
('test loss', 1.2265399)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
767
('Epoch Training Loss:', 0.0034674945536607993)
('Epoch Training Acc:', 1.0)
('test loss', 1.2359825)
('test Acc:', 0.7887376)
model_080 F1_score: 82.79% >>>
768
('Epoch Training Loss:', 0.0036586061523848912)
('Epoch Training Acc:', 1.0)
('test loss', 1.2304845)
('test Acc:', 0.79020977)
model_080 F1_score: 82.91% >>>
769
('Epoch Training Loss:', 0.0035678330846167228)
('Epoch Training Acc:', 1.0)
('test loss', 1.253579)
('test Acc:', 0.78800148)
model_080 F1_score: 82.67% >>>
770
('Epoch Training Loss:', 0.0035272095274194726)
('Epoch Training Acc:', 1.0)
('test loss', 1.2130777)
('test Acc:', 0.79094589)
model_080 F1_score: 83.10% >>>
771
('Epoch Training Loss:', 0.0035816104755213019)
('Epoch Training Acc:', 1.0)
('test loss', 1.2247607)
('test Acc:', 0.78984171)
model_080 F1_score: 82.70% >>>
772
('Epoch Training Loss:', 0.0032032491135396413)
('Epoch Training Acc:', 1.0)
('test loss', 1.2148819)
('test Acc:', 0.79609865)
model_080 F1_score: 83.26% >>>
773
('Epoch Training Loss:', 0.0034488963374315063)
('Epoch Training Acc:', 1.0)
('test loss', 1.2232976)
('test Acc:', 0.78984171)
model_080 F1_score: 82.95% >>>
774
('Epoch Training Loss:', 0.0034201820089947432)
('Epoch Training Acc:', 1.0)
('test loss', 1.2310152)
('test Acc:', 0.78947371)
model_080 F1_score: 83.05% >>>
775
('Epoch Training Loss:', 0.0035759741340370965)
('Epoch Training Acc:', 1.0)
('test loss', 1.2322612)
('test Acc:', 0.78800148)
model_080 F1_score: 82.54% >>>
776
('Epoch Training Loss:', 0.0036736657775691128)
('Epoch Training Acc:', 1.0)
('test loss', 1.2243471)
('test Acc:', 0.78947371)
model_080 F1_score: 82.76% >>>
777
('Epoch Training Loss:', 0.0035624018428279669)
('Epoch Training Acc:', 1.0)
('test loss', 1.230008)
('test Acc:', 0.78505707)
model_080 F1_score: 82.31% >>>
778
('Epoch Training Loss:', 0.003426032301831583)
('Epoch Training Acc:', 1.0)
('test loss', 1.2010224)
('test Acc:', 0.79315424)
model_080 F1_score: 83.21% >>>
779
('Epoch Training Loss:', 0.0036432475499168504)
('Epoch Training Acc:', 1.0)
('test loss', 1.2179345)
('test Acc:', 0.78836954)
model_080 F1_score: 82.94% >>>
780
('Epoch Training Loss:', 0.0033240394423046382)
('Epoch Training Acc:', 1.0)
('test loss', 1.2132256)
('test Acc:', 0.79094589)
model_080 F1_score: 82.95% >>>
781
('Epoch Training Loss:', 0.0037077700890222332)
('Epoch Training Acc:', 1.0)
('test loss', 1.2228744)
('test Acc:', 0.79057783)
model_080 F1_score: 83.19% >>>
782
('Epoch Training Loss:', 0.0035348570063433726)
('Epoch Training Acc:', 1.0)
('test loss', 1.2224169)
('test Acc:', 0.79020977)
model_080 F1_score: 82.93% >>>
783
('Epoch Training Loss:', 0.0035513216889739851)
('Epoch Training Acc:', 1.0)
('test loss', 1.2472689)
('test Acc:', 0.78984171)
model_080 F1_score: 83.01% >>>
784
('Epoch Training Loss:', 0.0034462667863408569)
('Epoch Training Acc:', 1.0)
('test loss', 1.2393802)
('test Acc:', 0.79094589)
model_080 F1_score: 82.92% >>>
785
('Epoch Training Loss:', 0.0034806165026566305)
('Epoch Training Acc:', 1.0)
('test loss', 1.226698)
('test Acc:', 0.791682)
model_080 F1_score: 82.87% >>>
786
('Epoch Training Loss:', 0.003410370010897168)
('Epoch Training Acc:', 1.0)
('test loss', 1.2370954)
('test Acc:', 0.78652924)
model_080 F1_score: 82.48% >>>
787
('Epoch Training Loss:', 0.0034385148474029847)
('Epoch Training Acc:', 1.0)
('test loss', 1.2417122)
('test Acc:', 0.7868973)
model_080 F1_score: 82.72% >>>
788
('Epoch Training Loss:', 0.0032478266366524622)
('Epoch Training Acc:', 1.0)
('test loss', 1.2124984)
('test Acc:', 0.791682)
model_080 F1_score: 82.86% >>>
789
('Epoch Training Loss:', 0.0035674892551469384)
('Epoch Training Acc:', 1.0)
('test loss', 1.2348251)
('test Acc:', 0.78910565)
model_080 F1_score: 82.90% >>>
790
('Epoch Training Loss:', 0.0034602110790729057)
('Epoch Training Acc:', 1.0)
('test loss', 1.2309245)
('test Acc:', 0.78910565)
model_080 F1_score: 82.82% >>>
791
('Epoch Training Loss:', 0.0033944458000405575)
('Epoch Training Acc:', 1.0)
('test loss', 1.2413921)
('test Acc:', 0.78836954)
model_080 F1_score: 82.65% >>>
792
('Epoch Training Loss:', 0.0033390104540558241)
('Epoch Training Acc:', 1.0)
('test loss', 1.2302182)
('test Acc:', 0.79057783)
model_080 F1_score: 83.02% >>>
793
('Epoch Training Loss:', 0.003331184174385271)
('Epoch Training Acc:', 1.0)
('test loss', 1.237694)
('test Acc:', 0.7887376)
model_080 F1_score: 82.60% >>>
794
('Epoch Training Loss:', 0.0034976824335899437)
('Epoch Training Acc:', 1.0)
('test loss', 1.2267424)
('test Acc:', 0.7938903)
model_080 F1_score: 83.21% >>>
795
('Epoch Training Loss:', 0.003246461707931303)
('Epoch Training Acc:', 1.0)
('test loss', 1.2179443)
('test Acc:', 0.791682)
model_080 F1_score: 82.89% >>>
796
('Epoch Training Loss:', 0.0033873592324198398)
('Epoch Training Acc:', 1.0)
('test loss', 1.2305701)
('test Acc:', 0.791682)
model_080 F1_score: 82.99% >>>
797
('Epoch Training Loss:', 0.0034638466768228682)
('Epoch Training Acc:', 1.0)
('test loss', 1.2175753)
('test Acc:', 0.7938903)
model_080 F1_score: 83.14% >>>
798
('Epoch Training Loss:', 0.003348987167555606)
('Epoch Training Acc:', 1.0)
('test loss', 1.2356367)
('test Acc:', 0.79241812)
model_080 F1_score: 83.02% >>>
799
('Epoch Training Loss:', 0.0035065730908172554)
('Epoch Training Acc:', 1.0)
('test loss', 1.2408282)
('test Acc:', 0.79315424)
model_080 F1_score: 83.13% >>>
800
('Epoch Training Loss:', 0.0032979302150124568)
('Epoch Training Acc:', 1.0)
('test loss', 1.2409693)
('test Acc:', 0.79057783)
model_080 F1_score: 82.91% >>>
801
('Epoch Training Loss:', 0.0032500606521352893)
('Epoch Training Acc:', 1.0)
('test loss', 1.2410716)
('test Acc:', 0.79094589)
model_080 F1_score: 83.13% >>>
802
('Epoch Training Loss:', 0.0033187832832481945)
('Epoch Training Acc:', 1.0)
('test loss', 1.2306726)
('test Acc:', 0.78836954)
model_080 F1_score: 82.92% >>>
803
('Epoch Training Loss:', 0.0034247017474626773)
('Epoch Training Acc:', 1.0)
('test loss', 1.2263955)
('test Acc:', 0.7938903)
model_080 F1_score: 83.14% >>>
804
('Epoch Training Loss:', 0.0033307617914033472)
('Epoch Training Acc:', 1.0)
('test loss', 1.2327585)
('test Acc:', 0.79278618)
model_080 F1_score: 83.20% >>>
805
('Epoch Training Loss:', 0.0033340196273456968)
('Epoch Training Acc:', 1.0)
('test loss', 1.2210708)
('test Acc:', 0.791682)
model_080 F1_score: 83.11% >>>
806
('Epoch Training Loss:', 0.0031990178922569612)
('Epoch Training Acc:', 1.0)
('test loss', 1.2087829)
('test Acc:', 0.79205006)
model_080 F1_score: 82.98% >>>
807
('Epoch Training Loss:', 0.0032759655850895797)
('Epoch Training Acc:', 1.0)
('test loss', 1.2396517)
('test Acc:', 0.78947371)
model_080 F1_score: 82.90% >>>
808
('Epoch Training Loss:', 0.003317595956104924)
('Epoch Training Acc:', 1.0)
('test loss', 1.2311386)
('test Acc:', 0.79094589)
model_080 F1_score: 82.90% >>>
809
('Epoch Training Loss:', 0.0032511451508980826)
('Epoch Training Acc:', 1.0)
('test loss', 1.2232355)
('test Acc:', 0.79020977)
model_080 F1_score: 82.78% >>>
810
('Epoch Training Loss:', 0.0032604521557004773)
('Epoch Training Acc:', 1.0)
('test loss', 1.2160118)
('test Acc:', 0.7938903)
model_080 F1_score: 83.16% >>>
811
('Epoch Training Loss:', 0.0032890769193727465)
('Epoch Training Acc:', 1.0)
('test loss', 1.2407449)
('test Acc:', 0.79020977)
model_080 F1_score: 82.89% >>>
812
('Epoch Training Loss:', 0.0033424095472582849)
('Epoch Training Acc:', 1.0)
('test loss', 1.2307193)
('test Acc:', 0.78836954)
model_080 F1_score: 82.88% >>>
813
('Epoch Training Loss:', 0.0035562383650358242)
('Epoch Training Acc:', 1.0)
('test loss', 1.2292699)
('test Acc:', 0.79315424)
model_080 F1_score: 83.05% >>>
814
('Epoch Training Loss:', 0.003278467379459471)
('Epoch Training Acc:', 1.0)
('test loss', 1.2209163)
('test Acc:', 0.79241812)
model_080 F1_score: 83.12% >>>
815
('Epoch Training Loss:', 0.0033676736802590312)
('Epoch Training Acc:', 1.0)
('test loss', 1.2463725)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
816
('Epoch Training Loss:', 0.0033217327054444468)
('Epoch Training Acc:', 1.0)
('test loss', 1.2249774)
('test Acc:', 0.791682)
model_080 F1_score: 82.99% >>>
817
('Epoch Training Loss:', 0.0033021477211150341)
('Epoch Training Acc:', 1.0)
('test loss', 1.2400554)
('test Acc:', 0.79020977)
model_080 F1_score: 83.02% >>>
818
('Epoch Training Loss:', 0.0033158907158394868)
('Epoch Training Acc:', 1.0)
('test loss', 1.2323155)
('test Acc:', 0.78542513)
model_080 F1_score: 82.66% >>>
819
('Epoch Training Loss:', 0.0032876030145416735)
('Epoch Training Acc:', 1.0)
('test loss', 1.2342598)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
820
('Epoch Training Loss:', 0.0032589711099717533)
('Epoch Training Acc:', 1.0)
('test loss', 1.2339771)
('test Acc:', 0.79205006)
model_080 F1_score: 83.02% >>>
821
('Epoch Training Loss:', 0.0033823684143499122)
('Epoch Training Acc:', 1.0)
('test loss', 1.240353)
('test Acc:', 0.78984171)
model_080 F1_score: 82.86% >>>
822
('Epoch Training Loss:', 0.0032043069954852399)
('Epoch Training Acc:', 1.0)
('test loss', 1.238025)
('test Acc:', 0.79278618)
model_080 F1_score: 83.25% >>>
823
('Epoch Training Loss:', 0.0031667242046751198)
('Epoch Training Acc:', 1.0)
('test loss', 1.2257107)
('test Acc:', 0.79609865)
model_080 F1_score: 83.23% >>>
824
('Epoch Training Loss:', 0.0032883919620871893)
('Epoch Training Acc:', 1.0)
('test loss', 1.2444668)
('test Acc:', 0.79020977)
model_080 F1_score: 82.97% >>>
825
('Epoch Training Loss:', 0.0032025975715441746)
('Epoch Training Acc:', 1.0)
('test loss', 1.2155756)
('test Acc:', 0.79057783)
model_080 F1_score: 83.08% >>>
826
('Epoch Training Loss:', 0.0033608106559768203)
('Epoch Training Acc:', 1.0)
('test loss', 1.2320291)
('test Acc:', 0.78726536)
model_080 F1_score: 82.76% >>>
827
('Epoch Training Loss:', 0.0033385142492079467)
('Epoch Training Acc:', 1.0)
('test loss', 1.2352095)
('test Acc:', 0.79094589)
model_080 F1_score: 82.84% >>>
828
('Epoch Training Loss:', 0.0032227372894340078)
('Epoch Training Acc:', 1.0)
('test loss', 1.242398)
('test Acc:', 0.78947371)
model_080 F1_score: 82.77% >>>
829
('Epoch Training Loss:', 0.0032620255205983995)
('Epoch Training Acc:', 1.0)
('test loss', 1.2109675)
('test Acc:', 0.79131395)
model_080 F1_score: 82.97% >>>
830
('Epoch Training Loss:', 0.0033578629063413246)
('Epoch Training Acc:', 1.0)
('test loss', 1.2478471)
('test Acc:', 0.79020977)
model_080 F1_score: 82.93% >>>
831
('Epoch Training Loss:', 0.0033102403376688017)
('Epoch Training Acc:', 1.0)
('test loss', 1.2277867)
('test Acc:', 0.7938903)
model_080 F1_score: 83.20% >>>
832
('Epoch Training Loss:', 0.0034724084371191566)
('Epoch Training Acc:', 1.0)
('test loss', 1.2255406)
('test Acc:', 0.79205006)
model_080 F1_score: 83.24% >>>
833
('Epoch Training Loss:', 0.0032214460889008478)
('Epoch Training Acc:', 1.0)
('test loss', 1.2291851)
('test Acc:', 0.78984171)
model_080 F1_score: 82.93% >>>
834
('Epoch Training Loss:', 0.0031300290074796067)
('Epoch Training Acc:', 1.0)
('test loss', 1.2488235)
('test Acc:', 0.79020977)
model_080 F1_score: 82.69% >>>
835
('Epoch Training Loss:', 0.0032036522970884107)
('Epoch Training Acc:', 1.0)
('test loss', 1.2283527)
('test Acc:', 0.78984171)
model_080 F1_score: 82.73% >>>
836
('Epoch Training Loss:', 0.0031344327126134885)
('Epoch Training Acc:', 1.0)
('test loss', 1.2247263)
('test Acc:', 0.79315424)
model_080 F1_score: 83.16% >>>
837
('Epoch Training Loss:', 0.0031655628918088041)
('Epoch Training Acc:', 1.0)
('test loss', 1.2235644)
('test Acc:', 0.79499447)
model_080 F1_score: 83.24% >>>
838
('Epoch Training Loss:', 0.0032126285677804844)
('Epoch Training Acc:', 1.0)
('test loss', 1.2491978)
('test Acc:', 0.78432095)
model_080 F1_score: 82.75% >>>
839
('Epoch Training Loss:', 0.0031594908741681138)
('Epoch Training Acc:', 1.0)
('test loss', 1.2421074)
('test Acc:', 0.78947371)
model_080 F1_score: 82.72% >>>
840
('Epoch Training Loss:', 0.0031467596613765636)
('Epoch Training Acc:', 1.0)
('test loss', 1.2357842)
('test Acc:', 0.78947371)
model_080 F1_score: 82.98% >>>
841
('Epoch Training Loss:', 0.003226621049179812)
('Epoch Training Acc:', 1.0)
('test loss', 1.2298274)
('test Acc:', 0.79205006)
model_080 F1_score: 83.34% >>>
842
('Epoch Training Loss:', 0.0031510423450527014)
('Epoch Training Acc:', 1.0)
('test loss', 1.2498916)
('test Acc:', 0.78284872)
model_080 F1_score: 82.17% >>>
843
('Epoch Training Loss:', 0.0031088999166968279)
('Epoch Training Acc:', 1.0)
('test loss', 1.2363104)
('test Acc:', 0.79131395)
model_080 F1_score: 82.98% >>>
844
('Epoch Training Loss:', 0.0031607836631337705)
('Epoch Training Acc:', 1.0)
('test loss', 1.2437856)
('test Acc:', 0.79020977)
model_080 F1_score: 82.80% >>>
845
('Epoch Training Loss:', 0.0030434382179009845)
('Epoch Training Acc:', 1.0)
('test loss', 1.2337832)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
846
('Epoch Training Loss:', 0.0031192862252282794)
('Epoch Training Acc:', 1.0)
('test loss', 1.2425523)
('test Acc:', 0.79020977)
model_080 F1_score: 82.77% >>>
847
('Epoch Training Loss:', 0.0032537409610995383)
('Epoch Training Acc:', 1.0)
('test loss', 1.2461168)
('test Acc:', 0.79315424)
model_080 F1_score: 82.93% >>>
848
('Epoch Training Loss:', 0.0032584142009000061)
('Epoch Training Acc:', 1.0)
('test loss', 1.2212638)
('test Acc:', 0.79205006)
model_080 F1_score: 82.93% >>>
849
('Epoch Training Loss:', 0.0030959062705733231)
('Epoch Training Acc:', 1.0)
('test loss', 1.2202951)
('test Acc:', 0.78800148)
model_080 F1_score: 82.70% >>>
850
('Epoch Training Loss:', 0.0031768299636496522)
('Epoch Training Acc:', 1.0)
('test loss', 1.2402848)
('test Acc:', 0.79020977)
model_080 F1_score: 82.58% >>>
851
('Epoch Training Loss:', 0.0031644707751183887)
('Epoch Training Acc:', 1.0)
('test loss', 1.2424241)
('test Acc:', 0.79278618)
model_080 F1_score: 82.92% >>>
852
('Epoch Training Loss:', 0.0031100299402169185)
('Epoch Training Acc:', 1.0)
('test loss', 1.2377454)
('test Acc:', 0.79094589)
model_080 F1_score: 83.02% >>>
853
('Epoch Training Loss:', 0.0033423320733163564)
('Epoch Training Acc:', 1.0)
('test loss', 1.2347796)
('test Acc:', 0.79205006)
model_080 F1_score: 83.17% >>>
854
('Epoch Training Loss:', 0.0029588732777483528)
('Epoch Training Acc:', 1.0)
('test loss', 1.2276965)
('test Acc:', 0.78836954)
model_080 F1_score: 82.76% >>>
855
('Epoch Training Loss:', 0.0029749637174063537)
('Epoch Training Acc:', 1.0)
('test loss', 1.2589295)
('test Acc:', 0.78542513)
model_080 F1_score: 82.30% >>>
856
('Epoch Training Loss:', 0.0033147902258860995)
('Epoch Training Acc:', 1.0)
('test loss', 1.2378876)
('test Acc:', 0.78726536)
model_080 F1_score: 82.61% >>>
857
('Epoch Training Loss:', 0.0030759768378629815)
('Epoch Training Acc:', 1.0)
('test loss', 1.2324531)
('test Acc:', 0.79352224)
model_080 F1_score: 83.03% >>>
858
('Epoch Training Loss:', 0.0032224729438894428)
('Epoch Training Acc:', 1.0)
('test loss', 1.2441332)
('test Acc:', 0.7887376)
model_080 F1_score: 82.96% >>>
859
('Epoch Training Loss:', 0.0030595833127335936)
('Epoch Training Acc:', 1.0)
('test loss', 1.2725228)
('test Acc:', 0.7887376)
model_080 F1_score: 82.77% >>>
860
('Epoch Training Loss:', 0.0030366233831955469)
('Epoch Training Acc:', 1.0)
('test loss', 1.2372136)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
861
('Epoch Training Loss:', 0.0030429097787418868)
('Epoch Training Acc:', 1.0)
('test loss', 1.2442517)
('test Acc:', 0.78947371)
model_080 F1_score: 82.85% >>>
862
('Epoch Training Loss:', 0.0032000144883568282)
('Epoch Training Acc:', 1.0)
('test loss', 1.2541959)
('test Acc:', 0.7887376)
model_080 F1_score: 82.91% >>>
863
('Epoch Training Loss:', 0.0032795137467473978)
('Epoch Training Acc:', 1.0)
('test loss', 1.2451656)
('test Acc:', 0.791682)
model_080 F1_score: 83.08% >>>
864
('Epoch Training Loss:', 0.0029997547321727325)
('Epoch Training Acc:', 1.0)
('test loss', 1.2439342)
('test Acc:', 0.78616118)
model_080 F1_score: 82.41% >>>
865
('Epoch Training Loss:', 0.0030858256250212435)
('Epoch Training Acc:', 1.0)
('test loss', 1.2245936)
('test Acc:', 0.7938903)
model_080 F1_score: 83.20% >>>
866
('Epoch Training Loss:', 0.0029808269009663491)
('Epoch Training Acc:', 1.0)
('test loss', 1.2499037)
('test Acc:', 0.78836954)
model_080 F1_score: 82.64% >>>
867
('Epoch Training Loss:', 0.0031302122706620139)
('Epoch Training Acc:', 1.0)
('test loss', 1.2369454)
('test Acc:', 0.79241812)
model_080 F1_score: 83.08% >>>
868
('Epoch Training Loss:', 0.002941520627700811)
('Epoch Training Acc:', 1.0)
('test loss', 1.2340025)
('test Acc:', 0.79241812)
model_080 F1_score: 83.22% >>>
869
('Epoch Training Loss:', 0.0031669900972701726)
('Epoch Training Acc:', 1.0)
('test loss', 1.2469612)
('test Acc:', 0.79094589)
model_080 F1_score: 82.92% >>>
870
('Epoch Training Loss:', 0.0034381916825623193)
('Epoch Training Acc:', 1.0)
('test loss', 1.2517434)
('test Acc:', 0.78984171)
model_080 F1_score: 82.89% >>>
871
('Epoch Training Loss:', 0.0031741763627906039)
('Epoch Training Acc:', 1.0)
('test loss', 1.23368)
('test Acc:', 0.79094589)
model_080 F1_score: 82.85% >>>
872
('Epoch Training Loss:', 0.0029631126590174972)
('Epoch Training Acc:', 1.0)
('test loss', 1.2431421)
('test Acc:', 0.78542513)
model_080 F1_score: 82.51% >>>
873
('Epoch Training Loss:', 0.0030888031392350968)
('Epoch Training Acc:', 1.0)
('test loss', 1.2372946)
('test Acc:', 0.79241812)
model_080 F1_score: 82.96% >>>
874
('Epoch Training Loss:', 0.0029966907072775939)
('Epoch Training Acc:', 1.0)
('test loss', 1.2322125)
('test Acc:', 0.79462641)
model_080 F1_score: 83.19% >>>
875
('Epoch Training Loss:', 0.0031465098254557233)
('Epoch Training Acc:', 1.0)
('test loss', 1.2336905)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
876
('Epoch Training Loss:', 0.0031198814485833282)
('Epoch Training Acc:', 1.0)
('test loss', 1.2673787)
('test Acc:', 0.79352224)
model_080 F1_score: 83.29% >>>
877
('Epoch Training Loss:', 0.0031765724461365608)
('Epoch Training Acc:', 1.0)
('test loss', 1.2234683)
('test Acc:', 0.78947371)
model_080 F1_score: 82.81% >>>
878
('Epoch Training Loss:', 0.0029168255509830487)
('Epoch Training Acc:', 1.0)
('test loss', 1.2293905)
('test Acc:', 0.79425836)
model_080 F1_score: 83.29% >>>
879
('Epoch Training Loss:', 0.0030060763674555346)
('Epoch Training Acc:', 1.0)
('test loss', 1.2260829)
('test Acc:', 0.79315424)
model_080 F1_score: 83.01% >>>
880
('Epoch Training Loss:', 0.0028419479667718406)
('Epoch Training Acc:', 1.0)
('test loss', 1.2369604)
('test Acc:', 0.79241812)
model_080 F1_score: 82.81% >>>
881
('Epoch Training Loss:', 0.0030655774635306443)
('Epoch Training Acc:', 1.0)
('test loss', 1.2539349)
('test Acc:', 0.79094589)
model_080 F1_score: 82.91% >>>
882
('Epoch Training Loss:', 0.0029929495735814271)
('Epoch Training Acc:', 1.0)
('test loss', 1.2511815)
('test Acc:', 0.78984171)
model_080 F1_score: 82.78% >>>
883
('Epoch Training Loss:', 0.002903765374867362)
('Epoch Training Acc:', 1.0)
('test loss', 1.2397001)
('test Acc:', 0.7887376)
model_080 F1_score: 82.83% >>>
884
('Epoch Training Loss:', 0.0031317517486968427)
('Epoch Training Acc:', 1.0)
('test loss', 1.2213143)
('test Acc:', 0.79057783)
model_080 F1_score: 82.96% >>>
885
('Epoch Training Loss:', 0.0029961999584884325)
('Epoch Training Acc:', 1.0)
('test loss', 1.2276284)
('test Acc:', 0.79278618)
model_080 F1_score: 82.85% >>>
886
('Epoch Training Loss:', 0.003050451275157684)
('Epoch Training Acc:', 1.0)
('test loss', 1.2309606)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
887
('Epoch Training Loss:', 0.0029628290662913059)
('Epoch Training Acc:', 1.0)
('test loss', 1.2497753)
('test Acc:', 0.78910565)
model_080 F1_score: 83.03% >>>
888
('Epoch Training Loss:', 0.0030197477622095903)
('Epoch Training Acc:', 1.0)
('test loss', 1.2320193)
('test Acc:', 0.79094589)
model_080 F1_score: 83.14% >>>
889
('Epoch Training Loss:', 0.0030307648244161101)
('Epoch Training Acc:', 1.0)
('test loss', 1.2284617)
('test Acc:', 0.7887376)
model_080 F1_score: 82.78% >>>
890
('Epoch Training Loss:', 0.0029557235620814026)
('Epoch Training Acc:', 1.0)
('test loss', 1.2280118)
('test Acc:', 0.78763342)
model_080 F1_score: 82.71% >>>
891
('Epoch Training Loss:', 0.0031229970218191738)
('Epoch Training Acc:', 1.0)
('test loss', 1.2368981)
('test Acc:', 0.78947371)
model_080 F1_score: 82.74% >>>
892
('Epoch Training Loss:', 0.0029963327697259956)
('Epoch Training Acc:', 1.0)
('test loss', 1.2248873)
('test Acc:', 0.79904306)
model_080 F1_score: 83.52% >>>
893
('Epoch Training Loss:', 0.0028491432340160827)
('Epoch Training Acc:', 1.0)
('test loss', 1.2419883)
('test Acc:', 0.78800148)
model_080 F1_score: 82.76% >>>
894
('Epoch Training Loss:', 0.0030074187043283018)
('Epoch Training Acc:', 1.0)
('test loss', 1.2416302)
('test Acc:', 0.79241812)
model_080 F1_score: 82.96% >>>
895
('Epoch Training Loss:', 0.002920740272202238)
('Epoch Training Acc:', 1.0)
('test loss', 1.2519612)
('test Acc:', 0.78652924)
model_080 F1_score: 82.49% >>>
896
('Epoch Training Loss:', 0.0029614515592584212)
('Epoch Training Acc:', 1.0)
('test loss', 1.2279031)
('test Acc:', 0.79020977)
model_080 F1_score: 82.74% >>>
897
('Epoch Training Loss:', 0.0030559956362594676)
('Epoch Training Acc:', 1.0)
('test loss', 1.2499487)
('test Acc:', 0.79425836)
model_080 F1_score: 83.02% >>>
898
('Epoch Training Loss:', 0.0029552743435488082)
('Epoch Training Acc:', 1.0)
('test loss', 1.2462965)
('test Acc:', 0.79205006)
model_080 F1_score: 83.14% >>>
899
('Epoch Training Loss:', 0.0029505203965527471)
('Epoch Training Acc:', 1.0)
('test loss', 1.2377051)
('test Acc:', 0.78836954)
model_080 F1_score: 82.57% >>>
900
('Epoch Training Loss:', 0.0029396167901722947)
('Epoch Training Acc:', 1.0)
('test loss', 1.2520654)
('test Acc:', 0.78910565)
model_080 F1_score: 82.79% >>>
901
('Epoch Training Loss:', 0.0026719875486378442)
('Epoch Training Acc:', 1.0)
('test loss', 1.2370982)
('test Acc:', 0.79241812)
model_080 F1_score: 83.01% >>>
902
('Epoch Training Loss:', 0.0028550155793709564)
('Epoch Training Acc:', 1.0)
('test loss', 1.2507952)
('test Acc:', 0.78800148)
model_080 F1_score: 82.59% >>>
903
('Epoch Training Loss:', 0.0029745322185590339)
('Epoch Training Acc:', 1.0)
('test loss', 1.2627507)
('test Acc:', 0.7868973)
model_080 F1_score: 82.72% >>>
904
('Epoch Training Loss:', 0.0029411993732537667)
('Epoch Training Acc:', 1.0)
('test loss', 1.2538861)
('test Acc:', 0.78836954)
model_080 F1_score: 82.74% >>>
905
('Epoch Training Loss:', 0.0029262553562148241)
('Epoch Training Acc:', 1.0)
('test loss', 1.232923)
('test Acc:', 0.79609865)
model_080 F1_score: 83.27% >>>
906
('Epoch Training Loss:', 0.0028833800861320924)
('Epoch Training Acc:', 1.0)
('test loss', 1.2448775)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
907
('Epoch Training Loss:', 0.0029570406049970188)
('Epoch Training Acc:', 1.0)
('test loss', 1.2630901)
('test Acc:', 0.78910565)
model_080 F1_score: 82.84% >>>
908
('Epoch Training Loss:', 0.0029392192495834024)
('Epoch Training Acc:', 1.0)
('test loss', 1.2271013)
('test Acc:', 0.78947371)
model_080 F1_score: 82.70% >>>
909
('Epoch Training Loss:', 0.0029440019816320273)
('Epoch Training Acc:', 1.0)
('test loss', 1.2350053)
('test Acc:', 0.79462641)
model_080 F1_score: 83.24% >>>
910
('Epoch Training Loss:', 0.0029711559182032943)
('Epoch Training Acc:', 1.0)
('test loss', 1.2507206)
('test Acc:', 0.78947371)
model_080 F1_score: 82.77% >>>
911
('Epoch Training Loss:', 0.0028578064907378575)
('Epoch Training Acc:', 1.0)
('test loss', 1.2393788)
('test Acc:', 0.78836954)
model_080 F1_score: 82.80% >>>
912
('Epoch Training Loss:', 0.0030122209295768698)
('Epoch Training Acc:', 1.0)
('test loss', 1.246133)
('test Acc:', 0.7938903)
model_080 F1_score: 83.17% >>>
913
('Epoch Training Loss:', 0.003098057482020522)
('Epoch Training Acc:', 1.0)
('test loss', 1.2327882)
('test Acc:', 0.7938903)
model_080 F1_score: 83.09% >>>
914
('Epoch Training Loss:', 0.0029281003153300844)
('Epoch Training Acc:', 1.0)
('test loss', 1.2383155)
('test Acc:', 0.79057783)
model_080 F1_score: 82.97% >>>
915
('Epoch Training Loss:', 0.0028484020908763341)
('Epoch Training Acc:', 1.0)
('test loss', 1.2414672)
('test Acc:', 0.78984171)
model_080 F1_score: 82.89% >>>
916
('Epoch Training Loss:', 0.0029176107273087837)
('Epoch Training Acc:', 1.0)
('test loss', 1.2446984)
('test Acc:', 0.7938903)
model_080 F1_score: 83.29% >>>
917
('Epoch Training Loss:', 0.0027313257151035941)
('Epoch Training Acc:', 1.0)
('test loss', 1.2415092)
('test Acc:', 0.79278618)
model_080 F1_score: 83.00% >>>
918
('Epoch Training Loss:', 0.0028340867975202855)
('Epoch Training Acc:', 1.0)
('test loss', 1.2473782)
('test Acc:', 0.79094589)
model_080 F1_score: 82.95% >>>
919
('Epoch Training Loss:', 0.0029807501250616042)
('Epoch Training Acc:', 1.0)
('test loss', 1.2414942)
('test Acc:', 0.79094589)
model_080 F1_score: 82.84% >>>
920
('Epoch Training Loss:', 0.0028211884891788941)
('Epoch Training Acc:', 1.0)
('test loss', 1.236001)
('test Acc:', 0.79241812)
model_080 F1_score: 83.13% >>>
921
('Epoch Training Loss:', 0.0029627300273205037)
('Epoch Training Acc:', 1.0)
('test loss', 1.2480613)
('test Acc:', 0.78910565)
model_080 F1_score: 82.83% >>>
922
('Epoch Training Loss:', 0.0028670647566286789)
('Epoch Training Acc:', 1.0)
('test loss', 1.2527076)
('test Acc:', 0.79094589)
model_080 F1_score: 82.99% >>>
923
('Epoch Training Loss:', 0.0029638417481692159)
('Epoch Training Acc:', 1.0)
('test loss', 1.2520329)
('test Acc:', 0.7938903)
model_080 F1_score: 83.04% >>>
924
('Epoch Training Loss:', 0.0030679410851917055)
('Epoch Training Acc:', 1.0)
('test loss', 1.2562836)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
925
('Epoch Training Loss:', 0.0030089424949437671)
('Epoch Training Acc:', 1.0)
('test loss', 1.2246475)
('test Acc:', 0.7868973)
model_080 F1_score: 82.31% >>>
926
('Epoch Training Loss:', 0.0028753364344993315)
('Epoch Training Acc:', 1.0)
('test loss', 1.2553848)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
927
('Epoch Training Loss:', 0.0029467057220244897)
('Epoch Training Acc:', 1.0)
('test loss', 1.2381014)
('test Acc:', 0.79683477)
model_080 F1_score: 83.13% >>>
928
('Epoch Training Loss:', 0.0028023676281918597)
('Epoch Training Acc:', 1.0)
('test loss', 1.2683136)
('test Acc:', 0.79094589)
model_080 F1_score: 82.87% >>>
929
('Epoch Training Loss:', 0.0028893779785903462)
('Epoch Training Acc:', 1.0)
('test loss', 1.2701112)
('test Acc:', 0.79020977)
model_080 F1_score: 82.95% >>>
930
('Epoch Training Loss:', 0.002801089226068143)
('Epoch Training Acc:', 1.0)
('test loss', 1.2470099)
('test Acc:', 0.78726536)
model_080 F1_score: 82.49% >>>
931
('Epoch Training Loss:', 0.0029238347801765485)
('Epoch Training Acc:', 1.0)
('test loss', 1.2576075)
('test Acc:', 0.79425836)
model_080 F1_score: 83.34% >>>
932
('Epoch Training Loss:', 0.0028678733106062282)
('Epoch Training Acc:', 1.0)
('test loss', 1.2477401)
('test Acc:', 0.78763342)
model_080 F1_score: 82.64% >>>
933
('Epoch Training Loss:', 0.0028843786035395169)
('Epoch Training Acc:', 1.0)
('test loss', 1.2340074)
('test Acc:', 0.79425836)
model_080 F1_score: 83.02% >>>
934
('Epoch Training Loss:', 0.0028804293451685226)
('Epoch Training Acc:', 1.0)
('test loss', 1.2419431)
('test Acc:', 0.78984171)
model_080 F1_score: 82.81% >>>
935
('Epoch Training Loss:', 0.0027950859443990339)
('Epoch Training Acc:', 1.0)
('test loss', 1.240443)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
936
('Epoch Training Loss:', 0.002818594160999055)
('Epoch Training Acc:', 1.0)
('test loss', 1.2562888)
('test Acc:', 0.79205006)
model_080 F1_score: 83.09% >>>
937
('Epoch Training Loss:', 0.0027363024528312963)
('Epoch Training Acc:', 1.0)
('test loss', 1.2536999)
('test Acc:', 0.79352224)
model_080 F1_score: 83.21% >>>
938
('Epoch Training Loss:', 0.0027721891128749121)
('Epoch Training Acc:', 1.0)
('test loss', 1.2450342)
('test Acc:', 0.79057783)
model_080 F1_score: 82.95% >>>
939
('Epoch Training Loss:', 0.0028981708605897438)
('Epoch Training Acc:', 1.0)
('test loss', 1.2426559)
('test Acc:', 0.79425836)
model_080 F1_score: 83.27% >>>
940
('Epoch Training Loss:', 0.0028593560491572134)
('Epoch Training Acc:', 1.0)
('test loss', 1.2218958)
('test Acc:', 0.78910565)
model_080 F1_score: 82.68% >>>
941
('Epoch Training Loss:', 0.0027627576250779384)
('Epoch Training Acc:', 1.0)
('test loss', 1.2476553)
('test Acc:', 0.78321677)
model_080 F1_score: 82.35% >>>
942
('Epoch Training Loss:', 0.0027819646779789764)
('Epoch Training Acc:', 1.0)
('test loss', 1.2451054)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
943
('Epoch Training Loss:', 0.0026494102985452628)
('Epoch Training Acc:', 1.0)
('test loss', 1.2420198)
('test Acc:', 0.79315424)
model_080 F1_score: 83.12% >>>
944
('Epoch Training Loss:', 0.0028399416310094239)
('Epoch Training Acc:', 1.0)
('test loss', 1.2491332)
('test Acc:', 0.79646671)
model_080 F1_score: 83.34% >>>
945
('Epoch Training Loss:', 0.0028841007924711448)
('Epoch Training Acc:', 1.0)
('test loss', 1.2348387)
('test Acc:', 0.79241812)
model_080 F1_score: 83.18% >>>
946
('Epoch Training Loss:', 0.0027920449797420588)
('Epoch Training Acc:', 1.0)
('test loss', 1.2676529)
('test Acc:', 0.79020977)
model_080 F1_score: 82.98% >>>
947
('Epoch Training Loss:', 0.002757875801762566)
('Epoch Training Acc:', 1.0)
('test loss', 1.2420403)
('test Acc:', 0.79352224)
model_080 F1_score: 83.18% >>>
948
('Epoch Training Loss:', 0.0029450130964505661)
('Epoch Training Acc:', 1.0)
('test loss', 1.2276524)
('test Acc:', 0.79425836)
model_080 F1_score: 83.18% >>>
949
('Epoch Training Loss:', 0.0028518663489194296)
('Epoch Training Acc:', 1.0)
('test loss', 1.2406746)
('test Acc:', 0.79315424)
model_080 F1_score: 83.21% >>>
950
('Epoch Training Loss:', 0.002894806465064903)
('Epoch Training Acc:', 1.0)
('test loss', 1.2556167)
('test Acc:', 0.79205006)
model_080 F1_score: 83.21% >>>
951
('Epoch Training Loss:', 0.0027815115790872369)
('Epoch Training Acc:', 1.0)
('test loss', 1.2427913)
('test Acc:', 0.78726536)
model_080 F1_score: 82.78% >>>
952
('Epoch Training Loss:', 0.0027511612133821473)
('Epoch Training Acc:', 1.0)
('test loss', 1.2503377)
('test Acc:', 0.78984171)
model_080 F1_score: 83.03% >>>
953
('Epoch Training Loss:', 0.00274580382347267)
('Epoch Training Acc:', 1.0)
('test loss', 1.2284806)
('test Acc:', 0.79241812)
model_080 F1_score: 83.10% >>>
954
('Epoch Training Loss:', 0.0029905925221100915)
('Epoch Training Acc:', 1.0)
('test loss', 1.2212774)
('test Acc:', 0.79094589)
model_080 F1_score: 82.77% >>>
955
('Epoch Training Loss:', 0.0026618540600793494)
('Epoch Training Acc:', 1.0)
('test loss', 1.2373857)
('test Acc:', 0.79205006)
model_080 F1_score: 83.14% >>>
956
('Epoch Training Loss:', 0.0027545660677787964)
('Epoch Training Acc:', 1.0)
('test loss', 1.2634338)
('test Acc:', 0.79205006)
model_080 F1_score: 83.02% >>>
957
('Epoch Training Loss:', 0.0028380890985317819)
('Epoch Training Acc:', 1.0)
('test loss', 1.2349117)
('test Acc:', 0.79462641)
model_080 F1_score: 83.29% >>>
958
('Epoch Training Loss:', 0.0028981387658859603)
('Epoch Training Acc:', 1.0)
('test loss', 1.2378926)
('test Acc:', 0.79425836)
model_080 F1_score: 83.31% >>>
959
('Epoch Training Loss:', 0.0028745812851411756)
('Epoch Training Acc:', 1.0)
('test loss', 1.2425334)
('test Acc:', 0.78836954)
model_080 F1_score: 82.72% >>>
960
('Epoch Training Loss:', 0.0027326017698214855)
('Epoch Training Acc:', 1.0)
('test loss', 1.2389525)
('test Acc:', 0.79278618)
model_080 F1_score: 83.11% >>>
961
('Epoch Training Loss:', 0.0027297613651171559)
('Epoch Training Acc:', 1.0)
('test loss', 1.2409744)
('test Acc:', 0.78836954)
model_080 F1_score: 82.77% >>>
962
('Epoch Training Loss:', 0.002768717783055763)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241861)
('test Acc:', 0.78652924)
model_080 F1_score: 82.59% >>>
963
('Epoch Training Loss:', 0.0027969831367045117)
('Epoch Training Acc:', 1.0)
('test loss', 1.2347852)
('test Acc:', 0.79573059)
model_080 F1_score: 83.45% >>>
964
('Epoch Training Loss:', 0.0027717912621483265)
('Epoch Training Acc:', 1.0)
('test loss', 1.2503493)
('test Acc:', 0.78947371)
model_080 F1_score: 82.77% >>>
965
('Epoch Training Loss:', 0.0028167988757559215)
('Epoch Training Acc:', 1.0)
('test loss', 1.2632506)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
966
('Epoch Training Loss:', 0.0027167472785549762)
('Epoch Training Acc:', 1.0)
('test loss', 1.2401328)
('test Acc:', 0.78726536)
model_080 F1_score: 82.64% >>>
967
('Epoch Training Loss:', 0.0027630206213871134)
('Epoch Training Acc:', 1.0)
('test loss', 1.2510729)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
968
('Epoch Training Loss:', 0.0027160067779732344)
('Epoch Training Acc:', 1.0)
('test loss', 1.2356341)
('test Acc:', 0.78248066)
model_080 F1_score: 82.24% >>>
969
('Epoch Training Loss:', 0.0026464606526133139)
('Epoch Training Acc:', 1.0)
('test loss', 1.2222139)
('test Acc:', 0.79057783)
model_080 F1_score: 82.84% >>>
970
('Epoch Training Loss:', 0.0027654127015921404)
('Epoch Training Acc:', 1.0)
('test loss', 1.2614526)
('test Acc:', 0.7887376)
model_080 F1_score: 82.83% >>>
971
('Epoch Training Loss:', 0.0026902792365035566)
('Epoch Training Acc:', 1.0)
('test loss', 1.2438446)
('test Acc:', 0.7868973)
model_080 F1_score: 82.57% >>>
972
('Epoch Training Loss:', 0.0027432544784460333)
('Epoch Training Acc:', 1.0)
('test loss', 1.2570786)
('test Acc:', 0.79205006)
model_080 F1_score: 82.92% >>>
973
('Epoch Training Loss:', 0.0028172495785838692)
('Epoch Training Acc:', 1.0)
('test loss', 1.2551908)
('test Acc:', 0.79094589)
model_080 F1_score: 82.99% >>>
974
('Epoch Training Loss:', 0.002794172027734021)
('Epoch Training Acc:', 1.0)
('test loss', 1.2575403)
('test Acc:', 0.78947371)
model_080 F1_score: 82.81% >>>
975
('Epoch Training Loss:', 0.0026337932185924728)
('Epoch Training Acc:', 1.0)
('test loss', 1.2543478)
('test Acc:', 0.79462641)
model_080 F1_score: 83.21% >>>
976
('Epoch Training Loss:', 0.0028016008668600989)
('Epoch Training Acc:', 1.0)
('test loss', 1.2641473)
('test Acc:', 0.78395289)
model_080 F1_score: 82.47% >>>
977
('Epoch Training Loss:', 0.0027793179369837162)
('Epoch Training Acc:', 1.0)
('test loss', 1.2489641)
('test Acc:', 0.78836954)
model_080 F1_score: 82.75% >>>
978
('Epoch Training Loss:', 0.0027909339428333624)
('Epoch Training Acc:', 1.0)
('test loss', 1.2624574)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
979
('Epoch Training Loss:', 0.0027484056649882405)
('Epoch Training Acc:', 1.0)
('test loss', 1.246199)
('test Acc:', 0.78947371)
model_080 F1_score: 82.74% >>>
980
('Epoch Training Loss:', 0.0026985647818946745)
('Epoch Training Acc:', 1.0)
('test loss', 1.228079)
('test Acc:', 0.79793888)
model_080 F1_score: 83.40% >>>
981
('Epoch Training Loss:', 0.0027575448434618011)
('Epoch Training Acc:', 1.0)
('test loss', 1.2405068)
('test Acc:', 0.79315424)
model_080 F1_score: 83.24% >>>
982
('Epoch Training Loss:', 0.0027075796297140187)
('Epoch Training Acc:', 1.0)
('test loss', 1.2508827)
('test Acc:', 0.79499447)
model_080 F1_score: 83.17% >>>
983
('Epoch Training Loss:', 0.002689094877041498)
('Epoch Training Acc:', 1.0)
('test loss', 1.2481934)
('test Acc:', 0.7887376)
model_080 F1_score: 82.70% >>>
984
('Epoch Training Loss:', 0.0027707396307050658)
('Epoch Training Acc:', 1.0)
('test loss', 1.2517195)
('test Acc:', 0.79131395)
model_080 F1_score: 82.87% >>>
985
('Epoch Training Loss:', 0.0027312816364428727)
('Epoch Training Acc:', 1.0)
('test loss', 1.2584437)
('test Acc:', 0.79425836)
model_080 F1_score: 83.28% >>>
986
('Epoch Training Loss:', 0.0027382364160075667)
('Epoch Training Acc:', 1.0)
('test loss', 1.2364885)
('test Acc:', 0.79057783)
model_080 F1_score: 82.59% >>>
987
('Epoch Training Loss:', 0.002630797865549539)
('Epoch Training Acc:', 1.0)
('test loss', 1.2350557)
('test Acc:', 0.79131395)
model_080 F1_score: 83.01% >>>
988
('Epoch Training Loss:', 0.0025432390498281165)
('Epoch Training Acc:', 1.0)
('test loss', 1.248451)
('test Acc:', 0.7868973)
model_080 F1_score: 82.68% >>>
989
('Epoch Training Loss:', 0.0026877375971707806)
('Epoch Training Acc:', 1.0)
('test loss', 1.2479331)
('test Acc:', 0.79720283)
model_080 F1_score: 83.28% >>>
990
('Epoch Training Loss:', 0.0026996813876394299)
('Epoch Training Acc:', 1.0)
('test loss', 1.2406448)
('test Acc:', 0.7938903)
model_080 F1_score: 83.07% >>>
991
('Epoch Training Loss:', 0.0025925521435965493)
('Epoch Training Acc:', 1.0)
('test loss', 1.2609121)
('test Acc:', 0.78836954)
model_080 F1_score: 82.90% >>>
992
('Epoch Training Loss:', 0.002610221235499921)
('Epoch Training Acc:', 1.0)
('test loss', 1.2464379)
('test Acc:', 0.79131395)
model_080 F1_score: 82.89% >>>
993
('Epoch Training Loss:', 0.0025299838839600852)
('Epoch Training Acc:', 1.0)
('test loss', 1.2379165)
('test Acc:', 0.79499447)
model_080 F1_score: 83.23% >>>
994
('Epoch Training Loss:', 0.0025751004523044685)
('Epoch Training Acc:', 1.0)
('test loss', 1.247016)
('test Acc:', 0.78910565)
model_080 F1_score: 82.93% >>>
995
('Epoch Training Loss:', 0.0026547313768787717)
('Epoch Training Acc:', 1.0)
('test loss', 1.2594929)
('test Acc:', 0.79241812)
model_080 F1_score: 83.24% >>>
996
('Epoch Training Loss:', 0.0027842772256008175)
('Epoch Training Acc:', 1.0)
('test loss', 1.2382826)
('test Acc:', 0.79720283)
model_080 F1_score: 83.45% >>>
997
('Epoch Training Loss:', 0.002687821951440128)
('Epoch Training Acc:', 1.0)
('test loss', 1.2380829)
('test Acc:', 0.78984171)
model_080 F1_score: 82.95% >>>
998
('Epoch Training Loss:', 0.0026802650177160103)
('Epoch Training Acc:', 1.0)
('test loss', 1.2654719)
('test Acc:', 0.78468901)
model_080 F1_score: 82.42% >>>
999
('Epoch Training Loss:', 0.0026162639223912265)
('Epoch Training Acc:', 1.0)
('test loss', 1.2410491)
('test Acc:', 0.78984171)
model_080 F1_score: 83.08% >>>
('Max Test Accuracy', 0.79904306)


>>>>>> Next Run <<<<<<<<<<<<<<<<<<<<<<<<<<

Start Training ------- 
0
('Epoch Training Loss:', 223.09461230039597)
('Epoch Training Acc:', 0.5633750037755817)
('test loss', 0.89502454)
('test Acc:', 0.7235922)
model_080 F1_score: 77.05% >>>
1
('Epoch Training Loss:', 99.19901695847511)
('Epoch Training Acc:', 0.7974999982863664)
('test loss', 0.7810048)
('test Acc:', 0.7622378)
model_080 F1_score: 80.49% >>>
2
('Epoch Training Loss:', 49.96612983942032)
('Epoch Training Acc:', 0.9037499945610762)
('test loss', 0.6887071)
('test Acc:', 0.7751196)
model_080 F1_score: 81.53% >>>
3
('Epoch Training Loss:', 17.44923062250018)
('Epoch Training Acc:', 0.9786250032484531)
('test loss', 0.7847257)
('test Acc:', 0.73941845)
model_080 F1_score: 78.67% >>>
4
('Epoch Training Loss:', 5.842766135931015)
('Epoch Training Acc:', 0.9970000021159648)
('test loss', 0.7601298)
('test Acc:', 0.7673905)
model_080 F1_score: 81.41% >>>
5
('Epoch Training Loss:', 2.502723596058786)
('Epoch Training Acc:', 0.9997500002384185)
('test loss', 0.73695165)
('test Acc:', 0.78652924)
model_080 F1_score: 82.74% >>>
6
('Epoch Training Loss:', 1.4724524547345936)
('Epoch Training Acc:', 0.9998750001192093)
('test loss', 0.7624703)
('test Acc:', 0.7857931)
model_080 F1_score: 82.61% >>>
7
('Epoch Training Loss:', 1.03095787554048)
('Epoch Training Acc:', 1.0)
('test loss', 0.8102417)
('test Acc:', 0.78652924)
model_080 F1_score: 82.76% >>>
8
('Epoch Training Loss:', 0.8059487687423825)
('Epoch Training Acc:', 1.0)
('test loss', 0.810173)
('test Acc:', 0.78726536)
model_080 F1_score: 82.75% >>>
9
('Epoch Training Loss:', 0.6337726167403162)
('Epoch Training Acc:', 1.0)
('test loss', 0.8168081)
('test Acc:', 0.7876334)
model_080 F1_score: 82.53% >>>
10
('Epoch Training Loss:', 0.5453513098182157)
('Epoch Training Acc:', 1.0)
('test loss', 0.8329667)
('test Acc:', 0.7861612)
model_080 F1_score: 82.63% >>>
11
('Epoch Training Loss:', 0.4692974964855239)
('Epoch Training Acc:', 1.0)
('test loss', 0.8473213)
('test Acc:', 0.7887376)
model_080 F1_score: 82.89% >>>
12
('Epoch Training Loss:', 0.41863069380633533)
('Epoch Training Acc:', 1.0)
('test loss', 0.8459579)
('test Acc:', 0.79131395)
model_080 F1_score: 83.04% >>>
13
('Epoch Training Loss:', 0.3717249153414741)
('Epoch Training Acc:', 1.0)
('test loss', 0.8538091)
('test Acc:', 0.78836954)
model_080 F1_score: 82.90% >>>
14
('Epoch Training Loss:', 0.3245271348860115)
('Epoch Training Acc:', 1.0)
('test loss', 0.8651041)
('test Acc:', 0.78726536)
model_080 F1_score: 82.55% >>>
15
('Epoch Training Loss:', 0.29871468694182113)
('Epoch Training Acc:', 1.0)
('test loss', 0.87450236)
('test Acc:', 0.79205006)
model_080 F1_score: 83.12% >>>
16
('Epoch Training Loss:', 0.2788369229529053)
('Epoch Training Acc:', 1.0)
('test loss', 0.87813205)
('test Acc:', 0.7898417)
model_080 F1_score: 82.94% >>>
17
('Epoch Training Loss:', 0.2644233767641708)
('Epoch Training Acc:', 1.0)
('test loss', 0.8866419)
('test Acc:', 0.79315424)
model_080 F1_score: 82.90% >>>
18
('Epoch Training Loss:', 0.23191475827479735)
('Epoch Training Acc:', 1.0)
('test loss', 0.8648329)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
19
('Epoch Training Loss:', 0.21943685796577483)
('Epoch Training Acc:', 1.0)
('test loss', 0.88790613)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
20
('Epoch Training Loss:', 0.2111084136995487)
('Epoch Training Acc:', 1.0)
('test loss', 0.90567183)
('test Acc:', 0.79131395)
model_080 F1_score: 82.99% >>>
21
('Epoch Training Loss:', 0.19438916811486706)
('Epoch Training Acc:', 1.0)
('test loss', 0.90927905)
('test Acc:', 0.79609865)
model_080 F1_score: 83.33% >>>
22
('Epoch Training Loss:', 0.18553956097457558)
('Epoch Training Acc:', 1.0)
('test loss', 0.909496)
('test Acc:', 0.7905778)
model_080 F1_score: 82.85% >>>
23
('Epoch Training Loss:', 0.17354099731892347)
('Epoch Training Acc:', 1.0)
('test loss', 0.91460073)
('test Acc:', 0.7949945)
model_080 F1_score: 83.15% >>>
24
('Epoch Training Loss:', 0.16522752097807825)
('Epoch Training Acc:', 1.0)
('test loss', 0.91020817)
('test Acc:', 0.7909459)
model_080 F1_score: 82.93% >>>
25
('Epoch Training Loss:', 0.1583440049435012)
('Epoch Training Acc:', 1.0)
('test loss', 0.90320015)
('test Acc:', 0.7905778)
model_080 F1_score: 82.96% >>>
26
('Epoch Training Loss:', 0.1540799333306495)
('Epoch Training Acc:', 1.0)
('test loss', 0.91798294)
('test Acc:', 0.78652924)
model_080 F1_score: 82.50% >>>
27
('Epoch Training Loss:', 0.1445455464127008)
('Epoch Training Acc:', 1.0)
('test loss', 0.90982044)
('test Acc:', 0.7957306)
model_080 F1_score: 83.35% >>>
28
('Epoch Training Loss:', 0.2638587392284535)
('Epoch Training Acc:', 0.9997500002384185)
('test loss', 0.9119968)
('test Acc:', 0.78910565)
model_080 F1_score: 82.92% >>>
29
('Epoch Training Loss:', 0.18583557827514596)
('Epoch Training Acc:', 0.9998750001192093)
('test loss', 0.92958003)
('test Acc:', 0.7902098)
model_080 F1_score: 82.97% >>>
30
('Epoch Training Loss:', 0.1315412508265581)
('Epoch Training Acc:', 1.0)
('test loss', 0.938262)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
31
('Epoch Training Loss:', 0.1290559737244621)
('Epoch Training Acc:', 1.0)
('test loss', 0.94099236)
('test Acc:', 0.7909459)
model_080 F1_score: 82.77% >>>
32
('Epoch Training Loss:', 0.12350989747210406)
('Epoch Training Acc:', 1.0)
('test loss', 0.93568474)
('test Acc:', 0.7894737)
model_080 F1_score: 82.64% >>>
33
('Epoch Training Loss:', 0.11843150103231892)
('Epoch Training Acc:', 1.0)
('test loss', 0.93694973)
('test Acc:', 0.7894737)
model_080 F1_score: 82.72% >>>
34
('Epoch Training Loss:', 0.1138757296721451)
('Epoch Training Acc:', 1.0)
('test loss', 0.9396985)
('test Acc:', 0.7887376)
model_080 F1_score: 82.89% >>>
35
('Epoch Training Loss:', 0.10705742001300678)
('Epoch Training Acc:', 1.0)
('test loss', 0.9488284)
('test Acc:', 0.7924181)
model_080 F1_score: 83.30% >>>
36
('Epoch Training Loss:', 0.102225283451844)
('Epoch Training Acc:', 1.0)
('test loss', 0.94902575)
('test Acc:', 0.7909459)
model_080 F1_score: 82.94% >>>
37
('Epoch Training Loss:', 0.10292426048545167)
('Epoch Training Acc:', 1.0)
('test loss', 0.9566648)
('test Acc:', 0.7887376)
model_080 F1_score: 82.70% >>>
38
('Epoch Training Loss:', 0.09649529674788937)
('Epoch Training Acc:', 1.0)
('test loss', 0.9492743)
('test Acc:', 0.7887376)
model_080 F1_score: 82.89% >>>
39
('Epoch Training Loss:', 0.09382590232416987)
('Epoch Training Acc:', 1.0)
('test loss', 0.9616853)
('test Acc:', 0.7927862)
model_080 F1_score: 83.14% >>>
40
('Epoch Training Loss:', 0.09234915295382962)
('Epoch Training Acc:', 1.0)
('test loss', 0.9566111)
('test Acc:', 0.7909459)
model_080 F1_score: 82.84% >>>
41
('Epoch Training Loss:', 0.08709796049515717)
('Epoch Training Acc:', 1.0)
('test loss', 0.97885656)
('test Acc:', 0.7909459)
model_080 F1_score: 83.18% >>>
42
('Epoch Training Loss:', 0.0888857351965271)
('Epoch Training Acc:', 1.0)
('test loss', 0.93253416)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
43
('Epoch Training Loss:', 0.08382202037319075)
('Epoch Training Acc:', 1.0)
('test loss', 0.95705926)
('test Acc:', 0.78836954)
model_080 F1_score: 82.69% >>>
44
('Epoch Training Loss:', 0.08127874389174394)
('Epoch Training Acc:', 1.0)
('test loss', 0.9660143)
('test Acc:', 0.7876334)
model_080 F1_score: 82.49% >>>
45
('Epoch Training Loss:', 0.0824760221876204)
('Epoch Training Acc:', 1.0)
('test loss', 0.96501553)
('test Acc:', 0.79131395)
model_080 F1_score: 83.01% >>>
46
('Epoch Training Loss:', 0.07776130251295399)
('Epoch Training Acc:', 1.0)
('test loss', 0.97580266)
('test Acc:', 0.7927862)
model_080 F1_score: 83.04% >>>
47
('Epoch Training Loss:', 0.07764376023260411)
('Epoch Training Acc:', 1.0)
('test loss', 0.96571195)
('test Acc:', 0.7927862)
model_080 F1_score: 83.04% >>>
48
('Epoch Training Loss:', 0.07456052540510427)
('Epoch Training Acc:', 1.0)
('test loss', 0.95909005)
('test Acc:', 0.7905778)
model_080 F1_score: 83.04% >>>
49
('Epoch Training Loss:', 0.07389828846498858)
('Epoch Training Acc:', 1.0)
('test loss', 0.9711371)
('test Acc:', 0.79315424)
model_080 F1_score: 83.00% >>>
50
('Epoch Training Loss:', 0.07108121587953065)
('Epoch Training Acc:', 1.0)
('test loss', 0.9688975)
('test Acc:', 0.79352224)
model_080 F1_score: 83.36% >>>
51
('Epoch Training Loss:', 0.07096912288398016)
('Epoch Training Acc:', 1.0)
('test loss', 0.9900014)
('test Acc:', 0.7868973)
model_080 F1_score: 82.62% >>>
52
('Epoch Training Loss:', 0.07216050960414577)
('Epoch Training Acc:', 1.0)
('test loss', 0.9687854)
('test Acc:', 0.7898417)
model_080 F1_score: 82.88% >>>
53
('Epoch Training Loss:', 0.06578995520249009)
('Epoch Training Acc:', 1.0)
('test loss', 0.96988696)
('test Acc:', 0.79131395)
model_080 F1_score: 83.03% >>>
54
('Epoch Training Loss:', 0.06430803528928664)
('Epoch Training Acc:', 1.0)
('test loss', 0.97866166)
('test Acc:', 0.7880015)
model_080 F1_score: 82.94% >>>
55
('Epoch Training Loss:', 0.06301193955005147)
('Epoch Training Acc:', 1.0)
('test loss', 0.9720987)
('test Acc:', 0.7909459)
model_080 F1_score: 82.85% >>>
56
('Epoch Training Loss:', 0.060407243261579424)
('Epoch Training Acc:', 1.0)
('test loss', 0.98999596)
('test Acc:', 0.79205006)
model_080 F1_score: 82.92% >>>
57
('Epoch Training Loss:', 0.0597599408501992)
('Epoch Training Acc:', 1.0)
('test loss', 0.9890309)
('test Acc:', 0.78910565)
model_080 F1_score: 82.54% >>>
58
('Epoch Training Loss:', 0.06041482847649604)
('Epoch Training Acc:', 1.0)
('test loss', 1.0102618)
('test Acc:', 0.7876334)
model_080 F1_score: 82.48% >>>
59
('Epoch Training Loss:', 0.0578367434500251)
('Epoch Training Acc:', 1.0)
('test loss', 0.989346)
('test Acc:', 0.791682)
model_080 F1_score: 82.93% >>>
60
('Epoch Training Loss:', 0.056728314884821884)
('Epoch Training Acc:', 1.0)
('test loss', 1.0061872)
('test Acc:', 0.7894737)
model_080 F1_score: 82.75% >>>
61
('Epoch Training Loss:', 0.055860609107185155)
('Epoch Training Acc:', 1.0)
('test loss', 0.9883731)
('test Acc:', 0.7938903)
model_080 F1_score: 83.20% >>>
62
('Epoch Training Loss:', 0.05594685487449169)
('Epoch Training Acc:', 1.0)
('test loss', 0.994818)
('test Acc:', 0.78836954)
model_080 F1_score: 82.61% >>>
63
('Epoch Training Loss:', 0.055009165065712295)
('Epoch Training Acc:', 1.0)
('test loss', 1.0003436)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
64
('Epoch Training Loss:', 0.052682848792755976)
('Epoch Training Acc:', 1.0)
('test loss', 0.9943088)
('test Acc:', 0.7898417)
model_080 F1_score: 82.95% >>>
65
('Epoch Training Loss:', 0.05191946343256859)
('Epoch Training Acc:', 1.0)
('test loss', 0.9985869)
('test Acc:', 0.79205006)
model_080 F1_score: 83.13% >>>
66
('Epoch Training Loss:', 0.051693819055799395)
('Epoch Training Acc:', 1.0)
('test loss', 1.0015707)
('test Acc:', 0.7938903)
model_080 F1_score: 83.18% >>>
67
('Epoch Training Loss:', 0.050990180199733004)
('Epoch Training Acc:', 1.0)
('test loss', 0.9981053)
('test Acc:', 0.79205006)
model_080 F1_score: 82.94% >>>
68
('Epoch Training Loss:', 0.049762658920371905)
('Epoch Training Acc:', 1.0)
('test loss', 1.0108421)
('test Acc:', 0.7902098)
model_080 F1_score: 82.64% >>>
69
('Epoch Training Loss:', 0.04995564824639587)
('Epoch Training Acc:', 1.0)
('test loss', 1.0046147)
('test Acc:', 0.7964667)
model_080 F1_score: 83.26% >>>
70
('Epoch Training Loss:', 0.047493008809397)
('Epoch Training Acc:', 1.0)
('test loss', 1.0069181)
('test Acc:', 0.79904306)
model_080 F1_score: 83.70% >>>
71
('Epoch Training Loss:', 0.047543053107801825)
('Epoch Training Acc:', 1.0)
('test loss', 1.0091708)
('test Acc:', 0.7924181)
model_080 F1_score: 83.07% >>>
72
('Epoch Training Loss:', 0.04774149566947017)
('Epoch Training Acc:', 1.0)
('test loss', 1.0264232)
('test Acc:', 0.7905778)
model_080 F1_score: 82.88% >>>
73
('Epoch Training Loss:', 0.0461790057743201)
('Epoch Training Acc:', 1.0)
('test loss', 1.0204755)
('test Acc:', 0.7924181)
model_080 F1_score: 83.22% >>>
74
('Epoch Training Loss:', 0.04572039640333969)
('Epoch Training Acc:', 1.0)
('test loss', 1.0067506)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
75
('Epoch Training Loss:', 0.044162540609249845)
('Epoch Training Acc:', 1.0)
('test loss', 1.014865)
('test Acc:', 0.79352224)
model_080 F1_score: 83.11% >>>
76
('Epoch Training Loss:', 0.04384882183512673)
('Epoch Training Acc:', 1.0)
('test loss', 1.0181496)
('test Acc:', 0.78836954)
model_080 F1_score: 82.74% >>>
77
('Epoch Training Loss:', 0.04364008831907995)
('Epoch Training Acc:', 1.0)
('test loss', 1.0254196)
('test Acc:', 0.7938903)
model_080 F1_score: 83.17% >>>
78
('Epoch Training Loss:', 0.04207638527441304)
('Epoch Training Acc:', 1.0)
('test loss', 1.0070347)
('test Acc:', 0.79315424)
model_080 F1_score: 83.12% >>>
79
('Epoch Training Loss:', 0.0430389495450072)
('Epoch Training Acc:', 1.0)
('test loss', 1.0134662)
('test Acc:', 0.7894737)
model_080 F1_score: 82.83% >>>
80
('Epoch Training Loss:', 0.042055105266626924)
('Epoch Training Acc:', 1.0)
('test loss', 1.0168262)
('test Acc:', 0.7905778)
model_080 F1_score: 82.73% >>>
81
('Epoch Training Loss:', 0.03968772145890398)
('Epoch Training Acc:', 1.0)
('test loss', 1.0211484)
('test Acc:', 0.7924181)
model_080 F1_score: 83.11% >>>
82
('Epoch Training Loss:', 0.04078352914075367)
('Epoch Training Acc:', 1.0)
('test loss', 1.0178024)
('test Acc:', 0.7876334)
model_080 F1_score: 82.61% >>>
83
('Epoch Training Loss:', 0.04091211454215227)
('Epoch Training Acc:', 1.0)
('test loss', 1.0241228)
('test Acc:', 0.78836954)
model_080 F1_score: 82.73% >>>
84
('Epoch Training Loss:', 0.03866640170599567)
('Epoch Training Acc:', 1.0)
('test loss', 1.0277615)
('test Acc:', 0.7876334)
model_080 F1_score: 82.47% >>>
85
('Epoch Training Loss:', 0.037355989072239026)
('Epoch Training Acc:', 1.0)
('test loss', 1.0264494)
('test Acc:', 0.7964667)
model_080 F1_score: 83.25% >>>
86
('Epoch Training Loss:', 0.03978830455162097)
('Epoch Training Acc:', 1.0)
('test loss', 1.027999)
('test Acc:', 0.7909459)
model_080 F1_score: 82.89% >>>
87
('Epoch Training Loss:', 0.03853538054681849)
('Epoch Training Acc:', 1.0)
('test loss', 1.0369207)
('test Acc:', 0.7902098)
model_080 F1_score: 82.92% >>>
88
('Epoch Training Loss:', 0.03733464062679559)
('Epoch Training Acc:', 1.0)
('test loss', 1.0312239)
('test Acc:', 0.7898417)
model_080 F1_score: 82.88% >>>
89
('Epoch Training Loss:', 0.03668790969095426)
('Epoch Training Acc:', 1.0)
('test loss', 1.0267916)
('test Acc:', 0.7924181)
model_080 F1_score: 82.87% >>>
90
('Epoch Training Loss:', 0.03645437127852347)
('Epoch Training Acc:', 1.0)
('test loss', 1.0275806)
('test Acc:', 0.7905778)
model_080 F1_score: 82.99% >>>
91
('Epoch Training Loss:', 0.03610669718909776)
('Epoch Training Acc:', 1.0)
('test loss', 1.0380453)
('test Acc:', 0.7905778)
model_080 F1_score: 82.84% >>>
92
('Epoch Training Loss:', 0.03436929596500704)
('Epoch Training Acc:', 1.0)
('test loss', 1.0434015)
('test Acc:', 0.7938903)
model_080 F1_score: 83.19% >>>
93
('Epoch Training Loss:', 0.03483226033858955)
('Epoch Training Acc:', 1.0)
('test loss', 1.0394561)
('test Acc:', 0.7909459)
model_080 F1_score: 83.10% >>>
94
('Epoch Training Loss:', 0.03443451579369139)
('Epoch Training Acc:', 1.0)
('test loss', 1.0331861)
('test Acc:', 0.791682)
model_080 F1_score: 83.02% >>>
95
('Epoch Training Loss:', 0.033948683623748366)
('Epoch Training Acc:', 1.0)
('test loss', 1.0466231)
('test Acc:', 0.7924181)
model_080 F1_score: 82.98% >>>
96
('Epoch Training Loss:', 0.03295978293317603)
('Epoch Training Acc:', 1.0)
('test loss', 1.0313063)
('test Acc:', 0.7898417)
model_080 F1_score: 82.82% >>>
97
('Epoch Training Loss:', 0.03280407087004278)
('Epoch Training Acc:', 1.0)
('test loss', 1.0408589)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
98
('Epoch Training Loss:', 0.03308535544783808)
('Epoch Training Acc:', 1.0)
('test loss', 1.0220821)
('test Acc:', 0.79205006)
model_080 F1_score: 83.00% >>>
99
('Epoch Training Loss:', 0.03223469717340777)
('Epoch Training Acc:', 1.0)
('test loss', 1.0414436)
('test Acc:', 0.7924181)
model_080 F1_score: 82.86% >>>
100
('Epoch Training Loss:', 0.031814986425160896)
('Epoch Training Acc:', 1.0)
('test loss', 1.040475)
('test Acc:', 0.7949945)
model_080 F1_score: 83.26% >>>
101
('Epoch Training Loss:', 0.031872287843725644)
('Epoch Training Acc:', 1.0)
('test loss', 1.0166855)
('test Acc:', 0.798675)
model_080 F1_score: 83.58% >>>
102
('Epoch Training Loss:', 0.030690106621477753)
('Epoch Training Acc:', 1.0)
('test loss', 1.041365)
('test Acc:', 0.7828487)
model_080 F1_score: 82.36% >>>
103
('Epoch Training Loss:', 0.03184044572117273)
('Epoch Training Acc:', 1.0)
('test loss', 1.0477422)
('test Acc:', 0.791682)
model_080 F1_score: 82.88% >>>
104
('Epoch Training Loss:', 0.030981223586422857)
('Epoch Training Acc:', 1.0)
('test loss', 1.0329106)
('test Acc:', 0.79131395)
model_080 F1_score: 82.93% >>>
105
('Epoch Training Loss:', 0.031149747388553806)
('Epoch Training Acc:', 1.0)
('test loss', 1.0458934)
('test Acc:', 0.79425836)
model_080 F1_score: 83.20% >>>
106
('Epoch Training Loss:', 0.030032676382688805)
('Epoch Training Acc:', 1.0)
('test loss', 1.0484725)
('test Acc:', 0.7861612)
model_080 F1_score: 82.66% >>>
107
('Epoch Training Loss:', 0.030445480108028278)
('Epoch Training Acc:', 1.0)
('test loss', 1.0383636)
('test Acc:', 0.7887376)
model_080 F1_score: 82.84% >>>
108
('Epoch Training Loss:', 0.02865669094171608)
('Epoch Training Acc:', 1.0)
('test loss', 1.0311763)
('test Acc:', 0.79536253)
model_080 F1_score: 83.34% >>>
109
('Epoch Training Loss:', 0.028287206092500128)
('Epoch Training Acc:', 1.0)
('test loss', 1.0528349)
('test Acc:', 0.79315424)
model_080 F1_score: 83.08% >>>
110
('Epoch Training Loss:', 0.02954819549631793)
('Epoch Training Acc:', 1.0)
('test loss', 1.038335)
('test Acc:', 0.7894737)
model_080 F1_score: 82.87% >>>
111
('Epoch Training Loss:', 0.029327175114303827)
('Epoch Training Acc:', 1.0)
('test loss', 1.0267546)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
112
('Epoch Training Loss:', 0.02895860750140855)
('Epoch Training Acc:', 1.0)
('test loss', 1.0347763)
('test Acc:', 0.798675)
model_080 F1_score: 83.62% >>>
113
('Epoch Training Loss:', 0.028650071551965084)
('Epoch Training Acc:', 1.0)
('test loss', 1.026772)
('test Acc:', 0.7938903)
model_080 F1_score: 83.07% >>>
114
('Epoch Training Loss:', 0.027949685339990538)
('Epoch Training Acc:', 1.0)
('test loss', 1.0463022)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
115
('Epoch Training Loss:', 0.02814445723197423)
('Epoch Training Acc:', 1.0)
('test loss', 1.0522239)
('test Acc:', 0.7898417)
model_080 F1_score: 82.73% >>>
116
('Epoch Training Loss:', 0.028503507252025884)
('Epoch Training Acc:', 1.0)
('test loss', 1.0502421)
('test Acc:', 0.79131395)
model_080 F1_score: 83.06% >>>
117
('Epoch Training Loss:', 0.026897306721366476)
('Epoch Training Acc:', 1.0)
('test loss', 1.0548737)
('test Acc:', 0.79425836)
model_080 F1_score: 83.17% >>>
118
('Epoch Training Loss:', 0.02773597394116223)
('Epoch Training Acc:', 1.0)
('test loss', 1.0590017)
('test Acc:', 0.7905778)
model_080 F1_score: 82.93% >>>
119
('Epoch Training Loss:', 0.026798523744218983)
('Epoch Training Acc:', 1.0)
('test loss', 1.0573099)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
120
('Epoch Training Loss:', 0.025381106839631684)
('Epoch Training Acc:', 1.0)
('test loss', 1.0514797)
('test Acc:', 0.7887376)
model_080 F1_score: 82.79% >>>
121
('Epoch Training Loss:', 0.026499328108911868)
('Epoch Training Acc:', 1.0)
('test loss', 1.0679753)
('test Acc:', 0.7887376)
model_080 F1_score: 82.71% >>>
122
('Epoch Training Loss:', 0.025781093259865884)
('Epoch Training Acc:', 1.0)
('test loss', 1.0452228)
('test Acc:', 0.7949945)
model_080 F1_score: 83.27% >>>
123
('Epoch Training Loss:', 0.025449362081417348)
('Epoch Training Acc:', 1.0)
('test loss', 1.0545068)
('test Acc:', 0.7902098)
model_080 F1_score: 82.81% >>>
124
('Epoch Training Loss:', 0.024931944732088596)
('Epoch Training Acc:', 1.0)
('test loss', 1.0541768)
('test Acc:', 0.79131395)
model_080 F1_score: 82.98% >>>
125
('Epoch Training Loss:', 0.025532810665026773)
('Epoch Training Acc:', 1.0)
('test loss', 1.0605397)
('test Acc:', 0.78726536)
model_080 F1_score: 82.69% >>>
126
('Epoch Training Loss:', 0.025479501477093436)
('Epoch Training Acc:', 1.0)
('test loss', 1.0628473)
('test Acc:', 0.7898417)
model_080 F1_score: 82.80% >>>
127
('Epoch Training Loss:', 0.024649995564686833)
('Epoch Training Acc:', 1.0)
('test loss', 1.0654516)
('test Acc:', 0.7927862)
model_080 F1_score: 83.03% >>>
128
('Epoch Training Loss:', 0.02560155714309076)
('Epoch Training Acc:', 1.0)
('test loss', 1.0543809)
('test Acc:', 0.79425836)
model_080 F1_score: 83.02% >>>
129
('Epoch Training Loss:', 0.023902034208731493)
('Epoch Training Acc:', 1.0)
('test loss', 1.0399163)
('test Acc:', 0.7924181)
model_080 F1_score: 82.95% >>>
130
('Epoch Training Loss:', 0.024991837330162525)
('Epoch Training Acc:', 1.0)
('test loss', 1.06166)
('test Acc:', 0.79536253)
model_080 F1_score: 83.42% >>>
131
('Epoch Training Loss:', 0.024602035104180686)
('Epoch Training Acc:', 1.0)
('test loss', 1.0605463)
('test Acc:', 0.7927862)
model_080 F1_score: 83.07% >>>
132
('Epoch Training Loss:', 0.023076615274476353)
('Epoch Training Acc:', 1.0)
('test loss', 1.0725886)
('test Acc:', 0.7887376)
model_080 F1_score: 82.79% >>>
133
('Epoch Training Loss:', 0.02339461826340994)
('Epoch Training Acc:', 1.0)
('test loss', 1.0649576)
('test Acc:', 0.7898417)
model_080 F1_score: 82.71% >>>
134
('Epoch Training Loss:', 0.02321307141392026)
('Epoch Training Acc:', 1.0)
('test loss', 1.0717747)
('test Acc:', 0.7894737)
model_080 F1_score: 82.88% >>>
135
('Epoch Training Loss:', 0.024329802039574133)
('Epoch Training Acc:', 1.0)
('test loss', 1.0701933)
('test Acc:', 0.78836954)
model_080 F1_score: 82.83% >>>
136
('Epoch Training Loss:', 0.022776727746531833)
('Epoch Training Acc:', 1.0)
('test loss', 1.0601295)
('test Acc:', 0.7927862)
model_080 F1_score: 83.04% >>>
137
('Epoch Training Loss:', 0.02292875947750872)
('Epoch Training Acc:', 1.0)
('test loss', 1.0511124)
('test Acc:', 0.7924181)
model_080 F1_score: 82.95% >>>
138
('Epoch Training Loss:', 0.022401315793104004)
('Epoch Training Acc:', 1.0)
('test loss', 1.0834378)
('test Acc:', 0.7894737)
model_080 F1_score: 82.80% >>>
139
('Epoch Training Loss:', 0.022540308265888598)
('Epoch Training Acc:', 1.0)
('test loss', 1.0588912)
('test Acc:', 0.79131395)
model_080 F1_score: 83.01% >>>
140
('Epoch Training Loss:', 0.02261309276218526)
('Epoch Training Acc:', 1.0)
('test loss', 1.0602627)
('test Acc:', 0.7898417)
model_080 F1_score: 82.76% >>>
141
('Epoch Training Loss:', 0.02178097360592801)
('Epoch Training Acc:', 1.0)
('test loss', 1.0686105)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
142
('Epoch Training Loss:', 0.022742644614481833)
('Epoch Training Acc:', 1.0)
('test loss', 1.075401)
('test Acc:', 0.7887376)
model_080 F1_score: 82.78% >>>
143
('Epoch Training Loss:', 0.021755957110144664)
('Epoch Training Acc:', 1.0)
('test loss', 1.0653172)
('test Acc:', 0.79352224)
model_080 F1_score: 83.16% >>>
144
('Epoch Training Loss:', 0.021633667878631968)
('Epoch Training Acc:', 1.0)
('test loss', 1.0596904)
('test Acc:', 0.7927862)
model_080 F1_score: 83.09% >>>
145
('Epoch Training Loss:', 0.02149025863764109)
('Epoch Training Acc:', 1.0)
('test loss', 1.0621271)
('test Acc:', 0.7957306)
model_080 F1_score: 83.31% >>>
146
('Epoch Training Loss:', 0.020863019220996648)
('Epoch Training Acc:', 1.0)
('test loss', 1.074791)
('test Acc:', 0.79315424)
model_080 F1_score: 83.24% >>>
147
('Epoch Training Loss:', 0.020551476933178492)
('Epoch Training Acc:', 1.0)
('test loss', 1.0903848)
('test Acc:', 0.7887376)
model_080 F1_score: 82.72% >>>
148
('Epoch Training Loss:', 0.02045660377916647)
('Epoch Training Acc:', 1.0)
('test loss', 1.0797702)
('test Acc:', 0.7887376)
model_080 F1_score: 82.76% >>>
149
('Epoch Training Loss:', 0.020203514148306567)
('Epoch Training Acc:', 1.0)
('test loss', 1.0612196)
('test Acc:', 0.7949945)
model_080 F1_score: 83.36% >>>
150
('Epoch Training Loss:', 0.020995015154767316)
('Epoch Training Acc:', 1.0)
('test loss', 1.0785325)
('test Acc:', 0.7924181)
model_080 F1_score: 83.05% >>>
151
('Epoch Training Loss:', 0.02006962171799387)
('Epoch Training Acc:', 1.0)
('test loss', 1.0668741)
('test Acc:', 0.7927862)
model_080 F1_score: 82.95% >>>
152
('Epoch Training Loss:', 0.0207154939926113)
('Epoch Training Acc:', 1.0)
('test loss', 1.063012)
('test Acc:', 0.791682)
model_080 F1_score: 83.12% >>>
153
('Epoch Training Loss:', 0.020167206741461996)
('Epoch Training Acc:', 1.0)
('test loss', 1.0699751)
('test Acc:', 0.7905778)
model_080 F1_score: 82.86% >>>
154
('Epoch Training Loss:', 0.01995466192238382)
('Epoch Training Acc:', 1.0)
('test loss', 1.0823478)
('test Acc:', 0.79352224)
model_080 F1_score: 83.07% >>>
155
('Epoch Training Loss:', 0.0193411910404393)
('Epoch Training Acc:', 1.0)
('test loss', 1.0991417)
('test Acc:', 0.7894737)
model_080 F1_score: 82.70% >>>
156
('Epoch Training Loss:', 0.019765713790548034)
('Epoch Training Acc:', 1.0)
('test loss', 1.0769583)
('test Acc:', 0.7894737)
model_080 F1_score: 82.81% >>>
157
('Epoch Training Loss:', 0.01955370431096526)
('Epoch Training Acc:', 1.0)
('test loss', 1.076373)
('test Acc:', 0.78836954)
model_080 F1_score: 82.71% >>>
158
('Epoch Training Loss:', 0.020052772313647438)
('Epoch Training Acc:', 1.0)
('test loss', 1.0764022)
('test Acc:', 0.7949945)
model_080 F1_score: 83.31% >>>
159
('Epoch Training Loss:', 0.019049899001402082)
('Epoch Training Acc:', 1.0)
('test loss', 1.0777763)
('test Acc:', 0.7938903)
model_080 F1_score: 83.26% >>>
160
('Epoch Training Loss:', 0.01922179547182168)
('Epoch Training Acc:', 1.0)
('test loss', 1.0843302)
('test Acc:', 0.7887376)
model_080 F1_score: 82.65% >>>
161
('Epoch Training Loss:', 0.019404750786634395)
('Epoch Training Acc:', 1.0)
('test loss', 1.0833882)
('test Acc:', 0.7927862)
model_080 F1_score: 83.20% >>>
162
('Epoch Training Loss:', 0.0193073292466579)
('Epoch Training Acc:', 1.0)
('test loss', 1.097957)
('test Acc:', 0.7909459)
model_080 F1_score: 82.91% >>>
163
('Epoch Training Loss:', 0.018810601879522437)
('Epoch Training Acc:', 1.0)
('test loss', 1.0756289)
('test Acc:', 0.7949945)
model_080 F1_score: 83.34% >>>
164
('Epoch Training Loss:', 0.018446343052346492)
('Epoch Training Acc:', 1.0)
('test loss', 1.0671823)
('test Acc:', 0.79536253)
model_080 F1_score: 83.34% >>>
165
('Epoch Training Loss:', 0.01835247543567675)
('Epoch Training Acc:', 1.0)
('test loss', 1.0917027)
('test Acc:', 0.7909459)
model_080 F1_score: 82.84% >>>
166
('Epoch Training Loss:', 0.017476238321251003)
('Epoch Training Acc:', 1.0)
('test loss', 1.0894572)
('test Acc:', 0.7938903)
model_080 F1_score: 83.43% >>>
167
('Epoch Training Loss:', 0.018823175258148694)
('Epoch Training Acc:', 1.0)
('test loss', 1.0737829)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
168
('Epoch Training Loss:', 0.01818263082532212)
('Epoch Training Acc:', 1.0)
('test loss', 1.0878617)
('test Acc:', 0.79609865)
model_080 F1_score: 83.30% >>>
169
('Epoch Training Loss:', 0.01869026499116444)
('Epoch Training Acc:', 1.0)
('test loss', 1.0809493)
('test Acc:', 0.79425836)
model_080 F1_score: 83.14% >>>
170
('Epoch Training Loss:', 0.01809348614187911)
('Epoch Training Acc:', 1.0)
('test loss', 1.0858055)
('test Acc:', 0.7909459)
model_080 F1_score: 82.98% >>>
171
('Epoch Training Loss:', 0.01798674976453185)
('Epoch Training Acc:', 1.0)
('test loss', 1.0697962)
('test Acc:', 0.79352224)
model_080 F1_score: 82.92% >>>
172
('Epoch Training Loss:', 0.017935974639840424)
('Epoch Training Acc:', 1.0)
('test loss', 1.1006514)
('test Acc:', 0.7905778)
model_080 F1_score: 82.63% >>>
173
('Epoch Training Loss:', 0.017150666390080005)
('Epoch Training Acc:', 1.0)
('test loss', 1.0857322)
('test Acc:', 0.7894737)
model_080 F1_score: 82.70% >>>
174
('Epoch Training Loss:', 0.018320766073884442)
('Epoch Training Acc:', 1.0)
('test loss', 1.0870792)
('test Acc:', 0.79315424)
model_080 F1_score: 83.31% >>>
175
('Epoch Training Loss:', 0.017344807885820046)
('Epoch Training Acc:', 1.0)
('test loss', 1.1003579)
('test Acc:', 0.79425836)
model_080 F1_score: 83.13% >>>
176
('Epoch Training Loss:', 0.01753456589358393)
('Epoch Training Acc:', 1.0)
('test loss', 1.1107428)
('test Acc:', 0.79609865)
model_080 F1_score: 83.32% >>>
177
('Epoch Training Loss:', 0.017206594566232525)
('Epoch Training Acc:', 1.0)
('test loss', 1.0708203)
('test Acc:', 0.79536253)
model_080 F1_score: 83.33% >>>
178
('Epoch Training Loss:', 0.01650661169696832)
('Epoch Training Acc:', 1.0)
('test loss', 1.093421)
('test Acc:', 0.791682)
model_080 F1_score: 82.99% >>>
179
('Epoch Training Loss:', 0.017534194921609014)
('Epoch Training Acc:', 1.0)
('test loss', 1.0940058)
('test Acc:', 0.7887376)
model_080 F1_score: 82.78% >>>
180
('Epoch Training Loss:', 0.017463110449170927)
('Epoch Training Acc:', 1.0)
('test loss', 1.1099424)
('test Acc:', 0.7924181)
model_080 F1_score: 83.06% >>>
181
('Epoch Training Loss:', 0.017333056475763442)
('Epoch Training Acc:', 1.0)
('test loss', 1.0873456)
('test Acc:', 0.7861612)
model_080 F1_score: 82.55% >>>
182
('Epoch Training Loss:', 0.01699404187820619)
('Epoch Training Acc:', 1.0)
('test loss', 1.1014018)
('test Acc:', 0.7876334)
model_080 F1_score: 82.55% >>>
183
('Epoch Training Loss:', 0.016555369846173562)
('Epoch Training Acc:', 1.0)
('test loss', 1.0947936)
('test Acc:', 0.78910565)
model_080 F1_score: 82.96% >>>
184
('Epoch Training Loss:', 0.016617728673736565)
('Epoch Training Acc:', 1.0)
('test loss', 1.0865641)
('test Acc:', 0.7975708)
model_080 F1_score: 83.57% >>>
185
('Epoch Training Loss:', 0.015624769166606711)
('Epoch Training Acc:', 1.0)
('test loss', 1.0913388)
('test Acc:', 0.79315424)
model_080 F1_score: 83.10% >>>
186
('Epoch Training Loss:', 0.016173014122614404)
('Epoch Training Acc:', 1.0)
('test loss', 1.0896721)
('test Acc:', 0.79315424)
model_080 F1_score: 83.12% >>>
187
('Epoch Training Loss:', 0.016318560497893486)
('Epoch Training Acc:', 1.0)
('test loss', 1.0870354)
('test Acc:', 0.79536253)
model_080 F1_score: 83.15% >>>
188
('Epoch Training Loss:', 0.01549613212409895)
('Epoch Training Acc:', 1.0)
('test loss', 1.0952938)
('test Acc:', 0.7946264)
model_080 F1_score: 83.09% >>>
189
('Epoch Training Loss:', 0.016112350960611366)
('Epoch Training Acc:', 1.0)
('test loss', 1.1042448)
('test Acc:', 0.7887376)
model_080 F1_score: 82.56% >>>
190
('Epoch Training Loss:', 0.016692496381438104)
('Epoch Training Acc:', 1.0)
('test loss', 1.0909318)
('test Acc:', 0.79536253)
model_080 F1_score: 83.31% >>>
191
('Epoch Training Loss:', 0.015968014791724272)
('Epoch Training Acc:', 1.0)
('test loss', 1.0857702)
('test Acc:', 0.791682)
model_080 F1_score: 83.06% >>>
192
('Epoch Training Loss:', 0.015902369170362363)
('Epoch Training Acc:', 1.0)
('test loss', 1.0778052)
('test Acc:', 0.791682)
model_080 F1_score: 82.89% >>>
193
('Epoch Training Loss:', 0.016090389792225324)
('Epoch Training Acc:', 1.0)
('test loss', 1.117846)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
194
('Epoch Training Loss:', 0.015718863782240078)
('Epoch Training Acc:', 1.0)
('test loss', 1.1078866)
('test Acc:', 0.7898417)
model_080 F1_score: 82.84% >>>
195
('Epoch Training Loss:', 0.01601509136344248)
('Epoch Training Acc:', 1.0)
('test loss', 1.0980781)
('test Acc:', 0.79205006)
model_080 F1_score: 82.67% >>>
196
('Epoch Training Loss:', 0.015061856818647357)
('Epoch Training Acc:', 1.0)
('test loss', 1.1034255)
('test Acc:', 0.78910565)
model_080 F1_score: 82.74% >>>
197
('Epoch Training Loss:', 0.01482026506710099)
('Epoch Training Acc:', 1.0)
('test loss', 1.0919988)
('test Acc:', 0.7894737)
model_080 F1_score: 82.82% >>>
198
('Epoch Training Loss:', 0.015719224778877106)
('Epoch Training Acc:', 1.0)
('test loss', 1.0817819)
('test Acc:', 0.78836954)
model_080 F1_score: 82.69% >>>
199
('Epoch Training Loss:', 0.015421265467011835)
('Epoch Training Acc:', 1.0)
('test loss', 1.087504)
('test Acc:', 0.791682)
model_080 F1_score: 83.09% >>>
200
('Epoch Training Loss:', 0.015132412965613184)
('Epoch Training Acc:', 1.0)
('test loss', 1.0923942)
('test Acc:', 0.7927862)
model_080 F1_score: 82.81% >>>
201
('Epoch Training Loss:', 0.015116937596758362)
('Epoch Training Acc:', 1.0)
('test loss', 1.1034983)
('test Acc:', 0.78836954)
model_080 F1_score: 82.72% >>>
202
('Epoch Training Loss:', 0.014472039991233032)
('Epoch Training Acc:', 1.0)
('test loss', 1.1056131)
('test Acc:', 0.7876334)
model_080 F1_score: 82.48% >>>
203
('Epoch Training Loss:', 0.015340918940637494)
('Epoch Training Acc:', 1.0)
('test loss', 1.0990243)
('test Acc:', 0.79315424)
model_080 F1_score: 82.82% >>>
204
('Epoch Training Loss:', 0.015270177285856334)
('Epoch Training Acc:', 1.0)
('test loss', 1.1037623)
('test Acc:', 0.791682)
model_080 F1_score: 82.89% >>>
205
('Epoch Training Loss:', 0.014510514294670429)
('Epoch Training Acc:', 1.0)
('test loss', 1.1075666)
('test Acc:', 0.7975708)
model_080 F1_score: 83.41% >>>
206
('Epoch Training Loss:', 0.014766228439839324)
('Epoch Training Acc:', 1.0)
('test loss', 1.1163155)
('test Acc:', 0.78910565)
model_080 F1_score: 82.69% >>>
207
('Epoch Training Loss:', 0.01486217009005486)
('Epoch Training Acc:', 1.0)
('test loss', 1.1001788)
('test Acc:', 0.79315424)
model_080 F1_score: 82.99% >>>
208
('Epoch Training Loss:', 0.014319573590910295)
('Epoch Training Acc:', 1.0)
('test loss', 1.1011355)
('test Acc:', 0.79425836)
model_080 F1_score: 83.24% >>>
209
('Epoch Training Loss:', 0.015045063362777)
('Epoch Training Acc:', 1.0)
('test loss', 1.0960869)
('test Acc:', 0.79425836)
model_080 F1_score: 83.25% >>>
210
('Epoch Training Loss:', 0.014590745719033293)
('Epoch Training Acc:', 1.0)
('test loss', 1.114246)
('test Acc:', 0.79131395)
model_080 F1_score: 82.89% >>>
211
('Epoch Training Loss:', 0.014347942156746285)
('Epoch Training Acc:', 1.0)
('test loss', 1.0999212)
('test Acc:', 0.79131395)
model_080 F1_score: 82.99% >>>
212
('Epoch Training Loss:', 0.014174108175211586)
('Epoch Training Acc:', 1.0)
('test loss', 1.1284257)
('test Acc:', 0.7857931)
model_080 F1_score: 82.81% >>>
213
('Epoch Training Loss:', 0.014127289086900419)
('Epoch Training Acc:', 1.0)
('test loss', 1.0978228)
('test Acc:', 0.7898417)
model_080 F1_score: 82.79% >>>
214
('Epoch Training Loss:', 0.014267802525864681)
('Epoch Training Acc:', 1.0)
('test loss', 1.1075625)
('test Acc:', 0.7876334)
model_080 F1_score: 82.58% >>>
215
('Epoch Training Loss:', 0.01395027469334309)
('Epoch Training Acc:', 1.0)
('test loss', 1.1047368)
('test Acc:', 0.791682)
model_080 F1_score: 83.21% >>>
216
('Epoch Training Loss:', 0.014149337206617929)
('Epoch Training Acc:', 1.0)
('test loss', 1.091868)
('test Acc:', 0.79609865)
model_080 F1_score: 83.47% >>>
217
('Epoch Training Loss:', 0.013412042546406155)
('Epoch Training Acc:', 1.0)
('test loss', 1.1135385)
('test Acc:', 0.78726536)
model_080 F1_score: 82.66% >>>
218
('Epoch Training Loss:', 0.014172125644108746)
('Epoch Training Acc:', 1.0)
('test loss', 1.1218904)
('test Acc:', 0.78726536)
model_080 F1_score: 82.61% >>>
219
('Epoch Training Loss:', 0.013657825213158503)
('Epoch Training Acc:', 1.0)
('test loss', 1.1138024)
('test Acc:', 0.7924181)
model_080 F1_score: 82.94% >>>
220
('Epoch Training Loss:', 0.014264751040172996)
('Epoch Training Acc:', 1.0)
('test loss', 1.1224172)
('test Acc:', 0.78836954)
model_080 F1_score: 82.79% >>>
221
('Epoch Training Loss:', 0.013584906795585994)
('Epoch Training Acc:', 1.0)
('test loss', 1.1117225)
('test Acc:', 0.79352224)
model_080 F1_score: 82.97% >>>
222
('Epoch Training Loss:', 0.013759807199676288)
('Epoch Training Acc:', 1.0)
('test loss', 1.1055021)
('test Acc:', 0.7924181)
model_080 F1_score: 83.06% >>>
223
('Epoch Training Loss:', 0.01332683226428344)
('Epoch Training Acc:', 1.0)
('test loss', 1.1240485)
('test Acc:', 0.78836954)
model_080 F1_score: 82.73% >>>
224
('Epoch Training Loss:', 0.013791435256280238)
('Epoch Training Acc:', 1.0)
('test loss', 1.1169356)
('test Acc:', 0.79425836)
model_080 F1_score: 83.24% >>>
225
('Epoch Training Loss:', 0.013408174101641634)
('Epoch Training Acc:', 1.0)
('test loss', 1.1123022)
('test Acc:', 0.79205006)
model_080 F1_score: 82.98% >>>
226
('Epoch Training Loss:', 0.01251869846964837)
('Epoch Training Acc:', 1.0)
('test loss', 1.1038933)
('test Acc:', 0.79536253)
model_080 F1_score: 83.18% >>>
227
('Epoch Training Loss:', 0.012942823568664608)
('Epoch Training Acc:', 1.0)
('test loss', 1.1042869)
('test Acc:', 0.78836954)
model_080 F1_score: 82.76% >>>
228
('Epoch Training Loss:', 0.013224150648966315)
('Epoch Training Acc:', 1.0)
('test loss', 1.1044579)
('test Acc:', 0.7938903)
model_080 F1_score: 83.23% >>>
229
('Epoch Training Loss:', 0.013416344467259478)
('Epoch Training Acc:', 1.0)
('test loss', 1.1132433)
('test Acc:', 0.7902098)
model_080 F1_score: 82.69% >>>
230
('Epoch Training Loss:', 0.012308738685533172)
('Epoch Training Acc:', 1.0)
('test loss', 1.1149338)
('test Acc:', 0.79352224)
model_080 F1_score: 83.16% >>>
231
('Epoch Training Loss:', 0.012701732754067052)
('Epoch Training Acc:', 1.0)
('test loss', 1.1283404)
('test Acc:', 0.79425836)
model_080 F1_score: 83.21% >>>
232
('Epoch Training Loss:', 0.01274204683068092)
('Epoch Training Acc:', 1.0)
('test loss', 1.1229324)
('test Acc:', 0.7894737)
model_080 F1_score: 82.83% >>>
233
('Epoch Training Loss:', 0.012775687489920529)
('Epoch Training Acc:', 1.0)
('test loss', 1.1227175)
('test Acc:', 0.78910565)
model_080 F1_score: 82.78% >>>
234
('Epoch Training Loss:', 0.012674838179009384)
('Epoch Training Acc:', 1.0)
('test loss', 1.1302389)
('test Acc:', 0.7949945)
model_080 F1_score: 83.18% >>>
235
('Epoch Training Loss:', 0.012466553725971607)
('Epoch Training Acc:', 1.0)
('test loss', 1.1335144)
('test Acc:', 0.7905778)
model_080 F1_score: 82.91% >>>
236
('Epoch Training Loss:', 0.012384905989165418)
('Epoch Training Acc:', 1.0)
('test loss', 1.1215923)
('test Acc:', 0.7894737)
model_080 F1_score: 83.05% >>>
237
('Epoch Training Loss:', 0.012305579417443369)
('Epoch Training Acc:', 1.0)
('test loss', 1.121523)
('test Acc:', 0.7902098)
model_080 F1_score: 82.93% >>>
238
('Epoch Training Loss:', 0.01283768011853681)
('Epoch Training Acc:', 1.0)
('test loss', 1.1215736)
('test Acc:', 0.7880015)
model_080 F1_score: 82.93% >>>
239
('Epoch Training Loss:', 0.012848341673816321)
('Epoch Training Acc:', 1.0)
('test loss', 1.1136289)
('test Acc:', 0.7997792)
model_080 F1_score: 83.68% >>>
240
('Epoch Training Loss:', 0.012509822336141951)
('Epoch Training Acc:', 1.0)
('test loss', 1.1091223)
('test Acc:', 0.7927862)
model_080 F1_score: 83.03% >>>
241
('Epoch Training Loss:', 0.01210635222014389)
('Epoch Training Acc:', 1.0)
('test loss', 1.0829402)
('test Acc:', 0.7902098)
model_080 F1_score: 82.96% >>>
242
('Epoch Training Loss:', 0.012351100060186582)
('Epoch Training Acc:', 1.0)
('test loss', 1.1174631)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
243
('Epoch Training Loss:', 0.012048481359670404)
('Epoch Training Acc:', 1.0)
('test loss', 1.1180412)
('test Acc:', 0.7902098)
model_080 F1_score: 83.13% >>>
244
('Epoch Training Loss:', 0.012113409145968035)
('Epoch Training Acc:', 1.0)
('test loss', 1.1149223)
('test Acc:', 0.79315424)
model_080 F1_score: 82.94% >>>
245
('Epoch Training Loss:', 0.011835161039925879)
('Epoch Training Acc:', 1.0)
('test loss', 1.1095687)
('test Acc:', 0.79352224)
model_080 F1_score: 83.02% >>>
246
('Epoch Training Loss:', 0.012182151458546286)
('Epoch Training Acc:', 1.0)
('test loss', 1.1164333)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
247
('Epoch Training Loss:', 0.012545398172733258)
('Epoch Training Acc:', 1.0)
('test loss', 1.1114532)
('test Acc:', 0.79352224)
model_080 F1_score: 83.12% >>>
248
('Epoch Training Loss:', 0.011754142771678744)
('Epoch Training Acc:', 1.0)
('test loss', 1.1313853)
('test Acc:', 0.79683477)
model_080 F1_score: 83.54% >>>
249
('Epoch Training Loss:', 0.012169076599093387)
('Epoch Training Acc:', 1.0)
('test loss', 1.1031376)
('test Acc:', 0.7927862)
model_080 F1_score: 83.27% >>>
250
('Epoch Training Loss:', 0.011916842915525194)
('Epoch Training Acc:', 1.0)
('test loss', 1.1092249)
('test Acc:', 0.7898417)
model_080 F1_score: 82.97% >>>
251
('Epoch Training Loss:', 0.012037422586217872)
('Epoch Training Acc:', 1.0)
('test loss', 1.1254513)
('test Acc:', 0.7909459)
model_080 F1_score: 82.88% >>>
252
('Epoch Training Loss:', 0.012170474930826458)
('Epoch Training Acc:', 1.0)
('test loss', 1.121858)
('test Acc:', 0.79352224)
model_080 F1_score: 83.30% >>>
253
('Epoch Training Loss:', 0.011405245526475483)
('Epoch Training Acc:', 1.0)
('test loss', 1.1276523)
('test Acc:', 0.79315424)
model_080 F1_score: 83.04% >>>
254
('Epoch Training Loss:', 0.011494741891510785)
('Epoch Training Acc:', 1.0)
('test loss', 1.1280951)
('test Acc:', 0.7909459)
model_080 F1_score: 82.99% >>>
255
('Epoch Training Loss:', 0.010980952178215375)
('Epoch Training Acc:', 1.0)
('test loss', 1.1213298)
('test Acc:', 0.79315424)
model_080 F1_score: 82.97% >>>
256
('Epoch Training Loss:', 0.011712411283951951)
('Epoch Training Acc:', 1.0)
('test loss', 1.1410763)
('test Acc:', 0.7868973)
model_080 F1_score: 82.59% >>>
257
('Epoch Training Loss:', 0.01168382183459471)
('Epoch Training Acc:', 1.0)
('test loss', 1.1289316)
('test Acc:', 0.7887376)
model_080 F1_score: 82.74% >>>
258
('Epoch Training Loss:', 0.011863351997817517)
('Epoch Training Acc:', 1.0)
('test loss', 1.1137165)
('test Acc:', 0.7946264)
model_080 F1_score: 83.40% >>>
259
('Epoch Training Loss:', 0.01125911605777219)
('Epoch Training Acc:', 1.0)
('test loss', 1.1469308)
('test Acc:', 0.7876334)
model_080 F1_score: 82.62% >>>
260
('Epoch Training Loss:', 0.011621466132055502)
('Epoch Training Acc:', 1.0)
('test loss', 1.1339064)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
261
('Epoch Training Loss:', 0.011592739567277022)
('Epoch Training Acc:', 1.0)
('test loss', 1.1192001)
('test Acc:', 0.7902098)
model_080 F1_score: 83.07% >>>
262
('Epoch Training Loss:', 0.01116734414063103)
('Epoch Training Acc:', 1.0)
('test loss', 1.1177037)
('test Acc:', 0.7902098)
model_080 F1_score: 82.76% >>>
263
('Epoch Training Loss:', 0.011159821948240278)
('Epoch Training Acc:', 1.0)
('test loss', 1.1331342)
('test Acc:', 0.7898417)
model_080 F1_score: 82.66% >>>
264
('Epoch Training Loss:', 0.01141907463897951)
('Epoch Training Acc:', 1.0)
('test loss', 1.1209668)
('test Acc:', 0.791682)
model_080 F1_score: 83.07% >>>
265
('Epoch Training Loss:', 0.010579574402072467)
('Epoch Training Acc:', 1.0)
('test loss', 1.118196)
('test Acc:', 0.7880015)
model_080 F1_score: 82.70% >>>
266
('Epoch Training Loss:', 0.010981107005136437)
('Epoch Training Acc:', 1.0)
('test loss', 1.124106)
('test Acc:', 0.7938903)
model_080 F1_score: 83.19% >>>
267
('Epoch Training Loss:', 0.011093823852206697)
('Epoch Training Acc:', 1.0)
('test loss', 1.1269746)
('test Acc:', 0.7905778)
model_080 F1_score: 83.03% >>>
268
('Epoch Training Loss:', 0.01156317282220698)
('Epoch Training Acc:', 1.0)
('test loss', 1.1422746)
('test Acc:', 0.7887376)
model_080 F1_score: 82.90% >>>
269
('Epoch Training Loss:', 0.011447027791291475)
('Epoch Training Acc:', 1.0)
('test loss', 1.1263914)
('test Acc:', 0.78726536)
model_080 F1_score: 82.65% >>>
270
('Epoch Training Loss:', 0.010831884008439374)
('Epoch Training Acc:', 1.0)
('test loss', 1.1126086)
('test Acc:', 0.78910565)
model_080 F1_score: 82.78% >>>
271
('Epoch Training Loss:', 0.010819686625836766)
('Epoch Training Acc:', 1.0)
('test loss', 1.1370555)
('test Acc:', 0.7887376)
model_080 F1_score: 82.75% >>>
272
('Epoch Training Loss:', 0.011040337923986954)
('Epoch Training Acc:', 1.0)
('test loss', 1.1399318)
('test Acc:', 0.7927862)
model_080 F1_score: 83.17% >>>
273
('Epoch Training Loss:', 0.011130974440675345)
('Epoch Training Acc:', 1.0)
('test loss', 1.1456609)
('test Acc:', 0.7898417)
model_080 F1_score: 82.96% >>>
274
('Epoch Training Loss:', 0.010805079959027353)
('Epoch Training Acc:', 1.0)
('test loss', 1.1113074)
('test Acc:', 0.79352224)
model_080 F1_score: 83.24% >>>
275
('Epoch Training Loss:', 0.01069788876156963)
('Epoch Training Acc:', 1.0)
('test loss', 1.1531396)
('test Acc:', 0.7898417)
model_080 F1_score: 82.79% >>>
276
('Epoch Training Loss:', 0.010345621116357506)
('Epoch Training Acc:', 1.0)
('test loss', 1.1238858)
('test Acc:', 0.791682)
model_080 F1_score: 82.87% >>>
277
('Epoch Training Loss:', 0.01023280481058464)
('Epoch Training Acc:', 1.0)
('test loss', 1.1281198)
('test Acc:', 0.7880015)
model_080 F1_score: 82.86% >>>
278
('Epoch Training Loss:', 0.010872774426388787)
('Epoch Training Acc:', 1.0)
('test loss', 1.1277093)
('test Acc:', 0.79536253)
model_080 F1_score: 83.34% >>>
279
('Epoch Training Loss:', 0.0105747759789665)
('Epoch Training Acc:', 1.0)
('test loss', 1.1272527)
('test Acc:', 0.7909459)
model_080 F1_score: 82.80% >>>
280
('Epoch Training Loss:', 0.010502183324206271)
('Epoch Training Acc:', 1.0)
('test loss', 1.1376572)
('test Acc:', 0.7902098)
model_080 F1_score: 82.83% >>>
281
('Epoch Training Loss:', 0.010506876929866849)
('Epoch Training Acc:', 1.0)
('test loss', 1.151121)
('test Acc:', 0.7894737)
model_080 F1_score: 83.01% >>>
282
('Epoch Training Loss:', 0.01045180686560343)
('Epoch Training Acc:', 1.0)
('test loss', 1.138503)
('test Acc:', 0.7894737)
model_080 F1_score: 82.82% >>>
283
('Epoch Training Loss:', 0.010120561066287337)
('Epoch Training Acc:', 1.0)
('test loss', 1.1314685)
('test Acc:', 0.7946264)
model_080 F1_score: 83.05% >>>
284
('Epoch Training Loss:', 0.00983829157485161)
('Epoch Training Acc:', 1.0)
('test loss', 1.1356398)
('test Acc:', 0.7927862)
model_080 F1_score: 83.07% >>>
285
('Epoch Training Loss:', 0.010109160812135087)
('Epoch Training Acc:', 1.0)
('test loss', 1.1222967)
('test Acc:', 0.7946264)
model_080 F1_score: 83.33% >>>
286
('Epoch Training Loss:', 0.010197145364145399)
('Epoch Training Acc:', 1.0)
('test loss', 1.1311799)
('test Acc:', 0.79205006)
model_080 F1_score: 83.16% >>>
287
('Epoch Training Loss:', 0.010363043560573715)
('Epoch Training Acc:', 1.0)
('test loss', 1.1343875)
('test Acc:', 0.7946264)
model_080 F1_score: 83.13% >>>
288
('Epoch Training Loss:', 0.0100799720876239)
('Epoch Training Acc:', 1.0)
('test loss', 1.126839)
('test Acc:', 0.7924181)
model_080 F1_score: 83.00% >>>
289
('Epoch Training Loss:', 0.010138561839994509)
('Epoch Training Acc:', 1.0)
('test loss', 1.1189966)
('test Acc:', 0.78910565)
model_080 F1_score: 82.89% >>>
290
('Epoch Training Loss:', 0.009744225639224169)
('Epoch Training Acc:', 1.0)
('test loss', 1.1473104)
('test Acc:', 0.7905778)
model_080 F1_score: 83.16% >>>
291
('Epoch Training Loss:', 0.010074209792946931)
('Epoch Training Acc:', 1.0)
('test loss', 1.133608)
('test Acc:', 0.7909459)
model_080 F1_score: 82.89% >>>
292
('Epoch Training Loss:', 0.010182363830608665)
('Epoch Training Acc:', 1.0)
('test loss', 1.1458155)
('test Acc:', 0.7924181)
model_080 F1_score: 83.02% >>>
293
('Epoch Training Loss:', 0.009877421818600851)
('Epoch Training Acc:', 1.0)
('test loss', 1.1344364)
('test Acc:', 0.7957306)
model_080 F1_score: 83.50% >>>
294
('Epoch Training Loss:', 0.010062719769848627)
('Epoch Training Acc:', 1.0)
('test loss', 1.1345016)
('test Acc:', 0.7868973)
model_080 F1_score: 82.62% >>>
295
('Epoch Training Loss:', 0.010124956332219881)
('Epoch Training Acc:', 1.0)
('test loss', 1.1324737)
('test Acc:', 0.7898417)
model_080 F1_score: 82.84% >>>
296
('Epoch Training Loss:', 0.010059562917376752)
('Epoch Training Acc:', 1.0)
('test loss', 1.1381006)
('test Acc:', 0.7861612)
model_080 F1_score: 82.63% >>>
297
('Epoch Training Loss:', 0.009834895232415874)
('Epoch Training Acc:', 1.0)
('test loss', 1.1309837)
('test Acc:', 0.7949945)
model_080 F1_score: 82.98% >>>
298
('Epoch Training Loss:', 0.00997735764758545)
('Epoch Training Acc:', 1.0)
('test loss', 1.1457548)
('test Acc:', 0.7909459)
model_080 F1_score: 83.06% >>>
299
('Epoch Training Loss:', 0.009581208407325903)
('Epoch Training Acc:', 1.0)
('test loss', 1.1408414)
('test Acc:', 0.7894737)
model_080 F1_score: 82.76% >>>
300
('Epoch Training Loss:', 0.010341748624341562)
('Epoch Training Acc:', 1.0)
('test loss', 1.1440592)
('test Acc:', 0.7868973)
model_080 F1_score: 82.47% >>>
301
('Epoch Training Loss:', 0.009542661262457841)
('Epoch Training Acc:', 1.0)
('test loss', 1.1446174)
('test Acc:', 0.78652924)
model_080 F1_score: 82.56% >>>
302
('Epoch Training Loss:', 0.009437034565053182)
('Epoch Training Acc:', 1.0)
('test loss', 1.135601)
('test Acc:', 0.78910565)
model_080 F1_score: 82.70% >>>
303
('Epoch Training Loss:', 0.009582518146999064)
('Epoch Training Acc:', 1.0)
('test loss', 1.1516224)
('test Acc:', 0.79609865)
model_080 F1_score: 83.41% >>>
304
('Epoch Training Loss:', 0.009573240044119302)
('Epoch Training Acc:', 1.0)
('test loss', 1.1552497)
('test Acc:', 0.7898417)
model_080 F1_score: 82.88% >>>
305
('Epoch Training Loss:', 0.009348324063466862)
('Epoch Training Acc:', 1.0)
('test loss', 1.1280895)
('test Acc:', 0.7927862)
model_080 F1_score: 83.06% >>>
306
('Epoch Training Loss:', 0.009292198858020129)
('Epoch Training Acc:', 1.0)
('test loss', 1.1516154)
('test Acc:', 0.7946264)
model_080 F1_score: 83.39% >>>
307
('Epoch Training Loss:', 0.009001919666843605)
('Epoch Training Acc:', 1.0)
('test loss', 1.1350188)
('test Acc:', 0.7957306)
model_080 F1_score: 83.26% >>>
308
('Epoch Training Loss:', 0.00953684008527489)
('Epoch Training Acc:', 1.0)
('test loss', 1.1513717)
('test Acc:', 0.78836954)
model_080 F1_score: 82.84% >>>
309
('Epoch Training Loss:', 0.009478232796027442)
('Epoch Training Acc:', 1.0)
('test loss', 1.1392602)
('test Acc:', 0.79131395)
model_080 F1_score: 82.99% >>>
310
('Epoch Training Loss:', 0.009539610458887182)
('Epoch Training Acc:', 1.0)
('test loss', 1.1478143)
('test Acc:', 0.79536253)
model_080 F1_score: 83.31% >>>
311
('Epoch Training Loss:', 0.009304322453317582)
('Epoch Training Acc:', 1.0)
('test loss', 1.1434045)
('test Acc:', 0.7887376)
model_080 F1_score: 82.88% >>>
312
('Epoch Training Loss:', 0.009546721003061975)
('Epoch Training Acc:', 1.0)
('test loss', 1.1518422)
('test Acc:', 0.7938903)
model_080 F1_score: 83.10% >>>
313
('Epoch Training Loss:', 0.008904549253202276)
('Epoch Training Acc:', 1.0)
('test loss', 1.1363723)
('test Acc:', 0.79683477)
model_080 F1_score: 83.46% >>>
314
('Epoch Training Loss:', 0.009303824002927286)
('Epoch Training Acc:', 1.0)
('test loss', 1.1369332)
('test Acc:', 0.79315424)
model_080 F1_score: 83.40% >>>
315
('Epoch Training Loss:', 0.00899496298552549)
('Epoch Training Acc:', 1.0)
('test loss', 1.149322)
('test Acc:', 0.7898417)
model_080 F1_score: 82.92% >>>
316
('Epoch Training Loss:', 0.009348179777589394)
('Epoch Training Acc:', 1.0)
('test loss', 1.1391187)
('test Acc:', 0.79205006)
model_080 F1_score: 83.15% >>>
317
('Epoch Training Loss:', 0.009043405432748841)
('Epoch Training Acc:', 1.0)
('test loss', 1.134353)
('test Acc:', 0.791682)
model_080 F1_score: 82.91% >>>
318
('Epoch Training Loss:', 0.008775724043516675)
('Epoch Training Acc:', 1.0)
('test loss', 1.147453)
('test Acc:', 0.7909459)
model_080 F1_score: 82.98% >>>
319
('Epoch Training Loss:', 0.009085458334084251)
('Epoch Training Acc:', 1.0)
('test loss', 1.1410228)
('test Acc:', 0.7905778)
model_080 F1_score: 83.02% >>>
320
('Epoch Training Loss:', 0.008993519975774689)
('Epoch Training Acc:', 1.0)
('test loss', 1.1282425)
('test Acc:', 0.7924181)
model_080 F1_score: 83.17% >>>
321
('Epoch Training Loss:', 0.0092149118463567)
('Epoch Training Acc:', 1.0)
('test loss', 1.1188929)
('test Acc:', 0.7949945)
model_080 F1_score: 83.11% >>>
322
('Epoch Training Loss:', 0.008812660060357302)
('Epoch Training Acc:', 1.0)
('test loss', 1.1553178)
('test Acc:', 0.7902098)
model_080 F1_score: 82.84% >>>
323
('Epoch Training Loss:', 0.009006420928926673)
('Epoch Training Acc:', 1.0)
('test loss', 1.1406628)
('test Acc:', 0.79315424)
model_080 F1_score: 83.20% >>>
324
('Epoch Training Loss:', 0.009294054587371647)
('Epoch Training Acc:', 1.0)
('test loss', 1.1472604)
('test Acc:', 0.7957306)
model_080 F1_score: 83.34% >>>
325
('Epoch Training Loss:', 0.008881289695636951)
('Epoch Training Acc:', 1.0)
('test loss', 1.1312956)
('test Acc:', 0.79352224)
model_080 F1_score: 83.11% >>>
326
('Epoch Training Loss:', 0.008683815774929826)
('Epoch Training Acc:', 1.0)
('test loss', 1.152082)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
327
('Epoch Training Loss:', 0.00862475005487795)
('Epoch Training Acc:', 1.0)
('test loss', 1.1539311)
('test Acc:', 0.78726536)
model_080 F1_score: 82.85% >>>
328
('Epoch Training Loss:', 0.009222826298355358)
('Epoch Training Acc:', 1.0)
('test loss', 1.1599929)
('test Acc:', 0.78726536)
model_080 F1_score: 82.59% >>>
329
('Epoch Training Loss:', 0.008542068335373187)
('Epoch Training Acc:', 1.0)
('test loss', 1.1519648)
('test Acc:', 0.7898417)
model_080 F1_score: 82.86% >>>
330
('Epoch Training Loss:', 0.008358022725587944)
('Epoch Training Acc:', 1.0)
('test loss', 1.15073)
('test Acc:', 0.79205006)
model_080 F1_score: 83.05% >>>
331
('Epoch Training Loss:', 0.008788265562543529)
('Epoch Training Acc:', 1.0)
('test loss', 1.1451645)
('test Acc:', 0.7876334)
model_080 F1_score: 82.64% >>>
332
('Epoch Training Loss:', 0.008651201986140222)
('Epoch Training Acc:', 1.0)
('test loss', 1.1347045)
('test Acc:', 0.79425836)
model_080 F1_score: 83.27% >>>
333
('Epoch Training Loss:', 0.008546236531401519)
('Epoch Training Acc:', 1.0)
('test loss', 1.1504866)
('test Acc:', 0.791682)
model_080 F1_score: 82.95% >>>
334
('Epoch Training Loss:', 0.008396944349442492)
('Epoch Training Acc:', 1.0)
('test loss', 1.1450552)
('test Acc:', 0.7946264)
model_080 F1_score: 83.21% >>>
335
('Epoch Training Loss:', 0.008753950291065848)
('Epoch Training Acc:', 1.0)
('test loss', 1.1650703)
('test Acc:', 0.791682)
model_080 F1_score: 83.00% >>>
336
('Epoch Training Loss:', 0.009037996678671334)
('Epoch Training Acc:', 1.0)
('test loss', 1.1337852)
('test Acc:', 0.7949945)
model_080 F1_score: 83.30% >>>
337
('Epoch Training Loss:', 0.008595555085776141)
('Epoch Training Acc:', 1.0)
('test loss', 1.1480321)
('test Acc:', 0.7905778)
model_080 F1_score: 82.93% >>>
338
('Epoch Training Loss:', 0.009275133706978522)
('Epoch Training Acc:', 1.0)
('test loss', 1.1429687)
('test Acc:', 0.7949945)
model_080 F1_score: 83.30% >>>
339
('Epoch Training Loss:', 0.00865333978254057)
('Epoch Training Acc:', 1.0)
('test loss', 1.1375685)
('test Acc:', 0.79315424)
model_080 F1_score: 83.01% >>>
340
('Epoch Training Loss:', 0.008195714379326091)
('Epoch Training Acc:', 1.0)
('test loss', 1.1680784)
('test Acc:', 0.79425836)
model_080 F1_score: 83.03% >>>
341
('Epoch Training Loss:', 0.008460621753329178)
('Epoch Training Acc:', 1.0)
('test loss', 1.1453445)
('test Acc:', 0.79131395)
model_080 F1_score: 83.12% >>>
342
('Epoch Training Loss:', 0.00855800524004735)
('Epoch Training Acc:', 1.0)
('test loss', 1.1413076)
('test Acc:', 0.79315424)
model_080 F1_score: 83.01% >>>
343
('Epoch Training Loss:', 0.008308426035000593)
('Epoch Training Acc:', 1.0)
('test loss', 1.1580544)
('test Acc:', 0.7861612)
model_080 F1_score: 82.61% >>>
344
('Epoch Training Loss:', 0.00810269106659689)
('Epoch Training Acc:', 1.0)
('test loss', 1.1552414)
('test Acc:', 0.7905778)
model_080 F1_score: 82.90% >>>
345
('Epoch Training Loss:', 0.008440552688625758)
('Epoch Training Acc:', 1.0)
('test loss', 1.167868)
('test Acc:', 0.7905778)
model_080 F1_score: 82.93% >>>
346
('Epoch Training Loss:', 0.00807376556804229)
('Epoch Training Acc:', 1.0)
('test loss', 1.1393129)
('test Acc:', 0.7949945)
model_080 F1_score: 83.21% >>>
347
('Epoch Training Loss:', 0.008506012598445523)
('Epoch Training Acc:', 1.0)
('test loss', 1.1447545)
('test Acc:', 0.79205006)
model_080 F1_score: 83.14% >>>
348
('Epoch Training Loss:', 0.008274476362203131)
('Epoch Training Acc:', 1.0)
('test loss', 1.1568625)
('test Acc:', 0.7868973)
model_080 F1_score: 82.58% >>>
349
('Epoch Training Loss:', 0.008392468500460382)
('Epoch Training Acc:', 1.0)
('test loss', 1.1572737)
('test Acc:', 0.7902098)
model_080 F1_score: 82.81% >>>
350
('Epoch Training Loss:', 0.007863126524171093)
('Epoch Training Acc:', 1.0)
('test loss', 1.1580997)
('test Acc:', 0.7902098)
model_080 F1_score: 82.89% >>>
351
('Epoch Training Loss:', 0.008272231818409637)
('Epoch Training Acc:', 1.0)
('test loss', 1.1489031)
('test Acc:', 0.7957306)
model_080 F1_score: 83.45% >>>
352
('Epoch Training Loss:', 0.008616342589448323)
('Epoch Training Acc:', 1.0)
('test loss', 1.1487969)
('test Acc:', 0.791682)
model_080 F1_score: 83.05% >>>
353
('Epoch Training Loss:', 0.00827189286974317)
('Epoch Training Acc:', 1.0)
('test loss', 1.14721)
('test Acc:', 0.78505707)
model_080 F1_score: 82.56% >>>
354
('Epoch Training Loss:', 0.008192418399630697)
('Epoch Training Acc:', 1.0)
('test loss', 1.1569848)
('test Acc:', 0.7898417)
model_080 F1_score: 83.02% >>>
355
('Epoch Training Loss:', 0.007913469471532153)
('Epoch Training Acc:', 1.0)
('test loss', 1.1513152)
('test Acc:', 0.79352224)
model_080 F1_score: 83.30% >>>
356
('Epoch Training Loss:', 0.008097771302345791)
('Epoch Training Acc:', 1.0)
('test loss', 1.1517435)
('test Acc:', 0.79352224)
model_080 F1_score: 82.97% >>>
357
('Epoch Training Loss:', 0.008834802185447188)
('Epoch Training Acc:', 1.0)
('test loss', 1.1560013)
('test Acc:', 0.7924181)
model_080 F1_score: 83.05% >>>
358
('Epoch Training Loss:', 0.007955423872772371)
('Epoch Training Acc:', 1.0)
('test loss', 1.1648191)
('test Acc:', 0.7924181)
model_080 F1_score: 83.17% >>>
359
('Epoch Training Loss:', 0.007836499928089324)
('Epoch Training Acc:', 1.0)
('test loss', 1.1538649)
('test Acc:', 0.7972028)
model_080 F1_score: 83.45% >>>
360
('Epoch Training Loss:', 0.007858231063437415)
('Epoch Training Acc:', 1.0)
('test loss', 1.152321)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
361
('Epoch Training Loss:', 0.008164287371982937)
('Epoch Training Acc:', 1.0)
('test loss', 1.1531105)
('test Acc:', 0.7938903)
model_080 F1_score: 83.00% >>>
362
('Epoch Training Loss:', 0.008146787809891975)
('Epoch Training Acc:', 1.0)
('test loss', 1.1581674)
('test Acc:', 0.79205006)
model_080 F1_score: 82.99% >>>
363
('Epoch Training Loss:', 0.007786472600855632)
('Epoch Training Acc:', 1.0)
('test loss', 1.1646665)
('test Acc:', 0.79315424)
model_080 F1_score: 83.23% >>>
364
('Epoch Training Loss:', 0.007874031392930192)
('Epoch Training Acc:', 1.0)
('test loss', 1.1585819)
('test Acc:', 0.79352224)
model_080 F1_score: 83.24% >>>
365
('Epoch Training Loss:', 0.008018587692276924)
('Epoch Training Acc:', 1.0)
('test loss', 1.1412923)
('test Acc:', 0.79683477)
model_080 F1_score: 83.45% >>>
366
('Epoch Training Loss:', 0.008033055268242606)
('Epoch Training Acc:', 1.0)
('test loss', 1.150566)
('test Acc:', 0.80014724)
model_080 F1_score: 83.83% >>>
367
('Epoch Training Loss:', 0.007965581015014322)
('Epoch Training Acc:', 1.0)
('test loss', 1.1501788)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
368
('Epoch Training Loss:', 0.007675696089791018)
('Epoch Training Acc:', 1.0)
('test loss', 1.153571)
('test Acc:', 0.79131395)
model_080 F1_score: 82.97% >>>
369
('Epoch Training Loss:', 0.007661654013645602)
('Epoch Training Acc:', 1.0)
('test loss', 1.13871)
('test Acc:', 0.79683477)
model_080 F1_score: 83.48% >>>
370
('Epoch Training Loss:', 0.007905781261797529)
('Epoch Training Acc:', 1.0)
('test loss', 1.1551877)
('test Acc:', 0.7924181)
model_080 F1_score: 83.00% >>>
371
('Epoch Training Loss:', 0.007456126058968948)
('Epoch Training Acc:', 1.0)
('test loss', 1.1614056)
('test Acc:', 0.7949945)
model_080 F1_score: 83.25% >>>
372
('Epoch Training Loss:', 0.0076974084422545275)
('Epoch Training Acc:', 1.0)
('test loss', 1.1562465)
('test Acc:', 0.7979389)
model_080 F1_score: 83.48% >>>
373
('Epoch Training Loss:', 0.00800675674872764)
('Epoch Training Acc:', 1.0)
('test loss', 1.1569653)
('test Acc:', 0.78910565)
model_080 F1_score: 82.87% >>>
374
('Epoch Training Loss:', 0.007508168511776603)
('Epoch Training Acc:', 1.0)
('test loss', 1.1624507)
('test Acc:', 0.7927862)
model_080 F1_score: 83.02% >>>
375
('Epoch Training Loss:', 0.0074050735238415655)
('Epoch Training Acc:', 1.0)
('test loss', 1.1697608)
('test Acc:', 0.7972028)
model_080 F1_score: 83.59% >>>
376
('Epoch Training Loss:', 0.0076127578868181445)
('Epoch Training Acc:', 1.0)
('test loss', 1.1574507)
('test Acc:', 0.78910565)
model_080 F1_score: 82.76% >>>
377
('Epoch Training Loss:', 0.007645152683835477)
('Epoch Training Acc:', 1.0)
('test loss', 1.1564528)
('test Acc:', 0.79830694)
model_080 F1_score: 83.66% >>>
378
('Epoch Training Loss:', 0.007366840329268598)
('Epoch Training Acc:', 1.0)
('test loss', 1.1544425)
('test Acc:', 0.7876334)
model_080 F1_score: 82.75% >>>
379
('Epoch Training Loss:', 0.007647378723959264)
('Epoch Training Acc:', 1.0)
('test loss', 1.1629243)
('test Acc:', 0.79131395)
model_080 F1_score: 82.84% >>>
380
('Epoch Training Loss:', 0.007257933208165923)
('Epoch Training Acc:', 1.0)
('test loss', 1.1594838)
('test Acc:', 0.7894737)
model_080 F1_score: 82.78% >>>
381
('Epoch Training Loss:', 0.007686255654334673)
('Epoch Training Acc:', 1.0)
('test loss', 1.1517005)
('test Acc:', 0.791682)
model_080 F1_score: 82.90% >>>
382
('Epoch Training Loss:', 0.0075085189109813655)
('Epoch Training Acc:', 1.0)
('test loss', 1.16454)
('test Acc:', 0.7927862)
model_080 F1_score: 83.11% >>>
383
('Epoch Training Loss:', 0.00723312804075249)
('Epoch Training Acc:', 1.0)
('test loss', 1.1613172)
('test Acc:', 0.79536253)
model_080 F1_score: 83.22% >>>
384
('Epoch Training Loss:', 0.0074352089413878275)
('Epoch Training Acc:', 1.0)
('test loss', 1.1653637)
('test Acc:', 0.7902098)
model_080 F1_score: 82.85% >>>
385
('Epoch Training Loss:', 0.007307675354240928)
('Epoch Training Acc:', 1.0)
('test loss', 1.1551534)
('test Acc:', 0.78910565)
model_080 F1_score: 82.92% >>>
386
('Epoch Training Loss:', 0.007448190248396713)
('Epoch Training Acc:', 1.0)
('test loss', 1.1477762)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
387
('Epoch Training Loss:', 0.0073739463496167446)
('Epoch Training Acc:', 1.0)
('test loss', 1.1671627)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
388
('Epoch Training Loss:', 0.007254072323121363)
('Epoch Training Acc:', 1.0)
('test loss', 1.1521033)
('test Acc:', 0.79205006)
model_080 F1_score: 83.02% >>>
389
('Epoch Training Loss:', 0.00724126810200687)
('Epoch Training Acc:', 1.0)
('test loss', 1.1475604)
('test Acc:', 0.79830694)
model_080 F1_score: 83.60% >>>
390
('Epoch Training Loss:', 0.007215470652226941)
('Epoch Training Acc:', 1.0)
('test loss', 1.1533506)
('test Acc:', 0.79352224)
model_080 F1_score: 83.19% >>>
391
('Epoch Training Loss:', 0.006722838190398761)
('Epoch Training Acc:', 1.0)
('test loss', 1.1641043)
('test Acc:', 0.79205006)
model_080 F1_score: 83.07% >>>
392
('Epoch Training Loss:', 0.0073370798763789935)
('Epoch Training Acc:', 1.0)
('test loss', 1.155598)
('test Acc:', 0.79352224)
model_080 F1_score: 83.14% >>>
393
('Epoch Training Loss:', 0.0072239647452079225)
('Epoch Training Acc:', 1.0)
('test loss', 1.178178)
('test Acc:', 0.791682)
model_080 F1_score: 83.12% >>>
394
('Epoch Training Loss:', 0.007235544881041278)
('Epoch Training Acc:', 1.0)
('test loss', 1.166117)
('test Acc:', 0.7957306)
model_080 F1_score: 83.24% >>>
395
('Epoch Training Loss:', 0.0072028341819532216)
('Epoch Training Acc:', 1.0)
('test loss', 1.1630031)
('test Acc:', 0.79536253)
model_080 F1_score: 83.34% >>>
396
('Epoch Training Loss:', 0.007094730557582807)
('Epoch Training Acc:', 1.0)
('test loss', 1.1825684)
('test Acc:', 0.7909459)
model_080 F1_score: 82.81% >>>
397
('Epoch Training Loss:', 0.006985842521316954)
('Epoch Training Acc:', 1.0)
('test loss', 1.1827114)
('test Acc:', 0.791682)
model_080 F1_score: 82.95% >>>
398
('Epoch Training Loss:', 0.006948360740352655)
('Epoch Training Acc:', 1.0)
('test loss', 1.155978)
('test Acc:', 0.79425836)
model_080 F1_score: 83.21% >>>
399
('Epoch Training Loss:', 0.007362280963207013)
('Epoch Training Acc:', 1.0)
('test loss', 1.153567)
('test Acc:', 0.7924181)
model_080 F1_score: 83.04% >>>
400
('Epoch Training Loss:', 0.0068440418854152085)
('Epoch Training Acc:', 1.0)
('test loss', 1.1580949)
('test Acc:', 0.79315424)
model_080 F1_score: 83.06% >>>
401
('Epoch Training Loss:', 0.007121184047718998)
('Epoch Training Acc:', 1.0)
('test loss', 1.169055)
('test Acc:', 0.79315424)
model_080 F1_score: 83.00% >>>
402
('Epoch Training Loss:', 0.007499757603000035)
('Epoch Training Acc:', 1.0)
('test loss', 1.1519487)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
403
('Epoch Training Loss:', 0.007005057210335508)
('Epoch Training Acc:', 1.0)
('test loss', 1.1649721)
('test Acc:', 0.7964667)
model_080 F1_score: 83.34% >>>
404
('Epoch Training Loss:', 0.006958681642572628)
('Epoch Training Acc:', 1.0)
('test loss', 1.1795818)
('test Acc:', 0.7909459)
model_080 F1_score: 82.95% >>>
405
('Epoch Training Loss:', 0.007023321442829911)
('Epoch Training Acc:', 1.0)
('test loss', 1.1640495)
('test Acc:', 0.79131395)
model_080 F1_score: 82.96% >>>
406
('Epoch Training Loss:', 0.006894767486301134)
('Epoch Training Acc:', 1.0)
('test loss', 1.1590374)
('test Acc:', 0.7946264)
model_080 F1_score: 83.33% >>>
407
('Epoch Training Loss:', 0.007218902432214236)
('Epoch Training Acc:', 1.0)
('test loss', 1.1724478)
('test Acc:', 0.7868973)
model_080 F1_score: 82.61% >>>
408
('Epoch Training Loss:', 0.007101455837073445)
('Epoch Training Acc:', 1.0)
('test loss', 1.1611855)
('test Acc:', 0.7909459)
model_080 F1_score: 82.99% >>>
409
('Epoch Training Loss:', 0.006983426901570056)
('Epoch Training Acc:', 1.0)
('test loss', 1.1632068)
('test Acc:', 0.7938903)
model_080 F1_score: 83.09% >>>
410
('Epoch Training Loss:', 0.006974832895139116)
('Epoch Training Acc:', 1.0)
('test loss', 1.1557521)
('test Acc:', 0.7894737)
model_080 F1_score: 82.89% >>>
411
('Epoch Training Loss:', 0.007003155400525429)
('Epoch Training Acc:', 1.0)
('test loss', 1.152949)
('test Acc:', 0.79205006)
model_080 F1_score: 82.97% >>>
412
('Epoch Training Loss:', 0.00707906700699823)
('Epoch Training Acc:', 1.0)
('test loss', 1.1648104)
('test Acc:', 0.7997792)
model_080 F1_score: 83.60% >>>
413
('Epoch Training Loss:', 0.006824840060289716)
('Epoch Training Acc:', 1.0)
('test loss', 1.1488096)
('test Acc:', 0.7946264)
model_080 F1_score: 83.06% >>>
414
('Epoch Training Loss:', 0.006702556234813528)
('Epoch Training Acc:', 1.0)
('test loss', 1.1803942)
('test Acc:', 0.7905778)
model_080 F1_score: 82.89% >>>
415
('Epoch Training Loss:', 0.006754726169674541)
('Epoch Training Acc:', 1.0)
('test loss', 1.1706879)
('test Acc:', 0.791682)
model_080 F1_score: 82.88% >>>
416
('Epoch Training Loss:', 0.006680836948362412)
('Epoch Training Acc:', 1.0)
('test loss', 1.1677582)
('test Acc:', 0.7946264)
model_080 F1_score: 83.23% >>>
417
('Epoch Training Loss:', 0.006746010441929684)
('Epoch Training Acc:', 1.0)
('test loss', 1.1647762)
('test Acc:', 0.7902098)
model_080 F1_score: 82.75% >>>
418
('Epoch Training Loss:', 0.006979186169701279)
('Epoch Training Acc:', 1.0)
('test loss', 1.1684437)
('test Acc:', 0.79425836)
model_080 F1_score: 83.11% >>>
419
('Epoch Training Loss:', 0.006798738235374913)
('Epoch Training Acc:', 1.0)
('test loss', 1.1661806)
('test Acc:', 0.79425836)
model_080 F1_score: 83.29% >>>
420
('Epoch Training Loss:', 0.006567037062268355)
('Epoch Training Acc:', 1.0)
('test loss', 1.1757282)
('test Acc:', 0.7876334)
model_080 F1_score: 82.58% >>>
421
('Epoch Training Loss:', 0.006492464777693385)
('Epoch Training Acc:', 1.0)
('test loss', 1.1695648)
('test Acc:', 0.79536253)
model_080 F1_score: 83.05% >>>
422
('Epoch Training Loss:', 0.006766537975636311)
('Epoch Training Acc:', 1.0)
('test loss', 1.1844621)
('test Acc:', 0.7909459)
model_080 F1_score: 83.07% >>>
423
('Epoch Training Loss:', 0.006811995133830351)
('Epoch Training Acc:', 1.0)
('test loss', 1.1695087)
('test Acc:', 0.79904306)
model_080 F1_score: 83.56% >>>
424
('Epoch Training Loss:', 0.006516229748740443)
('Epoch Training Acc:', 1.0)
('test loss', 1.1596105)
('test Acc:', 0.79131395)
model_080 F1_score: 82.96% >>>
425
('Epoch Training Loss:', 0.006718798211295507)
('Epoch Training Acc:', 1.0)
('test loss', 1.1686242)
('test Acc:', 0.79131395)
model_080 F1_score: 82.78% >>>
426
('Epoch Training Loss:', 0.0070242948222585255)
('Epoch Training Acc:', 1.0)
('test loss', 1.1751735)
('test Acc:', 0.78505707)
model_080 F1_score: 82.55% >>>
427
('Epoch Training Loss:', 0.00669048227427993)
('Epoch Training Acc:', 1.0)
('test loss', 1.1752537)
('test Acc:', 0.7964667)
model_080 F1_score: 83.61% >>>
428
('Epoch Training Loss:', 0.00694613595806004)
('Epoch Training Acc:', 1.0)
('test loss', 1.166303)
('test Acc:', 0.7994111)
model_080 F1_score: 83.58% >>>
429
('Epoch Training Loss:', 0.006640927918851958)
('Epoch Training Acc:', 1.0)
('test loss', 1.1634581)
('test Acc:', 0.79830694)
model_080 F1_score: 83.28% >>>
430
('Epoch Training Loss:', 0.006479355645751639)
('Epoch Training Acc:', 1.0)
('test loss', 1.1798395)
('test Acc:', 0.791682)
model_080 F1_score: 83.05% >>>
431
('Epoch Training Loss:', 0.006361001729601412)
('Epoch Training Acc:', 1.0)
('test loss', 1.1698864)
('test Acc:', 0.7887376)
model_080 F1_score: 82.70% >>>
432
('Epoch Training Loss:', 0.006255372913074098)
('Epoch Training Acc:', 1.0)
('test loss', 1.1667117)
('test Acc:', 0.7887376)
model_080 F1_score: 82.79% >>>
433
('Epoch Training Loss:', 0.006390755648681079)
('Epoch Training Acc:', 1.0)
('test loss', 1.179705)
('test Acc:', 0.7909459)
model_080 F1_score: 82.87% >>>
434
('Epoch Training Loss:', 0.006329201696644304)
('Epoch Training Acc:', 1.0)
('test loss', 1.1588867)
('test Acc:', 0.7909459)
model_080 F1_score: 83.01% >>>
435
('Epoch Training Loss:', 0.006546867110955645)
('Epoch Training Acc:', 1.0)
('test loss', 1.1859945)
('test Acc:', 0.7949945)
model_080 F1_score: 83.30% >>>
436
('Epoch Training Loss:', 0.006496502157460782)
('Epoch Training Acc:', 1.0)
('test loss', 1.1610386)
('test Acc:', 0.78836954)
model_080 F1_score: 82.89% >>>
437
('Epoch Training Loss:', 0.006470577458458138)
('Epoch Training Acc:', 1.0)
('test loss', 1.1526297)
('test Acc:', 0.7905778)
model_080 F1_score: 82.92% >>>
438
('Epoch Training Loss:', 0.006222894913662458)
('Epoch Training Acc:', 1.0)
('test loss', 1.1709743)
('test Acc:', 0.7898417)
model_080 F1_score: 82.83% >>>
439
('Epoch Training Loss:', 0.006371510747158027)
('Epoch Training Acc:', 1.0)
('test loss', 1.18073)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
440
('Epoch Training Loss:', 0.006497901120383176)
('Epoch Training Acc:', 1.0)
('test loss', 1.1752558)
('test Acc:', 0.79131395)
model_080 F1_score: 82.99% >>>
441
('Epoch Training Loss:', 0.0064556693760096096)
('Epoch Training Acc:', 1.0)
('test loss', 1.152379)
('test Acc:', 0.7938903)
model_080 F1_score: 83.25% >>>
442
('Epoch Training Loss:', 0.006222901774890488)
('Epoch Training Acc:', 1.0)
('test loss', 1.1770682)
('test Acc:', 0.7927862)
model_080 F1_score: 83.07% >>>
443
('Epoch Training Loss:', 0.0064364848331024405)
('Epoch Training Acc:', 1.0)
('test loss', 1.1750013)
('test Acc:', 0.7898417)
model_080 F1_score: 82.89% >>>
444
('Epoch Training Loss:', 0.00662385606119642)
('Epoch Training Acc:', 1.0)
('test loss', 1.1815581)
('test Acc:', 0.7938903)
model_080 F1_score: 83.22% >>>
445
('Epoch Training Loss:', 0.006208085998878232)
('Epoch Training Acc:', 1.0)
('test loss', 1.1678978)
('test Acc:', 0.78836954)
model_080 F1_score: 82.92% >>>
446
('Epoch Training Loss:', 0.006624490641115699)
('Epoch Training Acc:', 1.0)
('test loss', 1.1788551)
('test Acc:', 0.7876334)
model_080 F1_score: 82.73% >>>
447
('Epoch Training Loss:', 0.006279085595451761)
('Epoch Training Acc:', 1.0)
('test loss', 1.1790714)
('test Acc:', 0.78836954)
model_080 F1_score: 82.73% >>>
448
('Epoch Training Loss:', 0.006240549158974318)
('Epoch Training Acc:', 1.0)
('test loss', 1.185992)
('test Acc:', 0.7924181)
model_080 F1_score: 82.78% >>>
449
('Epoch Training Loss:', 0.00620312769751763)
('Epoch Training Acc:', 1.0)
('test loss', 1.1800128)
('test Acc:', 0.7924181)
model_080 F1_score: 83.05% >>>
450
('Epoch Training Loss:', 0.006026508200193348)
('Epoch Training Acc:', 1.0)
('test loss', 1.1822382)
('test Acc:', 0.7902098)
model_080 F1_score: 83.01% >>>
451
('Epoch Training Loss:', 0.006371781712005031)
('Epoch Training Acc:', 1.0)
('test loss', 1.1655607)
('test Acc:', 0.7927862)
model_080 F1_score: 82.84% >>>
452
('Epoch Training Loss:', 0.006270563750149449)
('Epoch Training Acc:', 1.0)
('test loss', 1.1765971)
('test Acc:', 0.7902098)
model_080 F1_score: 82.77% >>>
453
('Epoch Training Loss:', 0.006157990323117701)
('Epoch Training Acc:', 1.0)
('test loss', 1.1685104)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
454
('Epoch Training Loss:', 0.006342759964354627)
('Epoch Training Acc:', 1.0)
('test loss', 1.1637954)
('test Acc:', 0.79352224)
model_080 F1_score: 83.05% >>>
455
('Epoch Training Loss:', 0.006149201806692872)
('Epoch Training Acc:', 1.0)
('test loss', 1.1620475)
('test Acc:', 0.7957306)
model_080 F1_score: 83.19% >>>
456
('Epoch Training Loss:', 0.006039319612682448)
('Epoch Training Acc:', 1.0)
('test loss', 1.1597618)
('test Acc:', 0.79205006)
model_080 F1_score: 82.99% >>>
457
('Epoch Training Loss:', 0.006255232216972217)
('Epoch Training Acc:', 1.0)
('test loss', 1.1887233)
('test Acc:', 0.79131395)
model_080 F1_score: 83.04% >>>
458
('Epoch Training Loss:', 0.005994761809233751)
('Epoch Training Acc:', 1.0)
('test loss', 1.1858697)
('test Acc:', 0.7902098)
model_080 F1_score: 82.83% >>>
459
('Epoch Training Loss:', 0.00626760309569363)
('Epoch Training Acc:', 1.0)
('test loss', 1.178246)
('test Acc:', 0.7909459)
model_080 F1_score: 82.93% >>>
460
('Epoch Training Loss:', 0.006009670888488472)
('Epoch Training Acc:', 1.0)
('test loss', 1.1846182)
('test Acc:', 0.79352224)
model_080 F1_score: 83.01% >>>
461
('Epoch Training Loss:', 0.006181702605317696)
('Epoch Training Acc:', 1.0)
('test loss', 1.1741902)
('test Acc:', 0.7927862)
model_080 F1_score: 83.09% >>>
462
('Epoch Training Loss:', 0.006285807445237879)
('Epoch Training Acc:', 1.0)
('test loss', 1.1706452)
('test Acc:', 0.7924181)
model_080 F1_score: 83.02% >>>
463
('Epoch Training Loss:', 0.006244407443773525)
('Epoch Training Acc:', 1.0)
('test loss', 1.1692978)
('test Acc:', 0.7876334)
model_080 F1_score: 82.55% >>>
464
('Epoch Training Loss:', 0.006163612144518993)
('Epoch Training Acc:', 1.0)
('test loss', 1.178613)
('test Acc:', 0.7924181)
model_080 F1_score: 83.02% >>>
465
('Epoch Training Loss:', 0.006032642046193359)
('Epoch Training Acc:', 1.0)
('test loss', 1.164826)
('test Acc:', 0.79315424)
model_080 F1_score: 83.33% >>>
466
('Epoch Training Loss:', 0.006042948979484208)
('Epoch Training Acc:', 1.0)
('test loss', 1.1733752)
('test Acc:', 0.7938903)
model_080 F1_score: 83.18% >>>
467
('Epoch Training Loss:', 0.005854240915141418)
('Epoch Training Acc:', 1.0)
('test loss', 1.1740794)
('test Acc:', 0.79205006)
model_080 F1_score: 82.97% >>>
468
('Epoch Training Loss:', 0.006024056950991508)
('Epoch Training Acc:', 1.0)
('test loss', 1.1832988)
('test Acc:', 0.7924181)
model_080 F1_score: 83.01% >>>
469
('Epoch Training Loss:', 0.005865162922418676)
('Epoch Training Acc:', 1.0)
('test loss', 1.1926059)
('test Acc:', 0.7898417)
model_080 F1_score: 82.84% >>>
470
('Epoch Training Loss:', 0.005815008408717404)
('Epoch Training Acc:', 1.0)
('test loss', 1.190003)
('test Acc:', 0.79352224)
model_080 F1_score: 83.23% >>>
471
('Epoch Training Loss:', 0.006031774589246197)
('Epoch Training Acc:', 1.0)
('test loss', 1.1838315)
('test Acc:', 0.7902098)
model_080 F1_score: 82.96% >>>
472
('Epoch Training Loss:', 0.006014944102389563)
('Epoch Training Acc:', 1.0)
('test loss', 1.1620779)
('test Acc:', 0.79352224)
model_080 F1_score: 83.19% >>>
473
('Epoch Training Loss:', 0.005710736440960318)
('Epoch Training Acc:', 1.0)
('test loss', 1.1721969)
('test Acc:', 0.7868973)
model_080 F1_score: 82.66% >>>
474
('Epoch Training Loss:', 0.005722037980376626)
('Epoch Training Acc:', 1.0)
('test loss', 1.1712437)
('test Acc:', 0.79315424)
model_080 F1_score: 83.10% >>>
475
('Epoch Training Loss:', 0.005997562640004617)
('Epoch Training Acc:', 1.0)
('test loss', 1.1813046)
('test Acc:', 0.79205006)
model_080 F1_score: 82.85% >>>
476
('Epoch Training Loss:', 0.005995173944029375)
('Epoch Training Acc:', 1.0)
('test loss', 1.182073)
('test Acc:', 0.79315424)
model_080 F1_score: 82.99% >>>
477
('Epoch Training Loss:', 0.005656210667439154)
('Epoch Training Acc:', 1.0)
('test loss', 1.1690333)
('test Acc:', 0.7924181)
model_080 F1_score: 83.04% >>>
478
('Epoch Training Loss:', 0.005917887629038887)
('Epoch Training Acc:', 1.0)
('test loss', 1.1711518)
('test Acc:', 0.7957306)
model_080 F1_score: 83.33% >>>
479
('Epoch Training Loss:', 0.005915246401855256)
('Epoch Training Acc:', 1.0)
('test loss', 1.174693)
('test Acc:', 0.7894737)
model_080 F1_score: 82.84% >>>
480
('Epoch Training Loss:', 0.005761368322055205)
('Epoch Training Acc:', 1.0)
('test loss', 1.152199)
('test Acc:', 0.79425836)
model_080 F1_score: 83.32% >>>
481
('Epoch Training Loss:', 0.005707876869564643)
('Epoch Training Acc:', 1.0)
('test loss', 1.1696601)
('test Acc:', 0.7957306)
model_080 F1_score: 83.37% >>>
482
('Epoch Training Loss:', 0.005604078583019145)
('Epoch Training Acc:', 1.0)
('test loss', 1.1750801)
('test Acc:', 0.7905778)
model_080 F1_score: 82.91% >>>
483
('Epoch Training Loss:', 0.0056611328036524355)
('Epoch Training Acc:', 1.0)
('test loss', 1.1882774)
('test Acc:', 0.7927862)
model_080 F1_score: 83.18% >>>
484
('Epoch Training Loss:', 0.005472988606015861)
('Epoch Training Acc:', 1.0)
('test loss', 1.193446)
('test Acc:', 0.791682)
model_080 F1_score: 83.06% >>>
485
('Epoch Training Loss:', 0.005716681045669247)
('Epoch Training Acc:', 1.0)
('test loss', 1.1795192)
('test Acc:', 0.7924181)
model_080 F1_score: 83.08% >>>
486
('Epoch Training Loss:', 0.00584527682440239)
('Epoch Training Acc:', 1.0)
('test loss', 1.1679975)
('test Acc:', 0.7924181)
model_080 F1_score: 82.98% >>>
487
('Epoch Training Loss:', 0.006004323719025706)
('Epoch Training Acc:', 1.0)
('test loss', 1.1809781)
('test Acc:', 0.7957306)
model_080 F1_score: 83.42% >>>
488
('Epoch Training Loss:', 0.005752162211138057)
('Epoch Training Acc:', 1.0)
('test loss', 1.166988)
('test Acc:', 0.7905778)
model_080 F1_score: 82.80% >>>
489
('Epoch Training Loss:', 0.005802680216220324)
('Epoch Training Acc:', 1.0)
('test loss', 1.1655887)
('test Acc:', 0.79131395)
model_080 F1_score: 82.89% >>>
490
('Epoch Training Loss:', 0.005780914107162971)
('Epoch Training Acc:', 1.0)
('test loss', 1.1893818)
('test Acc:', 0.7887376)
model_080 F1_score: 82.90% >>>
491
('Epoch Training Loss:', 0.005604301024504821)
('Epoch Training Acc:', 1.0)
('test loss', 1.1894916)
('test Acc:', 0.7909459)
model_080 F1_score: 83.09% >>>
492
('Epoch Training Loss:', 0.005991338937747059)
('Epoch Training Acc:', 1.0)
('test loss', 1.1746308)
('test Acc:', 0.79425836)
model_080 F1_score: 83.63% >>>
493
('Epoch Training Loss:', 0.005898996404539503)
('Epoch Training Acc:', 1.0)
('test loss', 1.1907619)
('test Acc:', 0.7887376)
model_080 F1_score: 82.72% >>>
494
('Epoch Training Loss:', 0.005750400066972361)
('Epoch Training Acc:', 1.0)
('test loss', 1.2023157)
('test Acc:', 0.78836954)
model_080 F1_score: 82.78% >>>
495
('Epoch Training Loss:', 0.005672817160302657)
('Epoch Training Acc:', 1.0)
('test loss', 1.1876049)
('test Acc:', 0.79425836)
model_080 F1_score: 83.39% >>>
496
('Epoch Training Loss:', 0.005559249490033835)
('Epoch Training Acc:', 1.0)
('test loss', 1.1965925)
('test Acc:', 0.79536253)
model_080 F1_score: 83.33% >>>
497
('Epoch Training Loss:', 0.0055941396349226125)
('Epoch Training Acc:', 1.0)
('test loss', 1.1852463)
('test Acc:', 0.7938903)
model_080 F1_score: 83.25% >>>
498
('Epoch Training Loss:', 0.005450708090393164)
('Epoch Training Acc:', 1.0)
('test loss', 1.1804972)
('test Acc:', 0.7861612)
model_080 F1_score: 82.49% >>>
499
('Epoch Training Loss:', 0.005477340791003371)
('Epoch Training Acc:', 1.0)
('test loss', 1.1977113)
('test Acc:', 0.7898417)
model_080 F1_score: 82.80% >>>
500
('Epoch Training Loss:', 0.005389515945353196)
('Epoch Training Acc:', 1.0)
('test loss', 1.1710653)
('test Acc:', 0.79131395)
model_080 F1_score: 83.02% >>>
501
('Epoch Training Loss:', 0.0055994675121837645)
('Epoch Training Acc:', 1.0)
('test loss', 1.1880203)
('test Acc:', 0.7898417)
model_080 F1_score: 82.80% >>>
502
('Epoch Training Loss:', 0.005531831011467148)
('Epoch Training Acc:', 1.0)
('test loss', 1.183626)
('test Acc:', 0.7946264)
model_080 F1_score: 83.50% >>>
503
('Epoch Training Loss:', 0.005353963108063908)
('Epoch Training Acc:', 1.0)
('test loss', 1.192789)
('test Acc:', 0.791682)
model_080 F1_score: 82.88% >>>
504
('Epoch Training Loss:', 0.005724244480916241)
('Epoch Training Acc:', 1.0)
('test loss', 1.1898352)
('test Acc:', 0.79352224)
model_080 F1_score: 83.20% >>>
505
('Epoch Training Loss:', 0.005497362175447051)
('Epoch Training Acc:', 1.0)
('test loss', 1.2013078)
('test Acc:', 0.7909459)
model_080 F1_score: 82.91% >>>
506
('Epoch Training Loss:', 0.0053099044589544064)
('Epoch Training Acc:', 1.0)
('test loss', 1.1836733)
('test Acc:', 0.79315424)
model_080 F1_score: 82.95% >>>
507
('Epoch Training Loss:', 0.005570901857936406)
('Epoch Training Acc:', 1.0)
('test loss', 1.1846769)
('test Acc:', 0.79315424)
model_080 F1_score: 83.27% >>>
508
('Epoch Training Loss:', 0.005600145669632184)
('Epoch Training Acc:', 1.0)
('test loss', 1.197219)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
509
('Epoch Training Loss:', 0.0051852367287210654)
('Epoch Training Acc:', 1.0)
('test loss', 1.1812806)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
510
('Epoch Training Loss:', 0.005848974171385635)
('Epoch Training Acc:', 1.0)
('test loss', 1.1831354)
('test Acc:', 0.79315424)
model_080 F1_score: 83.02% >>>
511
('Epoch Training Loss:', 0.005476602980706957)
('Epoch Training Acc:', 1.0)
('test loss', 1.1800373)
('test Acc:', 0.7902098)
model_080 F1_score: 82.93% >>>
512
('Epoch Training Loss:', 0.005307826766511425)
('Epoch Training Acc:', 1.0)
('test loss', 1.2023654)
('test Acc:', 0.7887376)
model_080 F1_score: 82.89% >>>
513
('Epoch Training Loss:', 0.005234679934801534)
('Epoch Training Acc:', 1.0)
('test loss', 1.1927648)
('test Acc:', 0.7909459)
model_080 F1_score: 82.91% >>>
514
('Epoch Training Loss:', 0.005170595346498885)
('Epoch Training Acc:', 1.0)
('test loss', 1.1858797)
('test Acc:', 0.79315424)
model_080 F1_score: 83.19% >>>
515
('Epoch Training Loss:', 0.005696851376342238)
('Epoch Training Acc:', 1.0)
('test loss', 1.1883607)
('test Acc:', 0.7898417)
model_080 F1_score: 82.76% >>>
516
('Epoch Training Loss:', 0.005404899390669016)
('Epoch Training Acc:', 1.0)
('test loss', 1.1919502)
('test Acc:', 0.7905778)
model_080 F1_score: 82.94% >>>
517
('Epoch Training Loss:', 0.005238670210928831)
('Epoch Training Acc:', 1.0)
('test loss', 1.1772854)
('test Acc:', 0.78910565)
model_080 F1_score: 82.79% >>>
518
('Epoch Training Loss:', 0.005517711305401463)
('Epoch Training Acc:', 1.0)
('test loss', 1.1880252)
('test Acc:', 0.7972028)
model_080 F1_score: 83.46% >>>
519
('Epoch Training Loss:', 0.005348382569536625)
('Epoch Training Acc:', 1.0)
('test loss', 1.1810676)
('test Acc:', 0.79352224)
model_080 F1_score: 83.16% >>>
520
('Epoch Training Loss:', 0.005376062748837285)
('Epoch Training Acc:', 1.0)
('test loss', 1.1828315)
('test Acc:', 0.79536253)
model_080 F1_score: 83.43% >>>
521
('Epoch Training Loss:', 0.005530494747290504)
('Epoch Training Acc:', 1.0)
('test loss', 1.1875048)
('test Acc:', 0.7946264)
model_080 F1_score: 83.23% >>>
522
('Epoch Training Loss:', 0.005277084143926913)
('Epoch Training Acc:', 1.0)
('test loss', 1.2023757)
('test Acc:', 0.7876334)
model_080 F1_score: 82.80% >>>
523
('Epoch Training Loss:', 0.00510716930966737)
('Epoch Training Acc:', 1.0)
('test loss', 1.1885327)
('test Acc:', 0.798675)
model_080 F1_score: 83.71% >>>
524
('Epoch Training Loss:', 0.005296946809721703)
('Epoch Training Acc:', 1.0)
('test loss', 1.1878945)
('test Acc:', 0.79131395)
model_080 F1_score: 82.91% >>>
525
('Epoch Training Loss:', 0.0050373253479847335)
('Epoch Training Acc:', 1.0)
('test loss', 1.1789294)
('test Acc:', 0.7938903)
model_080 F1_score: 83.32% >>>
526
('Epoch Training Loss:', 0.005472973580253893)
('Epoch Training Acc:', 1.0)
('test loss', 1.1763861)
('test Acc:', 0.79536253)
model_080 F1_score: 83.44% >>>
527
('Epoch Training Loss:', 0.005372588135287515)
('Epoch Training Acc:', 1.0)
('test loss', 1.1725293)
('test Acc:', 0.7927862)
model_080 F1_score: 83.04% >>>
528
('Epoch Training Loss:', 0.0052137742177365)
('Epoch Training Acc:', 1.0)
('test loss', 1.1949)
('test Acc:', 0.7905778)
model_080 F1_score: 82.93% >>>
529
('Epoch Training Loss:', 0.00521462082178914)
('Epoch Training Acc:', 1.0)
('test loss', 1.1777834)
('test Acc:', 0.7924181)
model_080 F1_score: 82.98% >>>
530
('Epoch Training Loss:', 0.005143072716236929)
('Epoch Training Acc:', 1.0)
('test loss', 1.1808418)
('test Acc:', 0.7927862)
model_080 F1_score: 83.03% >>>
531
('Epoch Training Loss:', 0.005116955266203149)
('Epoch Training Acc:', 1.0)
('test loss', 1.1844419)
('test Acc:', 0.79352224)
model_080 F1_score: 83.15% >>>
532
('Epoch Training Loss:', 0.005149432538928522)
('Epoch Training Acc:', 1.0)
('test loss', 1.2031211)
('test Acc:', 0.7909459)
model_080 F1_score: 83.00% >>>
533
('Epoch Training Loss:', 0.005600110232990119)
('Epoch Training Acc:', 1.0)
('test loss', 1.2030679)
('test Acc:', 0.79425836)
model_080 F1_score: 83.38% >>>
534
('Epoch Training Loss:', 0.005772235458607611)
('Epoch Training Acc:', 1.0)
('test loss', 1.1954622)
('test Acc:', 0.79205006)
model_080 F1_score: 83.06% >>>
535
('Epoch Training Loss:', 0.005050360150562483)
('Epoch Training Acc:', 1.0)
('test loss', 1.1845067)
('test Acc:', 0.79425836)
model_080 F1_score: 83.36% >>>
536
('Epoch Training Loss:', 0.005173942245164653)
('Epoch Training Acc:', 1.0)
('test loss', 1.1955091)
('test Acc:', 0.7909459)
model_080 F1_score: 82.77% >>>
537
('Epoch Training Loss:', 0.005170854657990276)
('Epoch Training Acc:', 1.0)
('test loss', 1.1877633)
('test Acc:', 0.79352224)
model_080 F1_score: 83.26% >>>
538
('Epoch Training Loss:', 0.005024099050388031)
('Epoch Training Acc:', 1.0)
('test loss', 1.1826152)
('test Acc:', 0.7887376)
model_080 F1_score: 82.63% >>>
539
('Epoch Training Loss:', 0.005074053827229363)
('Epoch Training Acc:', 1.0)
('test loss', 1.1994078)
('test Acc:', 0.79425836)
model_080 F1_score: 83.20% >>>
540
('Epoch Training Loss:', 0.005257329907180974)
('Epoch Training Acc:', 1.0)
('test loss', 1.1845504)
('test Acc:', 0.7909459)
model_080 F1_score: 82.86% >>>
541
('Epoch Training Loss:', 0.005036812713115069)
('Epoch Training Acc:', 1.0)
('test loss', 1.201695)
('test Acc:', 0.791682)
model_080 F1_score: 83.20% >>>
542
('Epoch Training Loss:', 0.005055707724750391)
('Epoch Training Acc:', 1.0)
('test loss', 1.1825191)
('test Acc:', 0.7964667)
model_080 F1_score: 83.58% >>>
543
('Epoch Training Loss:', 0.005012865362004959)
('Epoch Training Acc:', 1.0)
('test loss', 1.176538)
('test Acc:', 0.7957306)
model_080 F1_score: 83.27% >>>
544
('Epoch Training Loss:', 0.005051434928645904)
('Epoch Training Acc:', 1.0)
('test loss', 1.1832564)
('test Acc:', 0.7975708)
model_080 F1_score: 83.52% >>>
545
('Epoch Training Loss:', 0.005209941011344199)
('Epoch Training Acc:', 1.0)
('test loss', 1.1901488)
('test Acc:', 0.7898417)
model_080 F1_score: 82.64% >>>
546
('Epoch Training Loss:', 0.005283716542180628)
('Epoch Training Acc:', 1.0)
('test loss', 1.2013532)
('test Acc:', 0.7927862)
model_080 F1_score: 83.25% >>>
547
('Epoch Training Loss:', 0.005101290278616943)
('Epoch Training Acc:', 1.0)
('test loss', 1.1952711)
('test Acc:', 0.791682)
model_080 F1_score: 83.02% >>>
548
('Epoch Training Loss:', 0.004944667442032369)
('Epoch Training Acc:', 1.0)
('test loss', 1.2039591)
('test Acc:', 0.791682)
model_080 F1_score: 82.87% >>>
549
('Epoch Training Loss:', 0.004959526376296708)
('Epoch Training Acc:', 1.0)
('test loss', 1.1851157)
('test Acc:', 0.791682)
model_080 F1_score: 83.03% >>>
550
('Epoch Training Loss:', 0.004943724035001651)
('Epoch Training Acc:', 1.0)
('test loss', 1.1984336)
('test Acc:', 0.7909459)
model_080 F1_score: 82.97% >>>
551
('Epoch Training Loss:', 0.0050186910448246635)
('Epoch Training Acc:', 1.0)
('test loss', 1.2029029)
('test Acc:', 0.791682)
model_080 F1_score: 82.84% >>>
552
('Epoch Training Loss:', 0.005080046006696648)
('Epoch Training Acc:', 1.0)
('test loss', 1.1996553)
('test Acc:', 0.7909459)
model_080 F1_score: 83.11% >>>
553
('Epoch Training Loss:', 0.0051682077728401055)
('Epoch Training Acc:', 1.0)
('test loss', 1.202698)
('test Acc:', 0.7902098)
model_080 F1_score: 82.78% >>>
554
('Epoch Training Loss:', 0.004857824946157052)
('Epoch Training Acc:', 1.0)
('test loss', 1.1826196)
('test Acc:', 0.791682)
model_080 F1_score: 82.89% >>>
555
('Epoch Training Loss:', 0.004884068849605683)
('Epoch Training Acc:', 1.0)
('test loss', 1.1893892)
('test Acc:', 0.79131395)
model_080 F1_score: 82.95% >>>
556
('Epoch Training Loss:', 0.005051409747466096)
('Epoch Training Acc:', 1.0)
('test loss', 1.1756648)
('test Acc:', 0.79425836)
model_080 F1_score: 83.19% >>>
557
('Epoch Training Loss:', 0.004794571380443813)
('Epoch Training Acc:', 1.0)
('test loss', 1.1879556)
('test Acc:', 0.7946264)
model_080 F1_score: 83.11% >>>
558
('Epoch Training Loss:', 0.004982676901818195)
('Epoch Training Acc:', 1.0)
('test loss', 1.1897631)
('test Acc:', 0.7927862)
model_080 F1_score: 83.05% >>>
559
('Epoch Training Loss:', 0.005023101726692403)
('Epoch Training Acc:', 1.0)
('test loss', 1.1967992)
('test Acc:', 0.79425836)
model_080 F1_score: 83.32% >>>
560
('Epoch Training Loss:', 0.005004917104088236)
('Epoch Training Acc:', 1.0)
('test loss', 1.2030845)
('test Acc:', 0.7898417)
model_080 F1_score: 82.87% >>>
561
('Epoch Training Loss:', 0.0048724665903137065)
('Epoch Training Acc:', 1.0)
('test loss', 1.2098067)
('test Acc:', 0.7909459)
model_080 F1_score: 82.88% >>>
562
('Epoch Training Loss:', 0.005032454280808452)
('Epoch Training Acc:', 1.0)
('test loss', 1.1855799)
('test Acc:', 0.7946264)
model_080 F1_score: 83.19% >>>
563
('Epoch Training Loss:', 0.004935096249937487)
('Epoch Training Acc:', 1.0)
('test loss', 1.1971325)
('test Acc:', 0.7902098)
model_080 F1_score: 82.94% >>>
564
('Epoch Training Loss:', 0.0049143010219268035)
('Epoch Training Acc:', 1.0)
('test loss', 1.2044351)
('test Acc:', 0.791682)
model_080 F1_score: 83.01% >>>
565
('Epoch Training Loss:', 0.004856158309848979)
('Epoch Training Acc:', 1.0)
('test loss', 1.1874144)
('test Acc:', 0.7924181)
model_080 F1_score: 82.98% >>>
566
('Epoch Training Loss:', 0.0047527872693535755)
('Epoch Training Acc:', 1.0)
('test loss', 1.2039672)
('test Acc:', 0.79131395)
model_080 F1_score: 83.02% >>>
567
('Epoch Training Loss:', 0.004851641366258264)
('Epoch Training Acc:', 1.0)
('test loss', 1.2054162)
('test Acc:', 0.7894737)
model_080 F1_score: 82.99% >>>
568
('Epoch Training Loss:', 0.004792537004504993)
('Epoch Training Acc:', 1.0)
('test loss', 1.2018812)
('test Acc:', 0.79131395)
model_080 F1_score: 83.10% >>>
569
('Epoch Training Loss:', 0.005052169130976836)
('Epoch Training Acc:', 1.0)
('test loss', 1.1919216)
('test Acc:', 0.79131395)
model_080 F1_score: 82.92% >>>
570
('Epoch Training Loss:', 0.004973372867425496)
('Epoch Training Acc:', 1.0)
('test loss', 1.2062496)
('test Acc:', 0.79315424)
model_080 F1_score: 83.22% >>>
571
('Epoch Training Loss:', 0.004996654535716516)
('Epoch Training Acc:', 1.0)
('test loss', 1.1988755)
('test Acc:', 0.7905778)
model_080 F1_score: 82.84% >>>
572
('Epoch Training Loss:', 0.0050287116682739)
('Epoch Training Acc:', 1.0)
('test loss', 1.2054259)
('test Acc:', 0.7905778)
model_080 F1_score: 82.71% >>>
573
('Epoch Training Loss:', 0.0046698283576915856)
('Epoch Training Acc:', 1.0)
('test loss', 1.2013489)
('test Acc:', 0.78836954)
model_080 F1_score: 82.84% >>>
574
('Epoch Training Loss:', 0.004807965273357695)
('Epoch Training Acc:', 1.0)
('test loss', 1.1948758)
('test Acc:', 0.78726536)
model_080 F1_score: 82.69% >>>
575
('Epoch Training Loss:', 0.004780906348969438)
('Epoch Training Acc:', 1.0)
('test loss', 1.1973749)
('test Acc:', 0.79205006)
model_080 F1_score: 82.91% >>>
576
('Epoch Training Loss:', 0.00500485306201881)
('Epoch Training Acc:', 1.0)
('test loss', 1.1999655)
('test Acc:', 0.7898417)
model_080 F1_score: 82.76% >>>
577
('Epoch Training Loss:', 0.004862114777097304)
('Epoch Training Acc:', 1.0)
('test loss', 1.1974013)
('test Acc:', 0.7957306)
model_080 F1_score: 83.39% >>>
578
('Epoch Training Loss:', 0.004740147131087724)
('Epoch Training Acc:', 1.0)
('test loss', 1.1848223)
('test Acc:', 0.7887376)
model_080 F1_score: 82.81% >>>
579
('Epoch Training Loss:', 0.004648759313568007)
('Epoch Training Acc:', 1.0)
('test loss', 1.1990485)
('test Acc:', 0.7938903)
model_080 F1_score: 83.39% >>>
580
('Epoch Training Loss:', 0.0046711335990039515)
('Epoch Training Acc:', 1.0)
('test loss', 1.2050034)
('test Acc:', 0.7946264)
model_080 F1_score: 83.26% >>>
581
('Epoch Training Loss:', 0.0047400378389284015)
('Epoch Training Acc:', 1.0)
('test loss', 1.1860331)
('test Acc:', 0.79315424)
model_080 F1_score: 83.28% >>>
582
('Epoch Training Loss:', 0.004951098386300146)
('Epoch Training Acc:', 1.0)
('test loss', 1.1893635)
('test Acc:', 0.7972028)
model_080 F1_score: 83.46% >>>
583
('Epoch Training Loss:', 0.004666594158152293)
('Epoch Training Acc:', 1.0)
('test loss', 1.1969424)
('test Acc:', 0.7946264)
model_080 F1_score: 83.25% >>>
584
('Epoch Training Loss:', 0.004640789249606314)
('Epoch Training Acc:', 1.0)
('test loss', 1.2149494)
('test Acc:', 0.79131395)
model_080 F1_score: 82.71% >>>
585
('Epoch Training Loss:', 0.004570743155454693)
('Epoch Training Acc:', 1.0)
('test loss', 1.1954356)
('test Acc:', 0.79425836)
model_080 F1_score: 83.12% >>>
586
('Epoch Training Loss:', 0.004631654401237029)
('Epoch Training Acc:', 1.0)
('test loss', 1.1852449)
('test Acc:', 0.7927862)
model_080 F1_score: 83.20% >>>
587
('Epoch Training Loss:', 0.004782664778758772)
('Epoch Training Acc:', 1.0)
('test loss', 1.1837618)
('test Acc:', 0.7957306)
model_080 F1_score: 83.42% >>>
588
('Epoch Training Loss:', 0.004664525565203803)
('Epoch Training Acc:', 1.0)
('test loss', 1.1990495)
('test Acc:', 0.7949945)
model_080 F1_score: 83.39% >>>
589
('Epoch Training Loss:', 0.004850102576710924)
('Epoch Training Acc:', 1.0)
('test loss', 1.2198708)
('test Acc:', 0.7898417)
model_080 F1_score: 82.71% >>>
590
('Epoch Training Loss:', 0.004605649269251444)
('Epoch Training Acc:', 1.0)
('test loss', 1.1928431)
('test Acc:', 0.7938903)
model_080 F1_score: 83.17% >>>
591
('Epoch Training Loss:', 0.004584013299790968)
('Epoch Training Acc:', 1.0)
('test loss', 1.1747589)
('test Acc:', 0.79315424)
model_080 F1_score: 83.02% >>>
592
('Epoch Training Loss:', 0.004769867246068316)
('Epoch Training Acc:', 1.0)
('test loss', 1.1955416)
('test Acc:', 0.7887376)
model_080 F1_score: 82.85% >>>
593
('Epoch Training Loss:', 0.0046771597562838)
('Epoch Training Acc:', 1.0)
('test loss', 1.2008014)
('test Acc:', 0.7909459)
model_080 F1_score: 82.87% >>>
594
('Epoch Training Loss:', 0.0046494474863720825)
('Epoch Training Acc:', 1.0)
('test loss', 1.187774)
('test Acc:', 0.79904306)
model_080 F1_score: 83.51% >>>
595
('Epoch Training Loss:', 0.004781560263836582)
('Epoch Training Acc:', 1.0)
('test loss', 1.1933609)
('test Acc:', 0.7949945)
model_080 F1_score: 83.50% >>>
596
('Epoch Training Loss:', 0.004479378931137035)
('Epoch Training Acc:', 1.0)
('test loss', 1.2042344)
('test Acc:', 0.79131395)
model_080 F1_score: 83.03% >>>
597
('Epoch Training Loss:', 0.004535369847872062)
('Epoch Training Acc:', 1.0)
('test loss', 1.2043422)
('test Acc:', 0.7909459)
model_080 F1_score: 82.80% >>>
598
('Epoch Training Loss:', 0.004567896430671681)
('Epoch Training Acc:', 1.0)
('test loss', 1.1980978)
('test Acc:', 0.791682)
model_080 F1_score: 83.00% >>>
599
('Epoch Training Loss:', 0.004798832197593583)
('Epoch Training Acc:', 1.0)
('test loss', 1.1988773)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
600
('Epoch Training Loss:', 0.0047601507812942145)
('Epoch Training Acc:', 1.0)
('test loss', 1.2035378)
('test Acc:', 0.7894737)
model_080 F1_score: 82.69% >>>
601
('Epoch Training Loss:', 0.004552081496512983)
('Epoch Training Acc:', 1.0)
('test loss', 1.2040448)
('test Acc:', 0.79205006)
model_080 F1_score: 83.08% >>>
602
('Epoch Training Loss:', 0.004612267523043556)
('Epoch Training Acc:', 1.0)
('test loss', 1.1979221)
('test Acc:', 0.79352224)
model_080 F1_score: 83.34% >>>
603
('Epoch Training Loss:', 0.004667639119361411)
('Epoch Training Acc:', 1.0)
('test loss', 1.1990281)
('test Acc:', 0.79131395)
model_080 F1_score: 82.95% >>>
604
('Epoch Training Loss:', 0.004459444212443486)
('Epoch Training Acc:', 1.0)
('test loss', 1.2099642)
('test Acc:', 0.7854251)
model_080 F1_score: 82.46% >>>
605
('Epoch Training Loss:', 0.004423923526701401)
('Epoch Training Acc:', 1.0)
('test loss', 1.208383)
('test Acc:', 0.7905778)
model_080 F1_score: 83.08% >>>
606
('Epoch Training Loss:', 0.004431690022101975)
('Epoch Training Acc:', 1.0)
('test loss', 1.1854528)
('test Acc:', 0.7972028)
model_080 F1_score: 83.50% >>>
607
('Epoch Training Loss:', 0.004433534270901873)
('Epoch Training Acc:', 1.0)
('test loss', 1.2141557)
('test Acc:', 0.7880015)
model_080 F1_score: 82.72% >>>
608
('Epoch Training Loss:', 0.00450855607050471)
('Epoch Training Acc:', 1.0)
('test loss', 1.1889553)
('test Acc:', 0.7927862)
model_080 F1_score: 83.02% >>>
609
('Epoch Training Loss:', 0.004552396258077351)
('Epoch Training Acc:', 1.0)
('test loss', 1.1984092)
('test Acc:', 0.7957306)
model_080 F1_score: 83.38% >>>
610
('Epoch Training Loss:', 0.004527216284259339)
('Epoch Training Acc:', 1.0)
('test loss', 1.2038572)
('test Acc:', 0.7909459)
model_080 F1_score: 82.90% >>>
611
('Epoch Training Loss:', 0.00456551845127251)
('Epoch Training Acc:', 1.0)
('test loss', 1.2115718)
('test Acc:', 0.79904306)
model_080 F1_score: 83.65% >>>
612
('Epoch Training Loss:', 0.004471387999728904)
('Epoch Training Acc:', 1.0)
('test loss', 1.1933284)
('test Acc:', 0.7927862)
model_080 F1_score: 83.24% >>>
613
('Epoch Training Loss:', 0.004514732489951712)
('Epoch Training Acc:', 1.0)
('test loss', 1.1941245)
('test Acc:', 0.7909459)
model_080 F1_score: 82.84% >>>
614
('Epoch Training Loss:', 0.004609854136106151)
('Epoch Training Acc:', 1.0)
('test loss', 1.2024949)
('test Acc:', 0.7898417)
model_080 F1_score: 82.94% >>>
615
('Epoch Training Loss:', 0.004410096436004096)
('Epoch Training Acc:', 1.0)
('test loss', 1.198746)
('test Acc:', 0.791682)
model_080 F1_score: 83.17% >>>
616
('Epoch Training Loss:', 0.004254803039657418)
('Epoch Training Acc:', 1.0)
('test loss', 1.2089761)
('test Acc:', 0.79315424)
model_080 F1_score: 83.23% >>>
617
('Epoch Training Loss:', 0.004346133234321314)
('Epoch Training Acc:', 1.0)
('test loss', 1.1996523)
('test Acc:', 0.7924181)
model_080 F1_score: 83.20% >>>
618
('Epoch Training Loss:', 0.0043971688364763395)
('Epoch Training Acc:', 1.0)
('test loss', 1.2164824)
('test Acc:', 0.79205006)
model_080 F1_score: 82.97% >>>
619
('Epoch Training Loss:', 0.004234627011101111)
('Epoch Training Acc:', 1.0)
('test loss', 1.2058606)
('test Acc:', 0.7972028)
model_080 F1_score: 83.33% >>>
620
('Epoch Training Loss:', 0.004263330532012333)
('Epoch Training Acc:', 1.0)
('test loss', 1.1868356)
('test Acc:', 0.8023555)
model_080 F1_score: 83.76% >>>
621
('Epoch Training Loss:', 0.004499048825891805)
('Epoch Training Acc:', 1.0)
('test loss', 1.1963966)
('test Acc:', 0.7909459)
model_080 F1_score: 83.15% >>>
622
('Epoch Training Loss:', 0.004666559305405826)
('Epoch Training Acc:', 1.0)
('test loss', 1.2087337)
('test Acc:', 0.79131395)
model_080 F1_score: 82.95% >>>
623
('Epoch Training Loss:', 0.0044043305088052875)
('Epoch Training Acc:', 1.0)
('test loss', 1.2330406)
('test Acc:', 0.7909459)
model_080 F1_score: 82.94% >>>
624
('Epoch Training Loss:', 0.004324071692280995)
('Epoch Training Acc:', 1.0)
('test loss', 1.2077645)
('test Acc:', 0.7887376)
model_080 F1_score: 82.80% >>>
625
('Epoch Training Loss:', 0.0041430912369833095)
('Epoch Training Acc:', 1.0)
('test loss', 1.1940752)
('test Acc:', 0.79683477)
model_080 F1_score: 83.41% >>>
626
('Epoch Training Loss:', 0.004342778544014436)
('Epoch Training Acc:', 1.0)
('test loss', 1.2123075)
('test Acc:', 0.7857931)
model_080 F1_score: 82.45% >>>
627
('Epoch Training Loss:', 0.004159562416134577)
('Epoch Training Acc:', 1.0)
('test loss', 1.1998107)
('test Acc:', 0.7894737)
model_080 F1_score: 82.71% >>>
628
('Epoch Training Loss:', 0.004200035940812086)
('Epoch Training Acc:', 1.0)
('test loss', 1.2021526)
('test Acc:', 0.79131395)
model_080 F1_score: 82.91% >>>
629
('Epoch Training Loss:', 0.0043545118442125386)
('Epoch Training Acc:', 1.0)
('test loss', 1.2050164)
('test Acc:', 0.7909459)
model_080 F1_score: 82.92% >>>
630
('Epoch Training Loss:', 0.004281340839042969)
('Epoch Training Acc:', 1.0)
('test loss', 1.2107973)
('test Acc:', 0.78910565)
model_080 F1_score: 82.71% >>>
631
('Epoch Training Loss:', 0.004210807813251449)
('Epoch Training Acc:', 1.0)
('test loss', 1.2148274)
('test Acc:', 0.7887376)
model_080 F1_score: 82.84% >>>
632
('Epoch Training Loss:', 0.004625627259883913)
('Epoch Training Acc:', 1.0)
('test loss', 1.2065523)
('test Acc:', 0.7868973)
model_080 F1_score: 82.55% >>>
633
('Epoch Training Loss:', 0.0041775482468437986)
('Epoch Training Acc:', 1.0)
('test loss', 1.2052611)
('test Acc:', 0.7927862)
model_080 F1_score: 83.09% >>>
634
('Epoch Training Loss:', 0.0043568243954723584)
('Epoch Training Acc:', 1.0)
('test loss', 1.2009951)
('test Acc:', 0.7909459)
model_080 F1_score: 83.07% >>>
635
('Epoch Training Loss:', 0.004146737919654697)
('Epoch Training Acc:', 1.0)
('test loss', 1.2169051)
('test Acc:', 0.78836954)
model_080 F1_score: 82.55% >>>
636
('Epoch Training Loss:', 0.004160821414188831)
('Epoch Training Acc:', 1.0)
('test loss', 1.1814976)
('test Acc:', 0.79352224)
model_080 F1_score: 83.20% >>>
637
('Epoch Training Loss:', 0.004120284959753917)
('Epoch Training Acc:', 1.0)
('test loss', 1.2007778)
('test Acc:', 0.7924181)
model_080 F1_score: 83.07% >>>
638
('Epoch Training Loss:', 0.004270309510502557)
('Epoch Training Acc:', 1.0)
('test loss', 1.2159125)
('test Acc:', 0.79315424)
model_080 F1_score: 82.98% >>>
639
('Epoch Training Loss:', 0.0042768619296111865)
('Epoch Training Acc:', 1.0)
('test loss', 1.2010049)
('test Acc:', 0.7927862)
model_080 F1_score: 83.25% >>>
640
('Epoch Training Loss:', 0.0040438267060380895)
('Epoch Training Acc:', 1.0)
('test loss', 1.179689)
('test Acc:', 0.79425836)
model_080 F1_score: 83.11% >>>
641
('Epoch Training Loss:', 0.004110748984203383)
('Epoch Training Acc:', 1.0)
('test loss', 1.223394)
('test Acc:', 0.791682)
model_080 F1_score: 82.99% >>>
642
('Epoch Training Loss:', 0.004353485500359966)
('Epoch Training Acc:', 1.0)
('test loss', 1.1838167)
('test Acc:', 0.79315424)
model_080 F1_score: 83.03% >>>
643
('Epoch Training Loss:', 0.004145865324971965)
('Epoch Training Acc:', 1.0)
('test loss', 1.2044213)
('test Acc:', 0.7994111)
model_080 F1_score: 83.43% >>>
644
('Epoch Training Loss:', 0.004226605297844799)
('Epoch Training Acc:', 1.0)
('test loss', 1.2025721)
('test Acc:', 0.78836954)
model_080 F1_score: 82.69% >>>
645
('Epoch Training Loss:', 0.004196577150651137)
('Epoch Training Acc:', 1.0)
('test loss', 1.2113262)
('test Acc:', 0.7854251)
model_080 F1_score: 82.45% >>>
646
('Epoch Training Loss:', 0.004152930279815337)
('Epoch Training Acc:', 1.0)
('test loss', 1.2231252)
('test Acc:', 0.7909459)
model_080 F1_score: 83.03% >>>
647
('Epoch Training Loss:', 0.004188224813333363)
('Epoch Training Acc:', 1.0)
('test loss', 1.1983248)
('test Acc:', 0.7949945)
model_080 F1_score: 83.14% >>>
648
('Epoch Training Loss:', 0.004236472266711644)
('Epoch Training Acc:', 1.0)
('test loss', 1.212852)
('test Acc:', 0.7927862)
model_080 F1_score: 83.21% >>>
649
('Epoch Training Loss:', 0.004170745897681627)
('Epoch Training Acc:', 1.0)
('test loss', 1.2141654)
('test Acc:', 0.791682)
model_080 F1_score: 82.90% >>>
650
('Epoch Training Loss:', 0.004396877084218431)
('Epoch Training Acc:', 1.0)
('test loss', 1.2080455)
('test Acc:', 0.791682)
model_080 F1_score: 82.98% >>>
651
('Epoch Training Loss:', 0.004260634377715178)
('Epoch Training Acc:', 1.0)
('test loss', 1.211832)
('test Acc:', 0.7898417)
model_080 F1_score: 82.82% >>>
652
('Epoch Training Loss:', 0.004086600120899675)
('Epoch Training Acc:', 1.0)
('test loss', 1.1846166)
('test Acc:', 0.79352224)
model_080 F1_score: 83.19% >>>
653
('Epoch Training Loss:', 0.004171396153651585)
('Epoch Training Acc:', 1.0)
('test loss', 1.2121716)
('test Acc:', 0.7949945)
model_080 F1_score: 83.18% >>>
654
('Epoch Training Loss:', 0.004084438010977465)
('Epoch Training Acc:', 1.0)
('test loss', 1.2193658)
('test Acc:', 0.7979389)
model_080 F1_score: 83.40% >>>
655
('Epoch Training Loss:', 0.004138859736485756)
('Epoch Training Acc:', 1.0)
('test loss', 1.1956505)
('test Acc:', 0.79205006)
model_080 F1_score: 83.18% >>>
656
('Epoch Training Loss:', 0.0040479473309460445)
('Epoch Training Acc:', 1.0)
('test loss', 1.1950996)
('test Acc:', 0.7927862)
model_080 F1_score: 83.04% >>>
657
('Epoch Training Loss:', 0.004314748255637824)
('Epoch Training Acc:', 1.0)
('test loss', 1.2062672)
('test Acc:', 0.7909459)
model_080 F1_score: 83.02% >>>
658
('Epoch Training Loss:', 0.004315012097322324)
('Epoch Training Acc:', 1.0)
('test loss', 1.2135414)
('test Acc:', 0.79205006)
model_080 F1_score: 82.97% >>>
659
('Epoch Training Loss:', 0.004246407038408506)
('Epoch Training Acc:', 1.0)
('test loss', 1.2189877)
('test Acc:', 0.7924181)
model_080 F1_score: 82.95% >>>
660
('Epoch Training Loss:', 0.0038812877392047085)
('Epoch Training Acc:', 1.0)
('test loss', 1.2011433)
('test Acc:', 0.7975708)
model_080 F1_score: 83.59% >>>
661
('Epoch Training Loss:', 0.004194415065285284)
('Epoch Training Acc:', 1.0)
('test loss', 1.2192079)
('test Acc:', 0.7994111)
model_080 F1_score: 83.87% >>>
662
('Epoch Training Loss:', 0.003967139396991115)
('Epoch Training Acc:', 1.0)
('test loss', 1.2117618)
('test Acc:', 0.7949945)
model_080 F1_score: 83.24% >>>
663
('Epoch Training Loss:', 0.004242405240802327)
('Epoch Training Acc:', 1.0)
('test loss', 1.2092729)
('test Acc:', 0.7887376)
model_080 F1_score: 82.77% >>>
664
('Epoch Training Loss:', 0.0039676041533311945)
('Epoch Training Acc:', 1.0)
('test loss', 1.227936)
('test Acc:', 0.7868973)
model_080 F1_score: 82.63% >>>
665
('Epoch Training Loss:', 0.004130238450670731)
('Epoch Training Acc:', 1.0)
('test loss', 1.2158747)
('test Acc:', 0.7909459)
model_080 F1_score: 82.87% >>>
666
('Epoch Training Loss:', 0.003872767778375419)
('Epoch Training Acc:', 1.0)
('test loss', 1.2028514)
('test Acc:', 0.7927862)
model_080 F1_score: 83.02% >>>
667
('Epoch Training Loss:', 0.004030113888802589)
('Epoch Training Acc:', 1.0)
('test loss', 1.210934)
('test Acc:', 0.79352224)
model_080 F1_score: 83.24% >>>
668
('Epoch Training Loss:', 0.003912531969945121)
('Epoch Training Acc:', 1.0)
('test loss', 1.2096392)
('test Acc:', 0.79315424)
model_080 F1_score: 83.13% >>>
669
('Epoch Training Loss:', 0.0040811476901581045)
('Epoch Training Acc:', 1.0)
('test loss', 1.2213503)
('test Acc:', 0.78910565)
model_080 F1_score: 82.78% >>>
670
('Epoch Training Loss:', 0.003988149303950195)
('Epoch Training Acc:', 1.0)
('test loss', 1.1977495)
('test Acc:', 0.79425836)
model_080 F1_score: 83.19% >>>
671
('Epoch Training Loss:', 0.004146055433011497)
('Epoch Training Acc:', 1.0)
('test loss', 1.2123721)
('test Acc:', 0.79131395)
model_080 F1_score: 83.11% >>>
672
('Epoch Training Loss:', 0.004006214698165422)
('Epoch Training Acc:', 1.0)
('test loss', 1.2100455)
('test Acc:', 0.7957306)
model_080 F1_score: 83.32% >>>
673
('Epoch Training Loss:', 0.004055058807352907)
('Epoch Training Acc:', 1.0)
('test loss', 1.214934)
('test Acc:', 0.78910565)
model_080 F1_score: 82.77% >>>
674
('Epoch Training Loss:', 0.004155911771704268)
('Epoch Training Acc:', 1.0)
('test loss', 1.2046583)
('test Acc:', 0.7949945)
model_080 F1_score: 83.31% >>>
675
('Epoch Training Loss:', 0.003854238130770682)
('Epoch Training Acc:', 1.0)
('test loss', 1.1910164)
('test Acc:', 0.7927862)
model_080 F1_score: 82.93% >>>
676
('Epoch Training Loss:', 0.0039211481180245755)
('Epoch Training Acc:', 1.0)
('test loss', 1.2101618)
('test Acc:', 0.7909459)
model_080 F1_score: 82.69% >>>
677
('Epoch Training Loss:', 0.003925638271539356)
('Epoch Training Acc:', 1.0)
('test loss', 1.21521)
('test Acc:', 0.78652924)
model_080 F1_score: 82.45% >>>
678
('Epoch Training Loss:', 0.00401027090538264)
('Epoch Training Acc:', 1.0)
('test loss', 1.2170792)
('test Acc:', 0.7924181)
model_080 F1_score: 83.02% >>>
679
('Epoch Training Loss:', 0.004067646354997123)
('Epoch Training Acc:', 1.0)
('test loss', 1.228045)
('test Acc:', 0.78836954)
model_080 F1_score: 82.76% >>>
680
('Epoch Training Loss:', 0.004037643666379154)
('Epoch Training Acc:', 1.0)
('test loss', 1.215053)
('test Acc:', 0.7924181)
model_080 F1_score: 83.11% >>>
681
('Epoch Training Loss:', 0.0038203413496376015)
('Epoch Training Acc:', 1.0)
('test loss', 1.2071398)
('test Acc:', 0.7902098)
model_080 F1_score: 82.88% >>>
682
('Epoch Training Loss:', 0.004198744502900809)
('Epoch Training Acc:', 1.0)
('test loss', 1.2150636)
('test Acc:', 0.7894737)
model_080 F1_score: 83.08% >>>
683
('Epoch Training Loss:', 0.004080895798324491)
('Epoch Training Acc:', 1.0)
('test loss', 1.2203095)
('test Acc:', 0.7880015)
model_080 F1_score: 82.83% >>>
684
('Epoch Training Loss:', 0.003998023894382641)
('Epoch Training Acc:', 1.0)
('test loss', 1.2151762)
('test Acc:', 0.7876334)
model_080 F1_score: 82.72% >>>
685
('Epoch Training Loss:', 0.0041441545417910675)
('Epoch Training Acc:', 1.0)
('test loss', 1.2181888)
('test Acc:', 0.79205006)
model_080 F1_score: 82.93% >>>
686
('Epoch Training Loss:', 0.004003649051810498)
('Epoch Training Acc:', 1.0)
('test loss', 1.224224)
('test Acc:', 0.79425836)
model_080 F1_score: 83.12% >>>
687
('Epoch Training Loss:', 0.003926309921553184)
('Epoch Training Acc:', 1.0)
('test loss', 1.2189739)
('test Acc:', 0.79425836)
model_080 F1_score: 83.26% >>>
688
('Epoch Training Loss:', 0.003911223005161446)
('Epoch Training Acc:', 1.0)
('test loss', 1.205087)
('test Acc:', 0.78836954)
model_080 F1_score: 82.72% >>>
689
('Epoch Training Loss:', 0.0037708071949964506)
('Epoch Training Acc:', 1.0)
('test loss', 1.1989957)
('test Acc:', 0.79683477)
model_080 F1_score: 83.40% >>>
690
('Epoch Training Loss:', 0.003911338984835311)
('Epoch Training Acc:', 1.0)
('test loss', 1.212207)
('test Acc:', 0.7902098)
model_080 F1_score: 82.93% >>>
691
('Epoch Training Loss:', 0.0038841733021399705)
('Epoch Training Acc:', 1.0)
('test loss', 1.2239367)
('test Acc:', 0.7857931)
model_080 F1_score: 82.46% >>>
692
('Epoch Training Loss:', 0.0037271243245413643)
('Epoch Training Acc:', 1.0)
('test loss', 1.2081054)
('test Acc:', 0.79609865)
model_080 F1_score: 83.41% >>>
693
('Epoch Training Loss:', 0.003986091784099699)
('Epoch Training Acc:', 1.0)
('test loss', 1.2011833)
('test Acc:', 0.79315424)
model_080 F1_score: 83.13% >>>
694
('Epoch Training Loss:', 0.003916161475899571)
('Epoch Training Acc:', 1.0)
('test loss', 1.2207729)
('test Acc:', 0.7876334)
model_080 F1_score: 82.57% >>>
695
('Epoch Training Loss:', 0.00406242877124896)
('Epoch Training Acc:', 1.0)
('test loss', 1.1961415)
('test Acc:', 0.79315424)
model_080 F1_score: 83.22% >>>
696
('Epoch Training Loss:', 0.003893889290338848)
('Epoch Training Acc:', 1.0)
('test loss', 1.2215543)
('test Acc:', 0.7880015)
model_080 F1_score: 82.66% >>>
697
('Epoch Training Loss:', 0.003777884186092706)
('Epoch Training Acc:', 1.0)
('test loss', 1.2248577)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
698
('Epoch Training Loss:', 0.0037457305907082628)
('Epoch Training Acc:', 1.0)
('test loss', 1.210559)
('test Acc:', 0.79536253)
model_080 F1_score: 83.25% >>>
699
('Epoch Training Loss:', 0.003935537337383721)
('Epoch Training Acc:', 1.0)
('test loss', 1.2276294)
('test Acc:', 0.78910565)
model_080 F1_score: 82.84% >>>
700
('Epoch Training Loss:', 0.0037855338650842896)
('Epoch Training Acc:', 1.0)
('test loss', 1.2151724)
('test Acc:', 0.7946264)
model_080 F1_score: 83.11% >>>
701
('Epoch Training Loss:', 0.003720013039128389)
('Epoch Training Acc:', 1.0)
('test loss', 1.2187405)
('test Acc:', 0.7924181)
model_080 F1_score: 83.13% >>>
702
('Epoch Training Loss:', 0.0037907653031652444)
('Epoch Training Acc:', 1.0)
('test loss', 1.211815)
('test Acc:', 0.78836954)
model_080 F1_score: 82.66% >>>
703
('Epoch Training Loss:', 0.003843600768959732)
('Epoch Training Acc:', 1.0)
('test loss', 1.2327918)
('test Acc:', 0.78836954)
model_080 F1_score: 82.78% >>>
704
('Epoch Training Loss:', 0.003838696326056379)
('Epoch Training Acc:', 1.0)
('test loss', 1.2203734)
('test Acc:', 0.79205006)
model_080 F1_score: 82.99% >>>
705
('Epoch Training Loss:', 0.003744204464055656)
('Epoch Training Acc:', 1.0)
('test loss', 1.2368648)
('test Acc:', 0.78652924)
model_080 F1_score: 82.43% >>>
706
('Epoch Training Loss:', 0.0037238319737298298)
('Epoch Training Acc:', 1.0)
('test loss', 1.2134702)
('test Acc:', 0.7924181)
model_080 F1_score: 83.08% >>>
707
('Epoch Training Loss:', 0.0037103674776517437)
('Epoch Training Acc:', 1.0)
('test loss', 1.2161092)
('test Acc:', 0.7924181)
model_080 F1_score: 83.17% >>>
708
('Epoch Training Loss:', 0.003783737948651833)
('Epoch Training Acc:', 1.0)
('test loss', 1.2043581)
('test Acc:', 0.7938903)
model_080 F1_score: 83.30% >>>
709
('Epoch Training Loss:', 0.0036921614155289717)
('Epoch Training Acc:', 1.0)
('test loss', 1.2252645)
('test Acc:', 0.7902098)
model_080 F1_score: 82.78% >>>
710
('Epoch Training Loss:', 0.0036698875001093256)
('Epoch Training Acc:', 1.0)
('test loss', 1.2164478)
('test Acc:', 0.7909459)
model_080 F1_score: 82.87% >>>
711
('Epoch Training Loss:', 0.0037824282080691773)
('Epoch Training Acc:', 1.0)
('test loss', 1.2005103)
('test Acc:', 0.79131395)
model_080 F1_score: 82.89% >>>
712
('Epoch Training Loss:', 0.003733271689270623)
('Epoch Training Acc:', 1.0)
('test loss', 1.2081336)
('test Acc:', 0.7887376)
model_080 F1_score: 82.74% >>>
713
('Epoch Training Loss:', 0.0038540197601832915)
('Epoch Training Acc:', 1.0)
('test loss', 1.212537)
('test Acc:', 0.7924181)
model_080 F1_score: 83.12% >>>
714
('Epoch Training Loss:', 0.0037236304697216838)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241081)
('test Acc:', 0.79425836)
model_080 F1_score: 83.07% >>>
715
('Epoch Training Loss:', 0.0038361480801540893)
('Epoch Training Acc:', 1.0)
('test loss', 1.2238262)
('test Acc:', 0.79205006)
model_080 F1_score: 83.08% >>>
716
('Epoch Training Loss:', 0.003847177801617363)
('Epoch Training Acc:', 1.0)
('test loss', 1.2120788)
('test Acc:', 0.7894737)
model_080 F1_score: 82.82% >>>
717
('Epoch Training Loss:', 0.003646802822004247)
('Epoch Training Acc:', 1.0)
('test loss', 1.2138236)
('test Acc:', 0.7949945)
model_080 F1_score: 83.20% >>>
718
('Epoch Training Loss:', 0.0038161792026585317)
('Epoch Training Acc:', 1.0)
('test loss', 1.1957086)
('test Acc:', 0.7972028)
model_080 F1_score: 83.30% >>>
719
('Epoch Training Loss:', 0.003830356481103081)
('Epoch Training Acc:', 1.0)
('test loss', 1.2152455)
('test Acc:', 0.79315424)
model_080 F1_score: 83.11% >>>
720
('Epoch Training Loss:', 0.0036042006213392597)
('Epoch Training Acc:', 1.0)
('test loss', 1.2098795)
('test Acc:', 0.7902098)
model_080 F1_score: 83.08% >>>
721
('Epoch Training Loss:', 0.0036538771437335527)
('Epoch Training Acc:', 1.0)
('test loss', 1.2231034)
('test Acc:', 0.7868973)
model_080 F1_score: 82.72% >>>
722
('Epoch Training Loss:', 0.003737264829396736)
('Epoch Training Acc:', 1.0)
('test loss', 1.2152723)
('test Acc:', 0.7938903)
model_080 F1_score: 83.21% >>>
723
('Epoch Training Loss:', 0.0037644481963070575)
('Epoch Training Acc:', 1.0)
('test loss', 1.2216524)
('test Acc:', 0.78652924)
model_080 F1_score: 82.63% >>>
724
('Epoch Training Loss:', 0.003677201383652573)
('Epoch Training Acc:', 1.0)
('test loss', 1.225141)
('test Acc:', 0.791682)
model_080 F1_score: 82.92% >>>
725
('Epoch Training Loss:', 0.0037332407218855224)
('Epoch Training Acc:', 1.0)
('test loss', 1.2146904)
('test Acc:', 0.79830694)
model_080 F1_score: 83.58% >>>
726
('Epoch Training Loss:', 0.003738941937626805)
('Epoch Training Acc:', 1.0)
('test loss', 1.2262619)
('test Acc:', 0.7938903)
model_080 F1_score: 83.08% >>>
727
('Epoch Training Loss:', 0.003694979604006221)
('Epoch Training Acc:', 1.0)
('test loss', 1.2337407)
('test Acc:', 0.7909459)
model_080 F1_score: 82.96% >>>
728
('Epoch Training Loss:', 0.0036236404052942817)
('Epoch Training Acc:', 1.0)
('test loss', 1.1892656)
('test Acc:', 0.79131395)
model_080 F1_score: 82.85% >>>
729
('Epoch Training Loss:', 0.003570865446363314)
('Epoch Training Acc:', 1.0)
('test loss', 1.2217706)
('test Acc:', 0.79315424)
model_080 F1_score: 83.02% >>>
730
('Epoch Training Loss:', 0.0037341672905313317)
('Epoch Training Acc:', 1.0)
('test loss', 1.2123431)
('test Acc:', 0.7957306)
model_080 F1_score: 83.21% >>>
731
('Epoch Training Loss:', 0.0037388156224551494)
('Epoch Training Acc:', 1.0)
('test loss', 1.2160281)
('test Acc:', 0.79830694)
model_080 F1_score: 83.53% >>>
732
('Epoch Training Loss:', 0.0036212929289831663)
('Epoch Training Acc:', 1.0)
('test loss', 1.2275783)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
733
('Epoch Training Loss:', 0.0037385228870334686)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241341)
('test Acc:', 0.78910565)
model_080 F1_score: 82.64% >>>
734
('Epoch Training Loss:', 0.003622352679940377)
('Epoch Training Acc:', 1.0)
('test loss', 1.1950927)
('test Acc:', 0.7975708)
model_080 F1_score: 83.29% >>>
735
('Epoch Training Loss:', 0.003614706181906513)
('Epoch Training Acc:', 1.0)
('test loss', 1.2176857)
('test Acc:', 0.7938903)
model_080 F1_score: 82.96% >>>
736
('Epoch Training Loss:', 0.0036469414217208396)
('Epoch Training Acc:', 1.0)
('test loss', 1.2176927)
('test Acc:', 0.7898417)
model_080 F1_score: 82.70% >>>
737
('Epoch Training Loss:', 0.0037632543144354713)
('Epoch Training Acc:', 1.0)
('test loss', 1.2265491)
('test Acc:', 0.7861612)
model_080 F1_score: 82.39% >>>
738
('Epoch Training Loss:', 0.003674529548334249)
('Epoch Training Acc:', 1.0)
('test loss', 1.2185475)
('test Acc:', 0.79352224)
model_080 F1_score: 83.24% >>>
739
('Epoch Training Loss:', 0.003628195995133865)
('Epoch Training Acc:', 1.0)
('test loss', 1.2286359)
('test Acc:', 0.7894737)
model_080 F1_score: 83.04% >>>
740
('Epoch Training Loss:', 0.0036245023120500264)
('Epoch Training Acc:', 1.0)
('test loss', 1.2091349)
('test Acc:', 0.7938903)
model_080 F1_score: 83.34% >>>
741
('Epoch Training Loss:', 0.0036943728791811736)
('Epoch Training Acc:', 1.0)
('test loss', 1.2278291)
('test Acc:', 0.79315424)
model_080 F1_score: 83.14% >>>
742
('Epoch Training Loss:', 0.0035956967240053928)
('Epoch Training Acc:', 1.0)
('test loss', 1.2104893)
('test Acc:', 0.7972028)
model_080 F1_score: 83.35% >>>
743
('Epoch Training Loss:', 0.003882683834490308)
('Epoch Training Acc:', 1.0)
('test loss', 1.2115191)
('test Acc:', 0.7946264)
model_080 F1_score: 83.28% >>>
744
('Epoch Training Loss:', 0.003364107349625556)
('Epoch Training Acc:', 1.0)
('test loss', 1.2180071)
('test Acc:', 0.7946264)
model_080 F1_score: 83.08% >>>
745
('Epoch Training Loss:', 0.0035661959564095014)
('Epoch Training Acc:', 1.0)
('test loss', 1.2192185)
('test Acc:', 0.7868973)
model_080 F1_score: 82.68% >>>
746
('Epoch Training Loss:', 0.003565724164218409)
('Epoch Training Acc:', 1.0)
('test loss', 1.203351)
('test Acc:', 0.791682)
model_080 F1_score: 82.90% >>>
747
('Epoch Training Loss:', 0.0034916895301648765)
('Epoch Training Acc:', 1.0)
('test loss', 1.2168008)
('test Acc:', 0.7957306)
model_080 F1_score: 83.37% >>>
748
('Epoch Training Loss:', 0.0035165659837730345)
('Epoch Training Acc:', 1.0)
('test loss', 1.222958)
('test Acc:', 0.78726536)
model_080 F1_score: 82.67% >>>
749
('Epoch Training Loss:', 0.003564800837011717)
('Epoch Training Acc:', 1.0)
('test loss', 1.2356386)
('test Acc:', 0.78910565)
model_080 F1_score: 82.80% >>>
750
('Epoch Training Loss:', 0.0034755041624521255)
('Epoch Training Acc:', 1.0)
('test loss', 1.219718)
('test Acc:', 0.7946264)
model_080 F1_score: 83.26% >>>
751
('Epoch Training Loss:', 0.0034102339295714046)
('Epoch Training Acc:', 1.0)
('test loss', 1.2014885)
('test Acc:', 0.79536253)
model_080 F1_score: 83.33% >>>
752
('Epoch Training Loss:', 0.0034734833670881926)
('Epoch Training Acc:', 1.0)
('test loss', 1.2159475)
('test Acc:', 0.79315424)
model_080 F1_score: 83.30% >>>
753
('Epoch Training Loss:', 0.0037181038678681944)
('Epoch Training Acc:', 1.0)
('test loss', 1.2116895)
('test Acc:', 0.798675)
model_080 F1_score: 83.55% >>>
754
('Epoch Training Loss:', 0.0034709933561316575)
('Epoch Training Acc:', 1.0)
('test loss', 1.2226571)
('test Acc:', 0.7972028)
model_080 F1_score: 83.44% >>>
755
('Epoch Training Loss:', 0.0035040709922213864)
('Epoch Training Acc:', 1.0)
('test loss', 1.2262878)
('test Acc:', 0.79536253)
model_080 F1_score: 83.28% >>>
756
('Epoch Training Loss:', 0.0035276674170745537)
('Epoch Training Acc:', 1.0)
('test loss', 1.2117246)
('test Acc:', 0.7938903)
model_080 F1_score: 83.06% >>>
757
('Epoch Training Loss:', 0.0035207231921958737)
('Epoch Training Acc:', 1.0)
('test loss', 1.2212481)
('test Acc:', 0.79131395)
model_080 F1_score: 82.90% >>>
758
('Epoch Training Loss:', 0.003636556683886738)
('Epoch Training Acc:', 1.0)
('test loss', 1.225556)
('test Acc:', 0.7924181)
model_080 F1_score: 83.05% >>>
759
('Epoch Training Loss:', 0.0033613520399740082)
('Epoch Training Acc:', 1.0)
('test loss', 1.2228976)
('test Acc:', 0.79425836)
model_080 F1_score: 83.18% >>>
760
('Epoch Training Loss:', 0.0034970408232766204)
('Epoch Training Acc:', 1.0)
('test loss', 1.2177722)
('test Acc:', 0.78910565)
model_080 F1_score: 82.83% >>>
761
('Epoch Training Loss:', 0.003577802574909583)
('Epoch Training Acc:', 1.0)
('test loss', 1.2094272)
('test Acc:', 0.79425836)
model_080 F1_score: 83.40% >>>
762
('Epoch Training Loss:', 0.0035575148631323827)
('Epoch Training Acc:', 1.0)
('test loss', 1.2219746)
('test Acc:', 0.78836954)
model_080 F1_score: 82.83% >>>
763
('Epoch Training Loss:', 0.003660484110696416)
('Epoch Training Acc:', 1.0)
('test loss', 1.2181576)
('test Acc:', 0.7905778)
model_080 F1_score: 82.98% >>>
764
('Epoch Training Loss:', 0.0034650043658075447)
('Epoch Training Acc:', 1.0)
('test loss', 1.2013582)
('test Acc:', 0.7949945)
model_080 F1_score: 83.16% >>>
765
('Epoch Training Loss:', 0.0033137526734208222)
('Epoch Training Acc:', 1.0)
('test loss', 1.2523952)
('test Acc:', 0.7876334)
model_080 F1_score: 82.53% >>>
766
('Epoch Training Loss:', 0.0036911468969265115)
('Epoch Training Acc:', 1.0)
('test loss', 1.2230475)
('test Acc:', 0.7894737)
model_080 F1_score: 82.88% >>>
767
('Epoch Training Loss:', 0.003468702019290504)
('Epoch Training Acc:', 1.0)
('test loss', 1.2247796)
('test Acc:', 0.7927862)
model_080 F1_score: 83.14% >>>
768
('Epoch Training Loss:', 0.003407881094062759)
('Epoch Training Acc:', 1.0)
('test loss', 1.2312331)
('test Acc:', 0.7949945)
model_080 F1_score: 83.28% >>>
769
('Epoch Training Loss:', 0.0035528675025489065)
('Epoch Training Acc:', 1.0)
('test loss', 1.2296573)
('test Acc:', 0.7909459)
model_080 F1_score: 82.84% >>>
770
('Epoch Training Loss:', 0.0033890588174472214)
('Epoch Training Acc:', 1.0)
('test loss', 1.2294863)
('test Acc:', 0.78836954)
model_080 F1_score: 82.87% >>>
771
('Epoch Training Loss:', 0.003522826204971352)
('Epoch Training Acc:', 1.0)
('test loss', 1.232346)
('test Acc:', 0.79205006)
model_080 F1_score: 83.03% >>>
772
('Epoch Training Loss:', 0.003547227483977622)
('Epoch Training Acc:', 1.0)
('test loss', 1.2116494)
('test Acc:', 0.79425836)
model_080 F1_score: 83.04% >>>
773
('Epoch Training Loss:', 0.003615192333199957)
('Epoch Training Acc:', 1.0)
('test loss', 1.2454549)
('test Acc:', 0.78836954)
model_080 F1_score: 82.71% >>>
774
('Epoch Training Loss:', 0.0034650973730094847)
('Epoch Training Acc:', 1.0)
('test loss', 1.2145408)
('test Acc:', 0.79425836)
model_080 F1_score: 83.17% >>>
775
('Epoch Training Loss:', 0.0033817157718658564)
('Epoch Training Acc:', 1.0)
('test loss', 1.2293593)
('test Acc:', 0.79205006)
model_080 F1_score: 83.00% >>>
776
('Epoch Training Loss:', 0.0034350135201748344)
('Epoch Training Acc:', 1.0)
('test loss', 1.2327994)
('test Acc:', 0.78652924)
model_080 F1_score: 82.60% >>>
777
('Epoch Training Loss:', 0.003385436059033964)
('Epoch Training Acc:', 1.0)
('test loss', 1.2247398)
('test Acc:', 0.791682)
model_080 F1_score: 82.98% >>>
778
('Epoch Training Loss:', 0.003328162172692828)
('Epoch Training Acc:', 1.0)
('test loss', 1.248271)
('test Acc:', 0.7887376)
model_080 F1_score: 82.65% >>>
779
('Epoch Training Loss:', 0.003390820002550754)
('Epoch Training Acc:', 1.0)
('test loss', 1.2424289)
('test Acc:', 0.78726536)
model_080 F1_score: 82.73% >>>
780
('Epoch Training Loss:', 0.00337728518206859)
('Epoch Training Acc:', 1.0)
('test loss', 1.2448815)
('test Acc:', 0.78726536)
model_080 F1_score: 82.71% >>>
781
('Epoch Training Loss:', 0.003435540231748746)
('Epoch Training Acc:', 1.0)
('test loss', 1.2162755)
('test Acc:', 0.79536253)
model_080 F1_score: 83.30% >>>
782
('Epoch Training Loss:', 0.0032504741525372083)
('Epoch Training Acc:', 1.0)
('test loss', 1.2552912)
('test Acc:', 0.79131395)
model_080 F1_score: 82.87% >>>
783
('Epoch Training Loss:', 0.0034731967871266534)
('Epoch Training Acc:', 1.0)
('test loss', 1.2389821)
('test Acc:', 0.79205006)
model_080 F1_score: 83.14% >>>
784
('Epoch Training Loss:', 0.0033140953469228407)
('Epoch Training Acc:', 1.0)
('test loss', 1.1995149)
('test Acc:', 0.79609865)
model_080 F1_score: 83.43% >>>
785
('Epoch Training Loss:', 0.0032491412171111733)
('Epoch Training Acc:', 1.0)
('test loss', 1.2234273)
('test Acc:', 0.78726536)
model_080 F1_score: 82.74% >>>
786
('Epoch Training Loss:', 0.00342342912699678)
('Epoch Training Acc:', 1.0)
('test loss', 1.2177467)
('test Acc:', 0.79352224)
model_080 F1_score: 82.96% >>>
787
('Epoch Training Loss:', 0.0033649712650003494)
('Epoch Training Acc:', 1.0)
('test loss', 1.2219795)
('test Acc:', 0.79352224)
model_080 F1_score: 83.31% >>>
788
('Epoch Training Loss:', 0.0033644862828623445)
('Epoch Training Acc:', 1.0)
('test loss', 1.2464981)
('test Acc:', 0.78910565)
model_080 F1_score: 82.58% >>>
789
('Epoch Training Loss:', 0.0032917476773945964)
('Epoch Training Acc:', 1.0)
('test loss', 1.2242143)
('test Acc:', 0.78910565)
model_080 F1_score: 82.83% >>>
790
('Epoch Training Loss:', 0.003511530094783666)
('Epoch Training Acc:', 1.0)
('test loss', 1.2363038)
('test Acc:', 0.7880015)
model_080 F1_score: 82.89% >>>
791
('Epoch Training Loss:', 0.0032463388861287967)
('Epoch Training Acc:', 1.0)
('test loss', 1.2186183)
('test Acc:', 0.791682)
model_080 F1_score: 82.99% >>>
792
('Epoch Training Loss:', 0.0034623912051756633)
('Epoch Training Acc:', 1.0)
('test loss', 1.2159331)
('test Acc:', 0.7957306)
model_080 F1_score: 83.36% >>>
793
('Epoch Training Loss:', 0.0031648858139305958)
('Epoch Training Acc:', 1.0)
('test loss', 1.2217722)
('test Acc:', 0.79352224)
model_080 F1_score: 83.27% >>>
794
('Epoch Training Loss:', 0.0033924756648957555)
('Epoch Training Acc:', 1.0)
('test loss', 1.2255449)
('test Acc:', 0.79315424)
model_080 F1_score: 83.24% >>>
795
('Epoch Training Loss:', 0.003278858832345577)
('Epoch Training Acc:', 1.0)
('test loss', 1.2182118)
('test Acc:', 0.79536253)
model_080 F1_score: 83.42% >>>
796
('Epoch Training Loss:', 0.00327813298554247)
('Epoch Training Acc:', 1.0)
('test loss', 1.2164798)
('test Acc:', 0.79315424)
model_080 F1_score: 83.12% >>>
797
('Epoch Training Loss:', 0.0033282420745308627)
('Epoch Training Acc:', 1.0)
('test loss', 1.2265986)
('test Acc:', 0.78652924)
model_080 F1_score: 82.63% >>>
798
('Epoch Training Loss:', 0.0032112535127453157)
('Epoch Training Acc:', 1.0)
('test loss', 1.2202399)
('test Acc:', 0.79352224)
model_080 F1_score: 83.02% >>>
799
('Epoch Training Loss:', 0.0033663682552287355)
('Epoch Training Acc:', 1.0)
('test loss', 1.2410945)
('test Acc:', 0.7894737)
model_080 F1_score: 82.75% >>>
800
('Epoch Training Loss:', 0.003225812279197271)
('Epoch Training Acc:', 1.0)
('test loss', 1.2222922)
('test Acc:', 0.79536253)
model_080 F1_score: 83.41% >>>
801
('Epoch Training Loss:', 0.0035922937549912604)
('Epoch Training Acc:', 1.0)
('test loss', 1.2272654)
('test Acc:', 0.7868973)
model_080 F1_score: 82.47% >>>
802
('Epoch Training Loss:', 0.003309167191218876)
('Epoch Training Acc:', 1.0)
('test loss', 1.2353446)
('test Acc:', 0.7938903)
model_080 F1_score: 83.18% >>>
803
('Epoch Training Loss:', 0.0033104646850006247)
('Epoch Training Acc:', 1.0)
('test loss', 1.2236769)
('test Acc:', 0.7938903)
model_080 F1_score: 83.14% >>>
804
('Epoch Training Loss:', 0.003372088700416498)
('Epoch Training Acc:', 1.0)
('test loss', 1.2298989)
('test Acc:', 0.7938903)
model_080 F1_score: 83.09% >>>
805
('Epoch Training Loss:', 0.003224713909730781)
('Epoch Training Acc:', 1.0)
('test loss', 1.2193147)
('test Acc:', 0.7887376)
model_080 F1_score: 82.72% >>>
806
('Epoch Training Loss:', 0.0031895840147626586)
('Epoch Training Acc:', 1.0)
('test loss', 1.2328633)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
807
('Epoch Training Loss:', 0.0033682583784866438)
('Epoch Training Acc:', 1.0)
('test loss', 1.226395)
('test Acc:', 0.79315424)
model_080 F1_score: 83.11% >>>
808
('Epoch Training Loss:', 0.003334382377943257)
('Epoch Training Acc:', 1.0)
('test loss', 1.232722)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
809
('Epoch Training Loss:', 0.0032922039381446666)
('Epoch Training Acc:', 1.0)
('test loss', 1.2326311)
('test Acc:', 0.7902098)
model_080 F1_score: 82.78% >>>
810
('Epoch Training Loss:', 0.0032251455677396734)
('Epoch Training Acc:', 1.0)
('test loss', 1.2142006)
('test Acc:', 0.7949945)
model_080 F1_score: 83.21% >>>
811
('Epoch Training Loss:', 0.003294057307357434)
('Epoch Training Acc:', 1.0)
('test loss', 1.2268063)
('test Acc:', 0.78910565)
model_080 F1_score: 82.75% >>>
812
('Epoch Training Loss:', 0.003332037686050171)
('Epoch Training Acc:', 1.0)
('test loss', 1.239336)
('test Acc:', 0.79352224)
model_080 F1_score: 83.03% >>>
813
('Epoch Training Loss:', 0.003404581543236418)
('Epoch Training Acc:', 1.0)
('test loss', 1.2355199)
('test Acc:', 0.79131395)
model_080 F1_score: 82.76% >>>
814
('Epoch Training Loss:', 0.003375806887561339)
('Epoch Training Acc:', 1.0)
('test loss', 1.2069347)
('test Acc:', 0.7946264)
model_080 F1_score: 83.40% >>>
815
('Epoch Training Loss:', 0.003366560917129391)
('Epoch Training Acc:', 1.0)
('test loss', 1.2147605)
('test Acc:', 0.79205006)
model_080 F1_score: 82.86% >>>
816
('Epoch Training Loss:', 0.00315323907398124)
('Epoch Training Acc:', 1.0)
('test loss', 1.2218901)
('test Acc:', 0.79131395)
model_080 F1_score: 82.91% >>>
817
('Epoch Training Loss:', 0.003379519343980064)
('Epoch Training Acc:', 1.0)
('test loss', 1.2412356)
('test Acc:', 0.7902098)
model_080 F1_score: 82.96% >>>
818
('Epoch Training Loss:', 0.0032158267613340286)
('Epoch Training Acc:', 1.0)
('test loss', 1.2352886)
('test Acc:', 0.7887376)
model_080 F1_score: 82.85% >>>
819
('Epoch Training Loss:', 0.00313000821370224)
('Epoch Training Acc:', 1.0)
('test loss', 1.2141017)
('test Acc:', 0.79352224)
model_080 F1_score: 83.04% >>>
820
('Epoch Training Loss:', 0.0032616918733765488)
('Epoch Training Acc:', 1.0)
('test loss', 1.231463)
('test Acc:', 0.7927862)
model_080 F1_score: 83.41% >>>
821
('Epoch Training Loss:', 0.003311456423944037)
('Epoch Training Acc:', 1.0)
('test loss', 1.228694)
('test Acc:', 0.791682)
model_080 F1_score: 82.90% >>>
822
('Epoch Training Loss:', 0.003170288184264791)
('Epoch Training Acc:', 1.0)
('test loss', 1.2342883)
('test Acc:', 0.791682)
model_080 F1_score: 82.96% >>>
823
('Epoch Training Loss:', 0.0032810094162414316)
('Epoch Training Acc:', 1.0)
('test loss', 1.2430065)
('test Acc:', 0.7938903)
model_080 F1_score: 83.06% >>>
824
('Epoch Training Loss:', 0.0032091155112539127)
('Epoch Training Acc:', 1.0)
('test loss', 1.2371861)
('test Acc:', 0.7898417)
model_080 F1_score: 82.83% >>>
825
('Epoch Training Loss:', 0.0031674645924795186)
('Epoch Training Acc:', 1.0)
('test loss', 1.2054981)
('test Acc:', 0.79205006)
model_080 F1_score: 83.21% >>>
826
('Epoch Training Loss:', 0.0034227299820486223)
('Epoch Training Acc:', 1.0)
('test loss', 1.2224112)
('test Acc:', 0.7972028)
model_080 F1_score: 83.31% >>>
827
('Epoch Training Loss:', 0.0033272338378083077)
('Epoch Training Acc:', 1.0)
('test loss', 1.2436701)
('test Acc:', 0.7898417)
model_080 F1_score: 82.99% >>>
828
('Epoch Training Loss:', 0.0033179369193021557)
('Epoch Training Acc:', 1.0)
('test loss', 1.2367488)
('test Acc:', 0.7909459)
model_080 F1_score: 82.85% >>>
829
('Epoch Training Loss:', 0.0031532405664620455)
('Epoch Training Acc:', 1.0)
('test loss', 1.2182976)
('test Acc:', 0.7909459)
model_080 F1_score: 82.78% >>>
830
('Epoch Training Loss:', 0.003356280716616311)
('Epoch Training Acc:', 1.0)
('test loss', 1.2249207)
('test Acc:', 0.7957306)
model_080 F1_score: 83.12% >>>
831
('Epoch Training Loss:', 0.003107060801994521)
('Epoch Training Acc:', 1.0)
('test loss', 1.2427119)
('test Acc:', 0.7938903)
model_080 F1_score: 83.21% >>>
832
('Epoch Training Loss:', 0.0030733155404050194)
('Epoch Training Acc:', 1.0)
('test loss', 1.2271739)
('test Acc:', 0.7868973)
model_080 F1_score: 82.61% >>>
833
('Epoch Training Loss:', 0.0032171556476896512)
('Epoch Training Acc:', 1.0)
('test loss', 1.203899)
('test Acc:', 0.7979389)
model_080 F1_score: 83.64% >>>
834
('Epoch Training Loss:', 0.0034366004438197706)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241392)
('test Acc:', 0.79352224)
model_080 F1_score: 83.00% >>>
835
('Epoch Training Loss:', 0.003115455435363401)
('Epoch Training Acc:', 1.0)
('test loss', 1.2422425)
('test Acc:', 0.79131395)
model_080 F1_score: 83.05% >>>
836
('Epoch Training Loss:', 0.003215955724044761)
('Epoch Training Acc:', 1.0)
('test loss', 1.2293626)
('test Acc:', 0.7880015)
model_080 F1_score: 82.56% >>>
837
('Epoch Training Loss:', 0.0032059720733741415)
('Epoch Training Acc:', 1.0)
('test loss', 1.2311481)
('test Acc:', 0.7905778)
model_080 F1_score: 82.93% >>>
838
('Epoch Training Loss:', 0.0032135509109139093)
('Epoch Training Acc:', 1.0)
('test loss', 1.2229462)
('test Acc:', 0.7946264)
model_080 F1_score: 83.28% >>>
839
('Epoch Training Loss:', 0.0031828656169636815)
('Epoch Training Acc:', 1.0)
('test loss', 1.2269785)
('test Acc:', 0.7876334)
model_080 F1_score: 82.72% >>>
840
('Epoch Training Loss:', 0.003061615184378752)
('Epoch Training Acc:', 1.0)
('test loss', 1.2342885)
('test Acc:', 0.7938903)
model_080 F1_score: 83.11% >>>
841
('Epoch Training Loss:', 0.0031186882333713584)
('Epoch Training Acc:', 1.0)
('test loss', 1.2560813)
('test Acc:', 0.791682)
model_080 F1_score: 83.09% >>>
842
('Epoch Training Loss:', 0.0033566945076017873)
('Epoch Training Acc:', 1.0)
('test loss', 1.2338278)
('test Acc:', 0.7949945)
model_080 F1_score: 83.21% >>>
843
('Epoch Training Loss:', 0.0032168473408091813)
('Epoch Training Acc:', 1.0)
('test loss', 1.2286115)
('test Acc:', 0.7938903)
model_080 F1_score: 83.19% >>>
844
('Epoch Training Loss:', 0.003152102873173135)
('Epoch Training Acc:', 1.0)
('test loss', 1.2394252)
('test Acc:', 0.7902098)
model_080 F1_score: 83.02% >>>
845
('Epoch Training Loss:', 0.003101503056313959)
('Epoch Training Acc:', 1.0)
('test loss', 1.2225425)
('test Acc:', 0.7905778)
model_080 F1_score: 82.63% >>>
846
('Epoch Training Loss:', 0.0030591583658861055)
('Epoch Training Acc:', 1.0)
('test loss', 1.2410624)
('test Acc:', 0.7924181)
model_080 F1_score: 83.20% >>>
847
('Epoch Training Loss:', 0.003101809901636443)
('Epoch Training Acc:', 1.0)
('test loss', 1.2368346)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
848
('Epoch Training Loss:', 0.0030148340010782704)
('Epoch Training Acc:', 1.0)
('test loss', 1.2361455)
('test Acc:', 0.7909459)
model_080 F1_score: 82.95% >>>
849
('Epoch Training Loss:', 0.0031501806724918424)
('Epoch Training Acc:', 1.0)
('test loss', 1.2348009)
('test Acc:', 0.79609865)
model_080 F1_score: 83.38% >>>
850
('Epoch Training Loss:', 0.0031295695716835326)
('Epoch Training Acc:', 1.0)
('test loss', 1.2379625)
('test Acc:', 0.7927862)
model_080 F1_score: 83.21% >>>
851
('Epoch Training Loss:', 0.0030190351631063095)
('Epoch Training Acc:', 1.0)
('test loss', 1.2311995)
('test Acc:', 0.79205006)
model_080 F1_score: 83.01% >>>
852
('Epoch Training Loss:', 0.0032334788552361715)
('Epoch Training Acc:', 1.0)
('test loss', 1.2141436)
('test Acc:', 0.79205006)
model_080 F1_score: 83.19% >>>
853
('Epoch Training Loss:', 0.003049412057407608)
('Epoch Training Acc:', 1.0)
('test loss', 1.2293406)
('test Acc:', 0.7927862)
model_080 F1_score: 83.22% >>>
854
('Epoch Training Loss:', 0.003099530886629509)
('Epoch Training Acc:', 1.0)
('test loss', 1.2397828)
('test Acc:', 0.7894737)
model_080 F1_score: 82.55% >>>
855
('Epoch Training Loss:', 0.003045191769615485)
('Epoch Training Acc:', 1.0)
('test loss', 1.2245457)
('test Acc:', 0.7909459)
model_080 F1_score: 82.95% >>>
856
('Epoch Training Loss:', 0.0030920152203179896)
('Epoch Training Acc:', 1.0)
('test loss', 1.23261)
('test Acc:', 0.78836954)
model_080 F1_score: 82.46% >>>
857
('Epoch Training Loss:', 0.003064477205498406)
('Epoch Training Acc:', 1.0)
('test loss', 1.2496551)
('test Acc:', 0.78836954)
model_080 F1_score: 82.91% >>>
858
('Epoch Training Loss:', 0.0030297708108264487)
('Epoch Training Acc:', 1.0)
('test loss', 1.2108694)
('test Acc:', 0.7938903)
model_080 F1_score: 83.16% >>>
859
('Epoch Training Loss:', 0.0029613480578518647)
('Epoch Training Acc:', 1.0)
('test loss', 1.2249181)
('test Acc:', 0.79352224)
model_080 F1_score: 83.05% >>>
860
('Epoch Training Loss:', 0.0032018661709116714)
('Epoch Training Acc:', 1.0)
('test loss', 1.249644)
('test Acc:', 0.7894737)
model_080 F1_score: 82.73% >>>
861
('Epoch Training Loss:', 0.0030687529306305805)
('Epoch Training Acc:', 1.0)
('test loss', 1.2255294)
('test Acc:', 0.79131395)
model_080 F1_score: 83.06% >>>
862
('Epoch Training Loss:', 0.0030342241589096375)
('Epoch Training Acc:', 1.0)
('test loss', 1.2430494)
('test Acc:', 0.7894737)
model_080 F1_score: 82.83% >>>
863
('Epoch Training Loss:', 0.003041052650587517)
('Epoch Training Acc:', 1.0)
('test loss', 1.2237836)
('test Acc:', 0.7946264)
model_080 F1_score: 82.92% >>>
864
('Epoch Training Loss:', 0.003014929306573322)
('Epoch Training Acc:', 1.0)
('test loss', 1.232948)
('test Acc:', 0.79315424)
model_080 F1_score: 82.98% >>>
865
('Epoch Training Loss:', 0.0030866376455378486)
('Epoch Training Acc:', 1.0)
('test loss', 1.2469442)
('test Acc:', 0.7924181)
model_080 F1_score: 82.92% >>>
866
('Epoch Training Loss:', 0.003064383221499156)
('Epoch Training Acc:', 1.0)
('test loss', 1.2306077)
('test Acc:', 0.7957306)
model_080 F1_score: 83.33% >>>
867
('Epoch Training Loss:', 0.003262964204623131)
('Epoch Training Acc:', 1.0)
('test loss', 1.2548628)
('test Acc:', 0.7909459)
model_080 F1_score: 82.94% >>>
868
('Epoch Training Loss:', 0.0030408073871512897)
('Epoch Training Acc:', 1.0)
('test loss', 1.248841)
('test Acc:', 0.791682)
model_080 F1_score: 82.91% >>>
869
('Epoch Training Loss:', 0.0030279247839644086)
('Epoch Training Acc:', 1.0)
('test loss', 1.2377404)
('test Acc:', 0.7927862)
model_080 F1_score: 82.94% >>>
870
('Epoch Training Loss:', 0.002934381255727203)
('Epoch Training Acc:', 1.0)
('test loss', 1.2205758)
('test Acc:', 0.7964667)
model_080 F1_score: 83.30% >>>
871
('Epoch Training Loss:', 0.0031038740808071452)
('Epoch Training Acc:', 1.0)
('test loss', 1.2435489)
('test Acc:', 0.78836954)
model_080 F1_score: 82.90% >>>
872
('Epoch Training Loss:', 0.0029824253060724004)
('Epoch Training Acc:', 1.0)
('test loss', 1.2325922)
('test Acc:', 0.7927862)
model_080 F1_score: 83.02% >>>
873
('Epoch Training Loss:', 0.003001172465246782)
('Epoch Training Acc:', 1.0)
('test loss', 1.2188755)
('test Acc:', 0.7972028)
model_080 F1_score: 83.42% >>>
874
('Epoch Training Loss:', 0.002895847484978731)
('Epoch Training Acc:', 1.0)
('test loss', 1.225485)
('test Acc:', 0.7938903)
model_080 F1_score: 83.17% >>>
875
('Epoch Training Loss:', 0.0029357175053519313)
('Epoch Training Acc:', 1.0)
('test loss', 1.2155644)
('test Acc:', 0.79683477)
model_080 F1_score: 83.51% >>>
876
('Epoch Training Loss:', 0.003091751761530759)
('Epoch Training Acc:', 1.0)
('test loss', 1.2437296)
('test Acc:', 0.79131395)
model_080 F1_score: 83.30% >>>
877
('Epoch Training Loss:', 0.003020767880116182)
('Epoch Training Acc:', 1.0)
('test loss', 1.2385256)
('test Acc:', 0.79352224)
model_080 F1_score: 83.21% >>>
878
('Epoch Training Loss:', 0.002942363348665822)
('Epoch Training Acc:', 1.0)
('test loss', 1.2266393)
('test Acc:', 0.79315424)
model_080 F1_score: 83.04% >>>
879
('Epoch Training Loss:', 0.003017977528998017)
('Epoch Training Acc:', 1.0)
('test loss', 1.2527083)
('test Acc:', 0.78836954)
model_080 F1_score: 82.71% >>>
880
('Epoch Training Loss:', 0.0029395516930890153)
('Epoch Training Acc:', 1.0)
('test loss', 1.2114526)
('test Acc:', 0.7946264)
model_080 F1_score: 83.25% >>>
881
('Epoch Training Loss:', 0.00308745615029693)
('Epoch Training Acc:', 1.0)
('test loss', 1.2337006)
('test Acc:', 0.7902098)
model_080 F1_score: 82.93% >>>
882
('Epoch Training Loss:', 0.0029448868435792974)
('Epoch Training Acc:', 1.0)
('test loss', 1.232671)
('test Acc:', 0.7927862)
model_080 F1_score: 83.14% >>>
883
('Epoch Training Loss:', 0.0030154843889249605)
('Epoch Training Acc:', 1.0)
('test loss', 1.2380209)
('test Acc:', 0.78505707)
model_080 F1_score: 82.51% >>>
884
('Epoch Training Loss:', 0.002994731116814364)
('Epoch Training Acc:', 1.0)
('test loss', 1.2398834)
('test Acc:', 0.7946264)
model_080 F1_score: 83.11% >>>
885
('Epoch Training Loss:', 0.003156500710247201)
('Epoch Training Acc:', 1.0)
('test loss', 1.2358356)
('test Acc:', 0.7909459)
model_080 F1_score: 82.99% >>>
886
('Epoch Training Loss:', 0.002958736682558083)
('Epoch Training Acc:', 1.0)
('test loss', 1.2285712)
('test Acc:', 0.7938903)
model_080 F1_score: 83.15% >>>
887
('Epoch Training Loss:', 0.0028607927797565935)
('Epoch Training Acc:', 1.0)
('test loss', 1.2263703)
('test Acc:', 0.7949945)
model_080 F1_score: 83.22% >>>
888
('Epoch Training Loss:', 0.002896422390222142)
('Epoch Training Acc:', 1.0)
('test loss', 1.23981)
('test Acc:', 0.7924181)
model_080 F1_score: 83.11% >>>
889
('Epoch Training Loss:', 0.0031433221511179)
('Epoch Training Acc:', 1.0)
('test loss', 1.2284695)
('test Acc:', 0.7924181)
model_080 F1_score: 83.26% >>>
890
('Epoch Training Loss:', 0.0029940217018520343)
('Epoch Training Acc:', 1.0)
('test loss', 1.2213358)
('test Acc:', 0.78726536)
model_080 F1_score: 82.59% >>>
891
('Epoch Training Loss:', 0.0027526766862138174)
('Epoch Training Acc:', 1.0)
('test loss', 1.2186)
('test Acc:', 0.79352224)
model_080 F1_score: 82.94% >>>
892
('Epoch Training Loss:', 0.0031541177841063472)
('Epoch Training Acc:', 1.0)
('test loss', 1.2518584)
('test Acc:', 0.7905778)
model_080 F1_score: 82.89% >>>
893
('Epoch Training Loss:', 0.00302214238081433)
('Epoch Training Acc:', 1.0)
('test loss', 1.2432941)
('test Acc:', 0.79425836)
model_080 F1_score: 83.02% >>>
894
('Epoch Training Loss:', 0.003153911748995597)
('Epoch Training Acc:', 1.0)
('test loss', 1.2400689)
('test Acc:', 0.79315424)
model_080 F1_score: 83.25% >>>
895
('Epoch Training Loss:', 0.002860539359971881)
('Epoch Training Acc:', 1.0)
('test loss', 1.2188271)
('test Acc:', 0.7979389)
model_080 F1_score: 83.60% >>>
896
('Epoch Training Loss:', 0.0030550112714990973)
('Epoch Training Acc:', 1.0)
('test loss', 1.2336125)
('test Acc:', 0.7957306)
model_080 F1_score: 83.12% >>>
897
('Epoch Training Loss:', 0.0028256865471121273)
('Epoch Training Acc:', 1.0)
('test loss', 1.2372736)
('test Acc:', 0.79315424)
model_080 F1_score: 82.95% >>>
898
('Epoch Training Loss:', 0.0029106631827744422)
('Epoch Training Acc:', 1.0)
('test loss', 1.2407457)
('test Acc:', 0.7957306)
model_080 F1_score: 83.43% >>>
899
('Epoch Training Loss:', 0.003035973079931864)
('Epoch Training Acc:', 1.0)
('test loss', 1.2435684)
('test Acc:', 0.79315424)
model_080 F1_score: 83.02% >>>
900
('Epoch Training Loss:', 0.0029409373914859316)
('Epoch Training Acc:', 1.0)
('test loss', 1.2471634)
('test Acc:', 0.7839529)
model_080 F1_score: 82.50% >>>
901
('Epoch Training Loss:', 0.002867806418635155)
('Epoch Training Acc:', 1.0)
('test loss', 1.2430489)
('test Acc:', 0.7946264)
model_080 F1_score: 83.17% >>>
902
('Epoch Training Loss:', 0.0029860396557523927)
('Epoch Training Acc:', 1.0)
('test loss', 1.2361159)
('test Acc:', 0.79131395)
model_080 F1_score: 83.10% >>>
903
('Epoch Training Loss:', 0.002871377480460069)
('Epoch Training Acc:', 1.0)
('test loss', 1.240357)
('test Acc:', 0.7924181)
model_080 F1_score: 83.05% >>>
904
('Epoch Training Loss:', 0.0029863640538678737)
('Epoch Training Acc:', 1.0)
('test loss', 1.2384492)
('test Acc:', 0.7898417)
model_080 F1_score: 82.83% >>>
905
('Epoch Training Loss:', 0.0029681011451430095)
('Epoch Training Acc:', 1.0)
('test loss', 1.2364371)
('test Acc:', 0.79425836)
model_080 F1_score: 83.15% >>>
906
('Epoch Training Loss:', 0.002850641230452311)
('Epoch Training Acc:', 1.0)
('test loss', 1.2244964)
('test Acc:', 0.7924181)
model_080 F1_score: 82.93% >>>
907
('Epoch Training Loss:', 0.002817528178638895)
('Epoch Training Acc:', 1.0)
('test loss', 1.25411)
('test Acc:', 0.7880015)
model_080 F1_score: 82.68% >>>
908
('Epoch Training Loss:', 0.0028972179234187934)
('Epoch Training Acc:', 1.0)
('test loss', 1.2425667)
('test Acc:', 0.7964667)
model_080 F1_score: 83.42% >>>
909
('Epoch Training Loss:', 0.002977749309138744)
('Epoch Training Acc:', 1.0)
('test loss', 1.2444649)
('test Acc:', 0.791682)
model_080 F1_score: 83.07% >>>
910
('Epoch Training Loss:', 0.0029132015642971965)
('Epoch Training Acc:', 1.0)
('test loss', 1.2464747)
('test Acc:', 0.7902098)
model_080 F1_score: 82.64% >>>
911
('Epoch Training Loss:', 0.0029039356240900815)
('Epoch Training Acc:', 1.0)
('test loss', 1.2381647)
('test Acc:', 0.7894737)
model_080 F1_score: 82.82% >>>
912
('Epoch Training Loss:', 0.002891148957587575)
('Epoch Training Acc:', 1.0)
('test loss', 1.2456713)
('test Acc:', 0.78836954)
model_080 F1_score: 82.74% >>>
913
('Epoch Training Loss:', 0.0028868979175058485)
('Epoch Training Acc:', 1.0)
('test loss', 1.2315533)
('test Acc:', 0.7938903)
model_080 F1_score: 82.95% >>>
914
('Epoch Training Loss:', 0.002874732251257228)
('Epoch Training Acc:', 1.0)
('test loss', 1.2377654)
('test Acc:', 0.79205006)
model_080 F1_score: 83.29% >>>
915
('Epoch Training Loss:', 0.002837396786617319)
('Epoch Training Acc:', 1.0)
('test loss', 1.242914)
('test Acc:', 0.7938903)
model_080 F1_score: 83.28% >>>
916
('Epoch Training Loss:', 0.0028545964960358106)
('Epoch Training Acc:', 1.0)
('test loss', 1.2286718)
('test Acc:', 0.78505707)
model_080 F1_score: 82.43% >>>
917
('Epoch Training Loss:', 0.002977354989070591)
('Epoch Training Acc:', 1.0)
('test loss', 1.2313355)
('test Acc:', 0.7905778)
model_080 F1_score: 83.13% >>>
918
('Epoch Training Loss:', 0.002931429417003528)
('Epoch Training Acc:', 1.0)
('test loss', 1.2404051)
('test Acc:', 0.79315424)
model_080 F1_score: 83.04% >>>
919
('Epoch Training Loss:', 0.0028719081678900693)
('Epoch Training Acc:', 1.0)
('test loss', 1.2208798)
('test Acc:', 0.79131395)
model_080 F1_score: 83.13% >>>
920
('Epoch Training Loss:', 0.002720456394399662)
('Epoch Training Acc:', 1.0)
('test loss', 1.2225534)
('test Acc:', 0.7876334)
model_080 F1_score: 82.78% >>>
921
('Epoch Training Loss:', 0.0029927216796750145)
('Epoch Training Acc:', 1.0)
('test loss', 1.2508061)
('test Acc:', 0.78836954)
model_080 F1_score: 82.60% >>>
922
('Epoch Training Loss:', 0.0027737624327528465)
('Epoch Training Acc:', 1.0)
('test loss', 1.2266499)
('test Acc:', 0.7898417)
model_080 F1_score: 82.71% >>>
923
('Epoch Training Loss:', 0.002872538301289751)
('Epoch Training Acc:', 1.0)
('test loss', 1.23981)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
924
('Epoch Training Loss:', 0.0029511103107324743)
('Epoch Training Acc:', 1.0)
('test loss', 1.2320629)
('test Acc:', 0.79536253)
model_080 F1_score: 83.36% >>>
925
('Epoch Training Loss:', 0.002897494294302305)
('Epoch Training Acc:', 1.0)
('test loss', 1.256358)
('test Acc:', 0.79131395)
model_080 F1_score: 83.25% >>>
926
('Epoch Training Loss:', 0.002754816294327611)
('Epoch Training Acc:', 1.0)
('test loss', 1.2416439)
('test Acc:', 0.79352224)
model_080 F1_score: 83.26% >>>
927
('Epoch Training Loss:', 0.0028896321728097973)
('Epoch Training Acc:', 1.0)
('test loss', 1.2578331)
('test Acc:', 0.7898417)
model_080 F1_score: 82.73% >>>
928
('Epoch Training Loss:', 0.0029254196974761726)
('Epoch Training Acc:', 1.0)
('test loss', 1.2459819)
('test Acc:', 0.7876334)
model_080 F1_score: 82.78% >>>
929
('Epoch Training Loss:', 0.002866195776277891)
('Epoch Training Acc:', 1.0)
('test loss', 1.255281)
('test Acc:', 0.79131395)
model_080 F1_score: 82.94% >>>
930
('Epoch Training Loss:', 0.002778043519356288)
('Epoch Training Acc:', 1.0)
('test loss', 1.2345139)
('test Acc:', 0.7949945)
model_080 F1_score: 83.33% >>>
931
('Epoch Training Loss:', 0.0027825131942336156)
('Epoch Training Acc:', 1.0)
('test loss', 1.2371533)
('test Acc:', 0.7905778)
model_080 F1_score: 82.83% >>>
932
('Epoch Training Loss:', 0.0027317341241541726)
('Epoch Training Acc:', 1.0)
('test loss', 1.2453859)
('test Acc:', 0.7894737)
model_080 F1_score: 82.73% >>>
933
('Epoch Training Loss:', 0.0027578235917644633)
('Epoch Training Acc:', 1.0)
('test loss', 1.2507935)
('test Acc:', 0.7905778)
model_080 F1_score: 82.87% >>>
934
('Epoch Training Loss:', 0.002830557568813674)
('Epoch Training Acc:', 1.0)
('test loss', 1.2233281)
('test Acc:', 0.79425836)
model_080 F1_score: 83.47% >>>
935
('Epoch Training Loss:', 0.002799695377234457)
('Epoch Training Acc:', 1.0)
('test loss', 1.2374698)
('test Acc:', 0.7909459)
model_080 F1_score: 82.94% >>>
936
('Epoch Training Loss:', 0.002789774142911483)
('Epoch Training Acc:', 1.0)
('test loss', 1.2467833)
('test Acc:', 0.79536253)
model_080 F1_score: 83.30% >>>
937
('Epoch Training Loss:', 0.002822623699103133)
('Epoch Training Acc:', 1.0)
('test loss', 1.2377602)
('test Acc:', 0.7975708)
model_080 F1_score: 83.53% >>>
938
('Epoch Training Loss:', 0.002865399615984643)
('Epoch Training Acc:', 1.0)
('test loss', 1.2385482)
('test Acc:', 0.78726536)
model_080 F1_score: 82.49% >>>
939
('Epoch Training Loss:', 0.002800343954731943)
('Epoch Training Acc:', 1.0)
('test loss', 1.2365448)
('test Acc:', 0.791682)
model_080 F1_score: 82.97% >>>
940
('Epoch Training Loss:', 0.0026912795392490807)
('Epoch Training Acc:', 1.0)
('test loss', 1.2347784)
('test Acc:', 0.78910565)
model_080 F1_score: 82.73% >>>
941
('Epoch Training Loss:', 0.0027681609899445903)
('Epoch Training Acc:', 1.0)
('test loss', 1.2353061)
('test Acc:', 0.79536253)
model_080 F1_score: 83.20% >>>
942
('Epoch Training Loss:', 0.0027669665737448668)
('Epoch Training Acc:', 1.0)
('test loss', 1.2295187)
('test Acc:', 0.7876334)
model_080 F1_score: 82.62% >>>
943
('Epoch Training Loss:', 0.0027243216427450534)
('Epoch Training Acc:', 1.0)
('test loss', 1.2482285)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
944
('Epoch Training Loss:', 0.0027401597767493513)
('Epoch Training Acc:', 1.0)
('test loss', 1.2378384)
('test Acc:', 0.7949945)
model_080 F1_score: 83.29% >>>
945
('Epoch Training Loss:', 0.002738276176387444)
('Epoch Training Acc:', 1.0)
('test loss', 1.2344737)
('test Acc:', 0.791682)
model_080 F1_score: 83.04% >>>
946
('Epoch Training Loss:', 0.0028404539325492806)
('Epoch Training Acc:', 1.0)
('test loss', 1.2413342)
('test Acc:', 0.7946264)
model_080 F1_score: 83.40% >>>
947
('Epoch Training Loss:', 0.0027461935342216748)
('Epoch Training Acc:', 1.0)
('test loss', 1.2462177)
('test Acc:', 0.791682)
model_080 F1_score: 83.17% >>>
948
('Epoch Training Loss:', 0.0029181981590227224)
('Epoch Training Acc:', 1.0)
('test loss', 1.2459728)
('test Acc:', 0.79352224)
model_080 F1_score: 83.08% >>>
949
('Epoch Training Loss:', 0.002714071652462735)
('Epoch Training Acc:', 1.0)
('test loss', 1.2526985)
('test Acc:', 0.7876334)
model_080 F1_score: 82.67% >>>
950
('Epoch Training Loss:', 0.002823586371505371)
('Epoch Training Acc:', 1.0)
('test loss', 1.2344716)
('test Acc:', 0.7927862)
model_080 F1_score: 83.01% >>>
951
('Epoch Training Loss:', 0.0027748754987442226)
('Epoch Training Acc:', 1.0)
('test loss', 1.2632066)
('test Acc:', 0.7946264)
model_080 F1_score: 83.21% >>>
952
('Epoch Training Loss:', 0.002717310951993568)
('Epoch Training Acc:', 1.0)
('test loss', 1.2430772)
('test Acc:', 0.7946264)
model_080 F1_score: 83.35% >>>
953
('Epoch Training Loss:', 0.002861239643607405)
('Epoch Training Acc:', 1.0)
('test loss', 1.2431427)
('test Acc:', 0.7902098)
model_080 F1_score: 82.70% >>>
954
('Epoch Training Loss:', 0.0027833633712361916)
('Epoch Training Acc:', 1.0)
('test loss', 1.2402692)
('test Acc:', 0.79352224)
model_080 F1_score: 83.11% >>>
955
('Epoch Training Loss:', 0.002837712059772457)
('Epoch Training Acc:', 1.0)
('test loss', 1.2218308)
('test Acc:', 0.79315424)
model_080 F1_score: 83.07% >>>
956
('Epoch Training Loss:', 0.002755840273493959)
('Epoch Training Acc:', 1.0)
('test loss', 1.2466108)
('test Acc:', 0.7905778)
model_080 F1_score: 82.79% >>>
957
('Epoch Training Loss:', 0.002808144785831246)
('Epoch Training Acc:', 1.0)
('test loss', 1.2398921)
('test Acc:', 0.7905778)
model_080 F1_score: 82.92% >>>
958
('Epoch Training Loss:', 0.0026626832086549257)
('Epoch Training Acc:', 1.0)
('test loss', 1.2368093)
('test Acc:', 0.7880015)
model_080 F1_score: 82.81% >>>
959
('Epoch Training Loss:', 0.002808644675951655)
('Epoch Training Acc:', 1.0)
('test loss', 1.2504123)
('test Acc:', 0.78910565)
model_080 F1_score: 82.71% >>>
960
('Epoch Training Loss:', 0.002803117162784474)
('Epoch Training Acc:', 1.0)
('test loss', 1.2471219)
('test Acc:', 0.7975708)
model_080 F1_score: 83.39% >>>
961
('Epoch Training Loss:', 0.0026802345755641)
('Epoch Training Acc:', 1.0)
('test loss', 1.2364464)
('test Acc:', 0.79609865)
model_080 F1_score: 83.39% >>>
962
('Epoch Training Loss:', 0.00283074684375606)
('Epoch Training Acc:', 1.0)
('test loss', 1.2302291)
('test Acc:', 0.7979389)
model_080 F1_score: 83.54% >>>
963
('Epoch Training Loss:', 0.002931751108917524)
('Epoch Training Acc:', 1.0)
('test loss', 1.2435288)
('test Acc:', 0.7905778)
model_080 F1_score: 82.89% >>>
964
('Epoch Training Loss:', 0.002538404301958508)
('Epoch Training Acc:', 1.0)
('test loss', 1.2482873)
('test Acc:', 0.7902098)
model_080 F1_score: 82.98% >>>
965
('Epoch Training Loss:', 0.0027466440424177563)
('Epoch Training Acc:', 1.0)
('test loss', 1.2371771)
('test Acc:', 0.7909459)
model_080 F1_score: 83.02% >>>
966
('Epoch Training Loss:', 0.0028875515436084243)
('Epoch Training Acc:', 1.0)
('test loss', 1.2515756)
('test Acc:', 0.79315424)
model_080 F1_score: 82.99% >>>
967
('Epoch Training Loss:', 0.0025602881974009506)
('Epoch Training Acc:', 1.0)
('test loss', 1.2303545)
('test Acc:', 0.79609865)
model_080 F1_score: 83.11% >>>
968
('Epoch Training Loss:', 0.002613653929984139)
('Epoch Training Acc:', 1.0)
('test loss', 1.2517223)
('test Acc:', 0.7924181)
model_080 F1_score: 82.92% >>>
969
('Epoch Training Loss:', 0.002557650899689179)
('Epoch Training Acc:', 1.0)
('test loss', 1.2248229)
('test Acc:', 0.79205006)
model_080 F1_score: 83.19% >>>
970
('Epoch Training Loss:', 0.002788162200431543)
('Epoch Training Acc:', 1.0)
('test loss', 1.2689824)
('test Acc:', 0.7902098)
model_080 F1_score: 82.83% >>>
971
('Epoch Training Loss:', 0.002654922921465186)
('Epoch Training Acc:', 1.0)
('test loss', 1.2660733)
('test Acc:', 0.79425836)
model_080 F1_score: 83.09% >>>
972
('Epoch Training Loss:', 0.0026248240956192603)
('Epoch Training Acc:', 1.0)
('test loss', 1.2539496)
('test Acc:', 0.7902098)
model_080 F1_score: 82.80% >>>
973
('Epoch Training Loss:', 0.0026865272084251046)
('Epoch Training Acc:', 1.0)
('test loss', 1.2154734)
('test Acc:', 0.7949945)
model_080 F1_score: 83.29% >>>
974
('Epoch Training Loss:', 0.002863852145310375)
('Epoch Training Acc:', 1.0)
('test loss', 1.2384557)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
975
('Epoch Training Loss:', 0.0028019916594530514)
('Epoch Training Acc:', 1.0)
('test loss', 1.2595178)
('test Acc:', 0.79425836)
model_080 F1_score: 83.16% >>>
976
('Epoch Training Loss:', 0.0027637780635814124)
('Epoch Training Acc:', 1.0)
('test loss', 1.245829)
('test Acc:', 0.7894737)
model_080 F1_score: 82.83% >>>
977
('Epoch Training Loss:', 0.0026614703401719453)
('Epoch Training Acc:', 1.0)
('test loss', 1.2543035)
('test Acc:', 0.7905778)
model_080 F1_score: 82.92% >>>
978
('Epoch Training Loss:', 0.0026007971168837685)
('Epoch Training Acc:', 1.0)
('test loss', 1.2334911)
('test Acc:', 0.7927862)
model_080 F1_score: 83.15% >>>
979
('Epoch Training Loss:', 0.002558759182193171)
('Epoch Training Acc:', 1.0)
('test loss', 1.2502527)
('test Acc:', 0.7887376)
model_080 F1_score: 82.82% >>>
980
('Epoch Training Loss:', 0.002718681850183202)
('Epoch Training Acc:', 1.0)
('test loss', 1.2415286)
('test Acc:', 0.7905778)
model_080 F1_score: 82.74% >>>
981
('Epoch Training Loss:', 0.0027022011499866494)
('Epoch Training Acc:', 1.0)
('test loss', 1.2506436)
('test Acc:', 0.7949945)
model_080 F1_score: 83.27% >>>
982
('Epoch Training Loss:', 0.002697467969483114)
('Epoch Training Acc:', 1.0)
('test loss', 1.2649457)
('test Acc:', 0.7902098)
model_080 F1_score: 83.03% >>>
983
('Epoch Training Loss:', 0.002578302170149982)
('Epoch Training Acc:', 1.0)
('test loss', 1.2440429)
('test Acc:', 0.7957306)
model_080 F1_score: 83.33% >>>
984
('Epoch Training Loss:', 0.0027342597563801974)
('Epoch Training Acc:', 1.0)
('test loss', 1.2509879)
('test Acc:', 0.79352224)
model_080 F1_score: 83.19% >>>
985
('Epoch Training Loss:', 0.002584532898708858)
('Epoch Training Acc:', 1.0)
('test loss', 1.2502056)
('test Acc:', 0.7905778)
model_080 F1_score: 83.01% >>>
986
('Epoch Training Loss:', 0.0027647866972984048)
('Epoch Training Acc:', 1.0)
('test loss', 1.2604612)
('test Acc:', 0.7909459)
model_080 F1_score: 82.80% >>>
987
('Epoch Training Loss:', 0.0026620686562637275)
('Epoch Training Acc:', 1.0)
('test loss', 1.2487009)
('test Acc:', 0.79315424)
model_080 F1_score: 83.01% >>>
988
('Epoch Training Loss:', 0.0024706495255486516)
('Epoch Training Acc:', 1.0)
('test loss', 1.2417588)
('test Acc:', 0.7957306)
model_080 F1_score: 83.28% >>>
989
('Epoch Training Loss:', 0.0026300976110178453)
('Epoch Training Acc:', 1.0)
('test loss', 1.2296606)
('test Acc:', 0.79425836)
model_080 F1_score: 83.38% >>>
990
('Epoch Training Loss:', 0.00260247953929138)
('Epoch Training Acc:', 1.0)
('test loss', 1.2363944)
('test Acc:', 0.79425836)
model_080 F1_score: 83.13% >>>
991
('Epoch Training Loss:', 0.0027518709243850026)
('Epoch Training Acc:', 1.0)
('test loss', 1.2517902)
('test Acc:', 0.7927862)
model_080 F1_score: 82.98% >>>
992
('Epoch Training Loss:', 0.002750697901319654)
('Epoch Training Acc:', 1.0)
('test loss', 1.2380052)
('test Acc:', 0.79131395)
model_080 F1_score: 82.97% >>>
993
('Epoch Training Loss:', 0.0026329657830501674)
('Epoch Training Acc:', 1.0)
('test loss', 1.2265111)
('test Acc:', 0.79425836)
model_080 F1_score: 83.22% >>>
994
('Epoch Training Loss:', 0.0025071938480323297)
('Epoch Training Acc:', 1.0)
('test loss', 1.2241005)
('test Acc:', 0.7975708)
model_080 F1_score: 83.33% >>>
995
('Epoch Training Loss:', 0.002731705764290382)
('Epoch Training Acc:', 1.0)
('test loss', 1.2492036)
('test Acc:', 0.7898417)
model_080 F1_score: 82.84% >>>
996
('Epoch Training Loss:', 0.002726722550505656)
('Epoch Training Acc:', 1.0)
('test loss', 1.2407626)
('test Acc:', 0.7898417)
model_080 F1_score: 82.90% >>>
997
('Epoch Training Loss:', 0.002596534167878417)
('Epoch Training Acc:', 1.0)
('test loss', 1.2463845)
('test Acc:', 0.791682)
model_080 F1_score: 83.19% >>>
998
('Epoch Training Loss:', 0.0026466437166163814)
('Epoch Training Acc:', 1.0)
('test loss', 1.255865)
('test Acc:', 0.7898417)
model_080 F1_score: 82.85% >>>
999
('Epoch Training Loss:', 0.0025211270599356794)
('Epoch Training Acc:', 1.0)
('test loss', 1.2743706)
('test Acc:', 0.7909459)
model_080 F1_score: 82.97% >>>
('Max Test Accuracy', 0.8023555)
